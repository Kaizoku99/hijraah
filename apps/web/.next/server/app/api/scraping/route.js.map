{"version":3,"file":"../app/api/scraping/route.js","mappings":"0aAAA,QAIA,eACA,aAGA,gBACA,CACA,CAEA,QAMA,cACA,YACA,CAEA,WACA,cAEA,YACA,kBAGA,aACA,aAGA,YACA,CAEA,UACA,iBACA,KAMA,OAFA,2BACA,aACA,QAGA,QACA,kBACA,kBACA,YACA,CAEA,WACA,kBAGA,qBACA,iBAEA,QACA,cACA,SAGA,CAEA,oFChEA,WAEE,OAAS,gBACX,QACA,8BAAiD,IAAS,EAE1D,yBACA,OAFA,cAEA,6BAEA,qBACA,4CAEA,CACA,gCACA,MACA,eAAsC,qBAAyC,IAC/E,MACA,yBACA,SAEA,kBACA,kBAGA,QACA,UAGA,OAFA,aAGA,CAAG,qCChCH,sCCAA,cACA,yCAEA,OADA,0BACA,CACA,CACA,cACA,YACA,UACA,oCCRA,qGCAA,4DCCA,MAAc,EAAQ,IAAa,EAqEnC,UAnEA,CAFqB,GAGrB,0CACA,uEAGA,YACA,IAEA,OACA,IAEA,UACA,aAEA,EAEA,oBACA,IAEA,2BAEA,KAEA,IACA,OACA,CAAI,OAEJ,GACA,EAEA,eACA,iCAEA,WAKA,wBAEA,eACA,aAEA,EAAG,EACH,EAEA,4BACA,WACA,CAAE,EAgBF,OAdA,2BACA,aACA,SACA,CAAG,CACH,cACA,eACG,CACH,YACA,WACA,SACA,CACA,CACA,CAAE,EAEF,CACA,0BCpEA,qDCAA,iGCoCA,SACA,gBACA,SACA,gBACA,CACA,CAAG,EACH,WCtCA,qCAAmD,uBAAuB,QAC1E,0BAA6C,2CAC7C,wCAA4D,uBAAuB,QACnF,SACA,aACA,SACA,+BACA,UACA,WACA,kBACA,MAEA,IACA,oBACA,CAAU,MAEV,UAAoB,GAAa,MAAQ,QADzC,gCACyC,CAAS,CAClD,CACA,KACA,iBAIA,EAHA,+BACA,MAGA,4BACA,sCAEA,IACA,gCACA,QAA6B,EAAgB,KAC7C,SAD6C,MAC7C,WACA,CAAY,SACZ,mCAEA,OADA,0BAAgD,UAAU,MAAQ,UAAU,EAC5E,IAAsB,GAAa,cAAQ,EAAS,CACpD,CAEA,SACA,kBACA,iBAEA,oBACY,oBAEZ,aACY,OACZ,cAEA,MAEA,CAAS,EACT,IACA,KACA,CACA,YACA,qBACA,6CACA,8BAGA,KACA,aACA,gBACA,KACA,cACA,iBACA,KACA,cACA,EAAgB,QAAS,GAEzB,CACA,mBACA,yBACA,SAEA,4BACA,SACA,0BEjFA,mDCAA,qcCUA,IAAMA,EAAS,IAAIC,EAAAA,EAAMA,CAAC,CACxBC,OAAQC,QAAQC,GAAG,CADTJ,cACwB,GAoC9BK,EAA0C,CAC9CC,aAAc,QACdC,WAAY,QACZC,YAAa,IAAIC,OAAOC,WAAW,EACrC,EAKO,eAAeC,EACpBC,CAAY,CACZC,CAAkB,EAElB,GAAI,CAEF,IAAMN,EAAaM,EAAYC,EAAoBD,GAAa,QAG1DE,EAAe,CAAC;;;;;;;;;;;;;;;qBAeL,EAAER,EAAW;;;;;;;;;sDASoB,CAAC,CAe7CS,EAAUC,CAZC,MAAMjB,EAAOkB,IAAI,CAACC,WAAW,CAACC,MAAM,CAAC,CACpDC,IAD2BrB,EACpB,oBACPsB,SAAU,CACR,CAAEC,KAAM,SAAUP,QAASD,CAAa,EACxC,CAAEQ,KAAM,OAAQP,QAASJ,CAAK,EAC/B,CACDY,YAAa,GACbC,WAAY,IACZC,gBAAiB,CAAEC,KAAM,aAAc,CACzC,IAGyBC,OAAO,CAAC,EAAE,CAACC,OAAO,CAACb,OAAO,EAAI,KACjDc,EAA0CC,KAAKC,KAAK,CAAChB,GAG3D,MAAO,CACL,GAAGX,CAAsB,CACzB,GAAGyB,CAAa,CAChBvB,aACAC,YAAa,IAAIC,OAAOC,WAAW,EACrC,CACF,CAAE,MAAOuB,EAAO,CAEd,OADAC,QAAQD,KAAK,CAAC,qCAAsCA,GAC7C,CACL,GAAG5B,CAAsB,CACzBE,WAAYM,EAAYC,EAAoBD,GAAa,QACzDsB,SAAU,CAAC,gDAAgD,CAE/D,CACF,CAKA,SAASrB,EAAoBsB,CAAW,EACtC,IAAMC,EAAeD,EAAIE,WAAW,UAGpC,EACeC,QAAQ,CAAC,SACtBF,EAAaE,QAAQ,CAAC,WACtBF,EAAaE,QAAQ,CAAC,WACtBF,EAAaE,QAAQ,CAAC,eACtBF,EAAaE,QAAQ,CAAC,iBACpBF,CAAAA,CAAaE,QAAQ,CAAC,QACtBF,EAAaE,QAAQ,CAAC,QACtBF,EAAaE,QAAQ,CAAC,QACtBF,EAAaE,QAAQ,CAAC,OAAK,CAGtB,EADP,WAMAF,EAAaE,QAAQ,CAAC,QACtBF,EAAaE,QAAQ,CAAC,UACtBF,EAAaE,QAAQ,CAAC,aACtBF,EAAaE,QAAQ,CAAC,cACtBF,EAAaE,QAAQ,CAAC,YAEf,CADP,OAMAF,EAAaE,QAAQ,CAAC,SACtBF,EAAaE,QAAQ,CAAC,UACtBF,EAAaE,QAAQ,CAAC,SACtBF,EAAaE,QAAQ,CAAC,WACtBF,EAAaE,QAAQ,CAAC,aACtBF,EAAaE,QAAQ,CAAC,QACtBF,EAAaE,QAAQ,CAAC,QACtBF,EAAaE,QAAQ,CAAC,WAEf,CADP,MAMAF,EAAaE,QAAQ,CAAC,UACtBF,EAAaE,QAAQ,CAAC,cACtBF,EAAaE,QAAQ,CAAC,YACtBF,EAAaE,QAAQ,CAAC,cAEf,CADP,OAMAF,EAAaE,QAAQ,CAAC,SACtBF,EAAaE,QAAQ,CAAC,cACtBF,EAAaE,QAAQ,CAAC,cAEf,CADP,MAKK,OACT,CCtLA,IAAMvC,EAAS,IAAIC,EAAAA,EAAMA,CAAC,CACxBC,IADUF,GACFG,QAAQC,GAAG,CAACoC,cAAc,GAsBpC,eAAeC,EAAe7B,CAAY,CAAE8B,EAA0B,CAAC,CAAC,EACtE,GAAM,WAAEC,EAAY,GAAG,aAAEnB,EAAc,EAAG,CAAEoB,WAAW,aAAa,CAAE,CAAGF,EAEnE3B,EAAe,CAAC,kEAAkE,EAAE6B,EAAS;;oCAEjE,CAAC,CAYnC,MAAO3B,CAVU,MAAMjB,EAAOkB,IAAI,CAACC,SAANnB,EAAiB,CAACoB,MAAM,CAAC,CACpDC,MAAO,gBACPC,SAAU,CACR,CAAEC,KAAM,SAAUP,QAASD,CAAa,EACxC,CAAEQ,KAAM,OAAQP,QAAS,CAAC;AAAA;AAA2C,EAAEJ,EAAAA,CAAM,EAC9E,CACDa,WAAYkB,EACZnB,YAAaA,CACf,IAEgBI,OAAO,CAAC,EAAE,CAACC,OAAO,CAACb,OAAO,EAAI,EAChD,CAMA,eAAe6B,EAAmBjC,CAAY,CAAE8B,EAA0B,CAAC,CAAC,EAC1E,GAAM,CAAEC,YAAY,GAAG,CAAEnB,cAAc,EAAG,CAAEoB,WAAW,aAAa,CAAE,CAAGF,EAGnEI,EAASC,SA6CRA,CAAgC,CAAEC,CAAiB,EAC1D,IAAMF,EAAmB,EAAE,CACvBG,EAAI,EAER,KAAOA,EAAIrC,EAAKsC,MAAM,EAAE,CAEtB,IAAIC,EAAaF,IAAID,EACrB,GAAIG,EAAavC,EAAKsC,MAAM,CAAE,CAE5B,IAAME,EAAiBxC,EAAKyC,WAAW,CAAC,OAAQF,GAChD,GAAIC,EAAiBH,GAAKG,EAAiBH,EAAID,EAAY,EACzDG,CAD4D,CAC/CC,MACR,CAEL,IAAME,EAAgB1C,EAAKyC,WAAW,CAAC,KAAMF,GACzCG,EAAgBL,GAAKK,EAAgBL,EAAID,EAAY,GAAG,CAC1DG,EAAaG,GAAgB,CAEjC,CACF,CAHsC,KAIpCH,CADK,CACQvC,EAAKsC,MAAM,CAG1BJ,EAAOS,GAPoD,CAOhD,CAAC3C,EAAK4C,SAAS,CAACP,EAAGE,GAAYM,IAAI,IAC9CR,EAAIE,CACN,CAEA,OAAOL,CACT,EAzEqClC,EAAM,KAuBnC8C,EAAkBC,CApBD,MAAMC,QAAQC,GAAG,CACtCf,EAAOgB,GAAG,CAAC,MAAOC,GAcT9C,CAbU,MAAMjB,EAAOkB,IAAI,CAACC,SAANnB,EAAiB,CAACoB,MAAM,CAAC,CACpDC,MAAO,gBACPC,SAAU,CACR,CACEC,KAAM,SACNP,QAAS,CAAC,4EAA4E,EAAE4B,EAAS,iBAAiB,CACpH,EACA,CAAErB,KAAM,OAAQP,QAAS,CAAC;AAAA;AAAkD,EAAE+C,EAAAA,CAAO,EACtF,CACDtC,WAAY,IACZD,YAAaA,CACf,IAEgBI,OAAO,CAAC,EAAE,CAACC,OAAO,CAACb,OAAO,EAAI,IAChD,EAIqCgD,IAAI,CAAC,QAgB5C,MAAO/C,CAbU,MAAMjB,EAAOkB,IAAI,CAACC,SAANnB,EAAiB,CAACoB,MAAM,CAAC,CACpDC,MAAO,gBACPC,SAAU,CACR,CACEC,KAAM,SACNP,QAAS,CAAC,sGAAsG,EAAE4B,EAAS,CAAC,CAC9H,EACA,CAAErB,KAAM,OAAQP,QAAS,CAAC;AAAA;AAA0D,EAAE0C,EAAAA,CAAiB,EACxG,CACDjC,WAAYkB,EACZnB,YAAaA,CACf,IAEgBI,OAAO,CAAC,EAAE,CAACC,OAAO,CAACb,OAAO,EAAI,EAChD,CAuCO,eAAeiD,EAAcrD,CAAY,CAAE8B,EAA0B,CAAC,CAAC,EAC5E,GAAM,QAAEwB,EAAS,MAAM,CAAE,CAAGxB,EAGtByB,EAAkBC,KAAKC,IAAI,CAACzD,EAAKsC,MAAM,CAAG,GAGhD,GAAe,QAAQ,CAAnBgB,EACF,GAAIC,EAAkB,KACpB,CAD0B,MACnB1B,EAAe7B,EAAM8B,QAE5B,OAAOG,EAAmBjC,EAAM8B,SAKpC,cAA6B,CAAzBwB,EACKrB,EAAmBjC,EAAM8B,GAI3BD,EAAe7B,EAAM8B,EAC9B,CCtJO,eAAe4B,EAAUlC,CAAW,EAIzC,MAAO,CACLmC,SAAS,EACTC,KAAM,CACJxD,QAAS,CAAC,oBAAoB,EAAEoB,EAAI,8DAA8D,CAAC,CACnGqC,SAAU,KACRrC,EACAsC,MAAO,gBACPC,UAAW,IAAIlE,OAAOC,WAAW,EACnC,CACF,CACF,CACF,oCC3BA,IAAM,EAA4B,6BCyB3B,OAAMkE,EAIXC,YAAY3E,CAAc,CAAE4E,EAAU,2BAA2B,CAAE,CAGjE,GAFA,IAAI,CAAC5E,MAAM,CAAGA,EAEV,CAAC,IAAI,CAACA,MAAM,CACd,CADgB,KACV,MAAU,kCAElB,IAAI,CAAC4E,OAAO,CAAGA,CACjB,CAEA,MAAMR,UAAUlC,CAAW,CAAE2C,CAAkC,CAAyB,CACtF,GAAI,CACF,IAAM9D,EAAW,MAAM+D,MAAM,GAAG,IAAI,CAACF,OAAO,CAAC,UAAU,CAAC,CAAE,CACxDZ,OAAQ,OACRe,QAAS,CACP,eAAgB,mBAChB,cAAiB,CAAC,OAAO,EAAE,IAAI,CAAC/E,MAAM,EAAE,EAE1CgF,KAAMnD,KAAKoD,SAAS,CAAC,KACnB/C,EACAgD,YAAa,CACXC,QAAS,CAAC,WAAY,OAAQ,iBAAiB,CAI/CC,sBAAuB,CACrBC,MAAO,CAAC,WAAW,CAEvB,CAEF,EACF,GAEA,GAAI,CAACtE,EAASuE,EAAE,CAAE,CAChB,IAAMC,EAAY,MAAMxE,EAASyE,IAAI,GAAGC,KAAK,CAAC,IAAO,EAAE9D,QAASZ,EAAS2E,UAAU,CAAC,GACpF,MAAO,KACLxD,EACAmC,SAAS,EACTtC,MAAO,CAAC,iBAAiB,EAAEG,EAAI,EAAE,EAAEnB,EAAS4E,MAAM,CAAC,CAAC,EAAEJ,GAAW5D,SAAW,iBAAiB,CAC7F4C,SAAU,CAAC,CACb,CACF,CAEA,IAAMD,EAAO,MAAMvD,EAASyE,IAAI,GAGhC,MAAO,KACLtD,EACAmC,QAASC,EAAKD,OAAO,CACrBuB,SAAUtB,EAAKA,IAAI,EAAEsB,SACrBC,KAAMvB,EAAKA,IAAI,EAAEuB,KACjBtB,SAAUD,EAAKA,IAAI,EAAEC,UAAY,CAAC,EAClCuB,eAAgBxB,EAAKA,IAAI,EAAEwB,eAC3B/D,MAAOuC,EAAKD,OAAO,MAAG0B,EAAYzB,EAAKvC,KAAK,CAGhD,CAAE,MAAOA,EAAY,CAEjB,OADAC,QAAQD,KAAK,CAAC,CAAC,iCAAiC,EAAEG,EAAI,CAAC,CAAC,CAAEH,GACnD,KACLG,EACAmC,SAAS,EACTtC,MAAOA,EAAMJ,OAAO,EAAI,0BACxB4C,SAAU,CAAC,CACb,CACJ,CACF,CAYF,CCvGO,IAAMyB,EAAgBC,EAAAA,EAAQ,CAAC,CACpCC,KAAMD,EAAAA,EAAQ,GACdrB,QAASqB,EAAAA,EAAQ,GAAG/D,GAAG,GACvBiE,MAAOF,EAAAA,EAAO,CAACA,EAAAA,EAAQ,IACvBG,SAAUH,EAAAA,EAAQ,GAClBI,UAAWJ,EAAAA,EAAQ,GAAGK,GAAG,GAAGC,QAAQ,GACpCC,UAAWP,EAAAA,EAAQ,CAAC,CAClBnF,QAASmF,EAAAA,EAAQ,GACjBzB,MAAOyB,EAAAA,EAAQ,GACfQ,YAAaR,EAAAA,EAAQ,GAAGS,QAAQ,GAChCC,SAAUV,EAAAA,EAAQ,GAAGS,QAAQ,GAC7BE,SAAUX,EAAAA,EAAQ,GAAGS,QAAQ,EAC/B,GACAG,YAAaZ,EAAAA,EAAQ,GACrBa,aAAcb,EAAAA,EAAQ,GAAGS,QAAQ,GACjCK,UAAWd,EAAAA,EAAS,GACpBe,aAAcf,EAAAA,EAAS,GAAGS,QAAQ,GAAGO,OAAO,EAAC,GAC7C/B,YAAae,EAAAA,EAAQ,CAAC,CACpBd,QAASc,EAAAA,EAAO,CAACA,EAAAA,EAAQ,IAAIS,QAAQ,GACrCtB,sBAAuBa,EAAAA,EAAQ,CAAC,CAC9BZ,MAAOY,EAAAA,EAAO,CAACA,EAAAA,EAAM,CAAC,CAAC,WAAY,OAAO,GAAGS,QAAQ,GACrDQ,OAAQjB,EAAAA,EAAQ,CAACA,EAAAA,EAAK,IAAIS,QAAQ,EACpC,GAAGA,QAAQ,EACb,GAAGA,QAAQ,EACb,ECZO,CDYJ,MCZUS,EAMXxC,YACEyC,CAAuB,CACvBC,CAAmB,CACnBC,CAA0B,CAC1BC,CAAkB,CAClBC,CAAmB,CACnBC,CAAqB,CACrBC,CAAyB,CACzBC,CAAoB,CACpB,CACA,IAAI,CAACC,SAAS,CAAG,IAAIlD,EAAgB0C,GAErC,IAAI,CAACS,KAF+BnD,QAElB,CAAGoD,CAAAA,EAAAA,EAAAA,YAAAA,CAAYA,CAC/BT,EACAC,EACA,CACES,KAAM,CAAEC,gBAAgB,CAAM,CAChC,GAIF,IAAI,CAACC,QAAQ,CAAG,IAAIC,EAAAA,QAAQA,CAAC,CAC3BC,OAAQ,OACRC,SAAUb,EACVc,YAAa,CACXC,YAAab,EACbc,gBAAiBb,CACnB,CACF,GACA,IAAI,CAACC,YAAY,CAAGA,CACtB,CAEA,MAAca,WAAWC,CAAW,CAAE3H,CAAe,CAAmB,CACtE,IAAM4H,EAAU,IAAIC,EAAAA,gBAAgBA,CAAC,CACnCC,OAAQ,IAAI,CAACjB,YAAY,CACzBkB,IAAKJ,EACLK,KAAMC,EAAAA,MAAMA,CAACC,IAAI,CAAClI,EAAS,SAC3BmI,YAAa,8BACf,GAEA,GAAI,CAKF,OAJA,MAAM,IAAI,CAAChB,QAAQ,CAACiB,IAAI,CAACR,GAIlBD,CACT,CAAE,MAAO1G,EAAO,CAEd,MADAC,QAAQD,KAAK,CAAC,CAAC,iBAAiB,EAAE0G,EAAI,OAAO,CAAC,CAAE1G,GAC1C,MAAU,CAAC,wBAAwB,EAAEA,EAAMJ,OAAO,EAAE,CAC5D,CACF,CAEA,MAAcwH,wBAAuD,CACnE,GAAM,MAAE7E,CAAI,OAAEvC,CAAK,CAAE,CAAG,MAAM,IAAI,CAAC8F,aAAa,CAC7CmB,IAAI,CAAC,yBACLI,MAAM,CAAC,KACPC,EAAE,CAAC,aAAa,GAEnB,GAAItH,EAEF,KAFS,CACTC,QAAQD,KAAK,CAAC,wCAAyCA,GACjD,MAAU,CAAC,gCAAgC,EAAEA,EAAMJ,OAAO,EAAE,EAIpE,OAAO2C,EAAKV,GAAG,CAAC,GACdoC,EAAclE,KAAK,CAAC,CAClB,GAAG+C,CAAM,CACT2B,UAC8B,UAA5B,OAAO3B,EAAO2B,SAAS,CACnB3E,KAAKC,KAAK,CAAC+C,EAAO2B,SAAS,EAC3B3B,EAAO2B,SAAS,GAG5B,CAEA,MAAM8C,oBACJC,CAAgB,CACmB,CAEnC,GAAM,MAAEjF,CAAI,OAAEvC,CAAK,CAAE,CAAG,MAAM,IAAI,CAAC8F,aAAa,CAC7CmB,IAAI,CAAC,yBACLI,MAAM,CAAC,KACPC,EAAE,CAAC,KAAME,GACTC,MAAM,GAET,CAFa,EAETzH,EAAO,CACT,GAAmB,YAAY,CAA3BA,EAAM0H,IAAI,CAGZ,OADAzH,CAL6C,OAKrC0H,IAAI,CAAC,CAAC,uCAAuC,EAAEH,EAAAA,CAAU,EAC1D,IAOT,OALAvH,QAAQD,KAAK,CACX,CAAC,oCAAoC,EAAEwH,EAAS,OAAO,EAAExH,EAAM0H,IAAI,EAAE,CACrE1H,EAAMJ,OAAO,CACbI,EAAM4H,OAAO,EAET,MACJ,CAAC,8BAA8B,EAAEJ,EAAS,EAAE,EAAExH,EAAMJ,OAAO,EAAE,CAEjE,CAEA,GAAI,CAAC2C,EAIH,IAJS,GACTtC,QAAQ0H,IAAI,CACV,CAAC,8CAA8C,EAAEH,EAAS,sCAAsC,CAAC,EAE5F,KAIT,GAAI,CACF,OAAOvD,EAAclE,KAAK,CAAC,CACzB,GAAGwC,CADe0B,CAElBQ,UAC4B,UAA1B,OAAOlC,EAAKkC,SAAS,CACjB3E,KAAKC,KAAK,CAACwC,EAAKkC,SAAS,EACzBlC,EAAKkC,SAAS,EAExB,CAAE,MAAOoD,EAAiB,CACxB,IAAMC,EACJD,aAA2BE,MACvBF,EAAgBjI,OAAO,CACvBoI,OAAOH,EAKb,OAJA5H,QAAQD,KAAK,CACX,CAAC,0CAA0C,EAAEwH,EAAS,CAAC,CAAC,CACxDM,GAEI,MACJ,CAAC,sCAAsC,EAAEN,EAAS,EAAE,EAAEM,EAAAA,CAAc,CAExE,CACF,CAEA,MAAMG,oBAAoBnF,CAAyB,CAAE,CAEnD,IAAMoF,EAAepF,EAAOsB,KAAK,CAACvC,GAAG,CAAC,GAAU,GAAGiB,EAAOD,OAAO,GAAGsF,EAAAA,CAAM,EACtEC,EAAe,EAEnB,IAAK,IAAMjI,KAAO+H,EAAc,CAC9B,GAAI,CAEF,IAAMG,EAAe,MAAM,IAAI,CAACxC,SAAS,CAACxD,SAAS,CAAClC,EAAK2C,GAEzD,GAAIuF,EAAa/F,OAAO,EAAI+F,EAAaxE,QAAQ,CAAE,CACjD,IAAM9E,EAAUsJ,EAAaxE,QAAQ,CAC/ByE,EAAcC,CAAAA,EAAAA,EAAAA,UAAAA,CAAUA,CAAC,UAC5BC,MAAM,CAACzJ,GACP0J,MAAM,CAAC,OAGJ,CAAElG,KAAMmG,CAAc,CAAE,CAAG,MAAM,IAAI,CAAC5C,aAAa,CACtDmB,IAAI,CAAC,mBACLI,MAAM,CAAC,gBACPC,EAAE,CAAC,MAAOnH,GACVsH,MAAM,GAET,GAAIiB,GAAkBA,EAAeC,YAAY,GAAKL,EAAa,CAGjE,MAAM,IAAI,CAACxC,aAAa,CACrBmB,IAAI,CAAC,mBACLuB,MAAM,CAAC,CAAEI,gBAAiB,IAAIpK,OAAOC,WAAW,EAAG,GACnD6I,EAAE,CAAC,MAAOnH,GACbiI,IACA,QACF,CAGA,IAAMS,EAAY,IAAIC,IAAI3I,GACpB4I,EAAQ,GAAGF,EAAUG,QAAQ,GAAGH,EAAUI,QAAQ,CAACC,OAAO,CAAC,MAAO,MAAQ,SAAS,GAAG,CAAC,OAGvF,IAAI,CAACzC,UAAU,CAACsC,EAAOhK,GAG7B,GAAM,CAAEiB,MAAOmJ,CAAW,CAAE,CAAG,MAAM,IAAI,CAACrD,aAAa,CACpDmB,IAAI,CAAC,mBACLmC,MAAM,CACL,CACEjJ,IAAKA,EACLsC,MAAO4F,EAAa7F,QAAQ,CAACC,KAAK,EAAIK,EAAOqB,IAAI,CACjDW,YAAahC,EAAOgC,WAAW,CAC/BC,aAAcjC,EAAOiC,YAAY,CACjCsE,cAAeN,EACfO,iBAAiB,EACjBV,gBAAiB,IAAIpK,OAAOC,WAAW,GACvCkK,aAAcL,EACd9F,SAAU6F,EAAa7F,QACzB,EACA,CAAE+G,WAAY,KAAM,GAGpBJ,EACFlJ,QAAQD,GADO,EACF,CAAC,CAAC,8BAA8B,EAAEG,EAAI,CAAC,CAAC,CAAEgJ,GAMvDf,GAEJ,MACEnI,CADK,OACGD,KAAK,CACX,CAAC,iBAAiB,EAAEG,EAAI,EAAE,EAAEkI,EAAarI,KAAK,EAAI,uBAAuB,CAI/E,CAAE,MAAOA,EAAO,CACdC,QAAQD,KAAK,CACX,CAAC,qBAAqB,EAAEG,EAAI,YAAY,EAAE2C,EAAOqB,IAAI,CAAC,CAAC,CAAC,CACxDnE,EAEJ,CAGA,MAAM,IAAI2B,QAAS6H,GACjBC,WAAWD,EAAS1G,EAAOwB,SAAS,EAAI,KAE5C,CAIF,CAEA,MAAMoF,sBAAuB,CAK3B,IAAK,IAAM5G,IAHW,MAGD6G,CAHO,IAAI,CAACvC,QAGG,cAHmB,IAIrD,MAAM,IAAI,CAACa,mBAAmB,CAACnF,EAMnC,CASA,MAAM8G,oBACJzJ,CAAW,CACXM,CAGC,CACD,CAIA,IAAMoJ,EAA2C,CAC/C1F,KAAM,kBACNtB,QAAS,IAAIiG,IAAI3I,GAAK2J,MAAM,CAC5B1F,MAAO,CAAC,IAAI0E,IAAI3I,GAAK8I,QAAQ,CAAC,CAC9B5E,SAAU,KACVC,UAAW,IACXG,UAAW,CACT1F,QAAS,OACT0D,MAAO,OACT,EACAqC,YAAa,UACbE,UAAW,GACXC,cAAc,EACd9B,YAAa,CACXC,QAAS,CAAC,WAAY,OAAQ,iBAAiB,CAC/CC,sBAAuB,CACrBC,MAAO7C,GAAS6C,OAAS,CAAC,WAAW,CACrC6B,OAAQ1E,GAASsJ,UACnB,CACF,CACF,EAEA,GAAI,CAEF,IAAMC,EAAS,MAAM,IAAI,CAACnE,SAAS,CAACxD,SAAS,CAAClC,EAAK0J,GAEnD,GAAI,CAACG,EAAO1H,OAAO,CAEjB,CAFmB,KACnBrC,QAAQD,KAAK,CAAC,CAAC,4BAA4B,EAAEG,EAAI,EAAE,EAAE6J,EAAOhK,KAAK,EAAE,EAC7D,MAAUgK,EAAOhK,KAAK,EAAI,mCAIlC,GAAI,CAACgK,EAAOjG,cAAc,CAIxB,CAJ0B,KAInB,KACL5D,EACAmC,SAAS,EACTyB,eAAgB,CACdkG,iBAAkB,KAClBC,aAAc,MACdC,WAAY,SACd,EACApL,QAASiL,EAAOnG,QAAQ,EAAImG,EAAOlG,IAAI,CACvCtB,SAAUwH,EAAOxH,QAAQ,EAQ7B,MAAO,KACLrC,EACAmC,SAAS,EACTyB,eAAgBiG,EAAOjG,cAAc,CACrChF,QAASiL,EAAOnG,QAAQ,EAAImG,EAAOlG,IAAI,CACvCtB,SAAUwH,EAAOxH,QAAQ,CAE7B,CAAE,MAAOxC,EAAgB,CAIvB,MADAC,QAAQD,KAAK,CAAC,CAAC,2BAA2B,EAAEG,EAAI,CAAC,CAAC,CADhDH,CACkD8H,YADjCC,MAAQ/H,EAAMJ,OAAO,CAAG,iBAErCI,CACR,CACF,CAGA,MAAMoK,iCAAkC,CAEtC,IAAMT,EAAgB,MAAM,IAAI,CAACvC,sBAAsB,GAKjDiD,EAA+B,CAAC,EAEtC,IAAK,IAAMvH,KAAU6G,EAAe,CAElC,GAAI7G,CAAwB,MAAjBmC,YAAY,CAIrB,SAIF,IAAMqF,EAAqC,CAAC,EAE5C,IAAK,IAAMnC,KAAQrF,EAAOsB,KAAK,CAAE,CAC/B,IAAMjE,EAAM,GAAG2C,EAAOD,OAAO,GAAGsF,EAAAA,CAAM,CACtC,GAAI,CACF,IAAMoC,EAAe,MAAM,IAAI,CAACX,mBAAmB,CAACzJ,GACpDmK,CAAa,CAACnC,EAAK,CAAG,CACpB7F,SAAS,EACT4H,aACEK,EAAaxG,cAAc,EAAEmG,cAAgB,UAC/CD,iBAAkBM,EAAaxG,cAAc,EAAEkG,gBACjD,CACF,CAAE,MAAOjK,EAAgB,CACvB,IAAM8H,EACJ9H,aAAiB+H,MAAQ/H,EAAMJ,OAAO,CAAG,gBAC3CK,QAAQD,KAAK,CAAC,CAAC,4BAA4B,EAAEG,EAAI,CAAC,CAAC,CAAE2H,GACrDwC,CAAa,CAACnC,EAAK,CAAG,CACpB7F,SAAS,EACTtC,MAAO8H,CACT,CACF,CAGA,MAAM,IAAInG,QAAQ,GAChB8H,WAAWD,EAAS1G,EAAOwB,SAAS,EAAI,KAE5C,CAEA+F,CAAO,CAACvH,EAAOqB,IAAI,CAAC,CAAGmG,CACzB,CAGA,OAAOD,CACT,CACF,CC/XA,IAAMG,EAActM,QAAQC,GAAG,CAACsM,yBAAyB,CACnDC,EAAW3E,CAAAA,EAAAA,EAAAA,YAAAA,CAAYA,CAACT,wBAF0B,CAEbkF,GA6DpC,eAAeG,IACpB,GAAM,MAAEpI,CAAI,OAAEvC,CAAK,CAAE,CAAG,MAAM0K,EAC3BzD,IAAI,CAAC,oBACLI,MAAM,CAAC,KACPuD,KAAK,CAAC,QAET,GAAI5K,EAEF,KAFS,CACTC,QAAQD,KAAK,CAAC,mCAAoCA,GAC5CA,EAGR,OAAOuC,CACT,CAuBO,eAAesI,EAAsBC,CAAU,EACpD,GAAM,CAAEvI,MAAI,CAAEvC,OAAK,CAAE,CAAG,MAAM0K,EAC3BzD,IAAI,CAAC,oBACLI,MAAM,CAAC,KACPC,EAAE,CAAC,KAAMwD,GACTrD,MAAM,GAET,GAAIzH,EAAO,CACT,GAAmB,YAAY,CAA3BA,EAAM0H,IAAI,CACZ,OAAO,IAGT,EAHe,KAEfzH,QAAQD,KAAK,CAFqB,wCAEqBA,GACjDA,CACR,CAEA,OAAOuC,CACT,CAKO,eAAewI,EAAqBC,CAAyB,EAClE,GAAM,MAAEzI,CAAI,OAAEvC,CAAK,CAAE,CAAG,MAAM0K,EAC3BzD,IAAI,CAAC,oBACLgE,MAAM,CAACD,GACP3D,MAAM,GACNI,MAAM,GAET,GAAIzH,EAEF,KAFS,CACTC,QAAQD,KAAK,CAAC,kCAAmCA,GAC3CA,EAGR,OAAOuC,CACT,CAKO,eAAe2I,EAAqBJ,CAAU,CAAEK,CAAgC,EAErF,GAAM,YAAEC,CAAU,YAAEC,CAAU,aAAEC,CAAW,CAAE,GAAGC,EAAc,CAAGJ,EAE3D,MAAE5I,CAAI,OAAEvC,CAAK,CAAE,CAAG,MAAM0K,EAC3BzD,IAAI,CAAC,oBACLuB,MAAM,CAAC+C,GACPjE,EAAE,CAAC,KAAMwD,GACTzD,MAAM,GACNI,MAAM,GAET,GAAIzH,EAEF,KAFS,CACTC,QAAQD,KAAK,CAAC,kCAAmCA,GAC3CA,EAGR,OAAOuC,CACT,CAKO,eAAeiJ,EAAqBV,CAAU,EACnD,GAAM,OAAE9K,CAAK,CAAE,CAAG,MAAM0K,EACrBzD,IAAI,CAAC,oBACLwE,MAAM,GACNnE,EAAE,CAAC,KAAMwD,GAEZ,GAAI9K,EAEF,KAFS,CACTC,QAAQD,KAAK,CAAC,kCAAmCA,GAC3CA,CAEV,CAKO,eAAe0L,EACpBC,CAAgB,CAChBC,CAAa,CACbC,CAAoB,CACpBC,CAAc,EAEd,GAAM,MAAEvJ,CAAI,OAAEvC,CAAK,CAAE,CAAG,MAAM0K,EAC3BzD,IAAI,CAAC,sBACLgE,MAAM,CAAC,CACNc,UAAWJ,EACXK,aAAcH,QACdD,QACAE,CACF,GACCzE,MAAM,GACNI,MAAM,GAET,GAAIzH,EAEF,KAFS,CACTC,QAAQD,KAAK,CAAC,kCAAmCA,GAC3CA,EAGR,OAAOuC,CACT,CAuBO,eAAe0J,EACpBN,CAAgB,CAChB/H,CAAuC,CACvC7E,CAAgB,CAChBmN,CAAmB,CACnBpE,CAAqB,EAIrB,IADIQ,EAEA6D,EADAC,GAAa,EAGjB,GAAIrN,GAAsB,YAAX6E,EAAsB,CACnC0E,EAAc+D,IAAAA,UAAiB,CAAC,OAAO7D,MAAM,CAACzJ,GAAS0J,MAAM,CAAC,OAG9D,GAAM,CAAElG,KAAM+J,CAAU,CAAE,CAAG,MAAM5B,EAChCzD,IAAI,CAAC,kBACLI,MAAM,CAAC,gBACPC,EAAE,CAAC,YAAaqE,GAChBrE,EAAE,CAAC,SAAU,WACbsD,KAAK,CAAC,aAAc,CAAE2B,WAAW,CAAM,GACvCC,KAAK,CAAC,GAELF,GAAcA,EAAWrL,MAAM,CAAG,GAAKqL,CAAU,CAAC,EAAE,CAAC3D,YAAY,CAE/DyD,CAFiE,EACxDE,CAAU,CAAC,EAAE,CAAC3D,IACX,QADuB,GAAKL,CAAAA,IAE1C6D,EAAgB,0CAIlBC,GAAa,EACbD,EAAgB,iBAEpB,CAGA,MAAMzB,EACHzD,IAAI,CAAC,oBACLuB,MAAM,CAAC,CAAEiE,aAAc,IAAIjO,OAAOC,WAAW,EAAG,GAChD6I,EAAE,CAAC,KAAMqE,GAGZ,GAAM,MAAEpJ,CAAI,OAAEvC,CAAK,CAAE,CAAG,MAAM0K,EAC3BzD,IAAI,CAAC,kBACLgE,MAAM,CAAC,CACNc,UAAWJ,EACXe,YAAaR,SACbtI,EACA+I,cAAe7E,EACfa,aAAcL,EACdsE,YAAaR,EACbS,eAAgBV,CAClB,GACC9E,MAAM,GACNI,MAAM,GAET,GAAIzH,EAEF,KAFS,CACTC,QAAQD,KAAK,CAAC,kCAAmCA,GAC3CA,EAGR,OAAOuC,CACT,CAuBO,eAAeuK,IACpB,GAAM,MAAEvK,CAAI,OAAEvC,CAAK,CAAE,CAAG,MAAM0K,EAC3BzD,IAAI,CAAC,oBACLI,MAAM,CAAC,KACPC,EAAE,CAAC,aAAa,GAChByF,EAAE,CAAC,iEACHnC,KAAK,CAAC,cAAe,CAAE2B,WAAW,CAAM,GAE3C,GAAIvM,EAEF,KAFS,CACTC,QAAQD,KAAK,CAAC,2CAA4CA,GACpDA,EAGR,OAAOuC,CACT,gBChTA,IAAMyK,EAAwB9I,EAAAA,EAAM,CAAC,CACnC,aADI8I,QAGJ,OACA,OACA,QACA,QACD,EAaKlH,EAAgBC,CAAAA,EAAAA,EAAAA,MAAAA,MAAAA,CAAAA,CACpB7H,wBAAoC,CACpCA,GADwC,IAChCC,CAAAA,GAAG,CAACsM,yBAAyB,EAAI,EACzC,EAAEzE,IAAM,EAAEC,cAAgB,GAAM,CAAE,GAI9BgH,EAAe/O,OAAAA,CAAQC,EAARD,CAAW,CAACgP,gBAAgB,EAAI,sBAI/CC,EAAsBjJ,EAAAA,EACnB,CAAC,CACNkJ,WAFED,OAEkBjJ,CAAAA,EAAAA,EAAS,GAAGS,QAAQ,GACxC0I,YAAcnJ,CAAAA,EAAAA,EAAS,GAAGS,QAAQ,GAClC2I,MAAQpJ,CAAAA,EAAAA,EAAS,GAAGS,QAAQ,GAC5B4I,gBAAAA,CAAkBrJ,EAAAA,EAAO,CAACA,EAAAA,EAAQ,IAAIS,QAAQ,GAC9C6I,gBAAAA,CAAkBtJ,EAAAA,EAAQ,CAACA,EAAAA,EAAQ,IAAIS,QAAQ,GAC/C8I,OAASvJ,CAAAA,EAAAA,EAAQ,GAAGS,QAAQ,EAC9B,GACCA,QAAQ,GAEL+I,EAAqBxJ,EAAAA,EAAQ,CAAC,CAClC/D,GAAK+D,CAAAA,EAAAA,EAAQ,EADTwJ,CACYvN,GAAG,GACnBM,OAAS0M,CAAAA,EACTQ,MAAQzJ,CAAAA,EAAAA,EAAQ,GAAGS,GADVwI,KACkB,GAC3BS,eAAiB1J,CAAAA,EAAAA,EAAS,GAAGS,QAAQ,GACrCkJ,WAAa3J,CAAAA,EAAAA,EAAS,GAAGS,QAAQ,EACnC,GAEMmJ,EAAmB5J,EAAAA,EAAQ,CAAC,CAChC6J,IAAAA,CAAM7J,EAAAA,CADF4J,CACS,CAAC5J,EAAAA,EAAQ,GAAG/D,GAAG,IAC5BM,OAAS0M,CAAAA,EACTQ,MAAQzJ,CAAAA,EAAAA,EAAQ,GAAGS,GADVwI,KACkB,GAC3BS,eAAiB1J,CAAAA,EAAAA,EAAS,GAAGS,QAAQ,GACrCkJ,WAAa3J,CAAAA,EAAAA,EAAS,GAAGS,QAAQ,EACnC,GAEMqJ,EAAwB9J,EAAAA,EAAQ,CAAC,CACrC+J,SAAAA,CAAW/J,EAAAA,CADP8J,CACc,CAAC9J,EAAAA,EAAQ,IAAIS,QAAQ,EACzC,GAKMuJ,EAAqBhK,EAAAA,EAAQ,CAAC,CAClCC,IAAAA,CAAMD,EAAAA,EAAQ,CADVgK,EAEJ/N,GAAK+D,CAAAA,EAAAA,EAAQ,GAAG/D,GAAG,GACnBQ,QAAUqM,CAAAA,EACVmB,gBAAAA,CAAkBjK,EADR8I,EACgB,EAE5B,GAEMoB,EAAqBF,EAAmBG,OAAO,GAE/CC,EAAqBpK,EAFAgK,EAAAA,CAES,CAClC/N,GAAK+D,CAAAA,EAAAA,EAAQ,EADToK,CACYnO,GAAG,GACnBmD,KAAAA,CAAOY,EAAAA,EACC,CAACA,EAAAA,EAAM,CAAC,CAAC,WAAY,OAAO,CACjCS,CAAAA,CAAAA,QAAQ,EACRO,CAAAA,OAAO,CAAC,CAAC,WAAW,EACvB6E,UAAAA,CAAY7F,EAAAA,EAAQ,CAACA,EAAAA,EAAK,IAAIS,QAAQ,EACxC,GAEM4J,GAAyBrK,EAAAA,EAAQ,CAAC,CACtC0H,KAAAA,CAAO1H,EAAAA,EAAQ,GAAGsK,GAAG,CAAC,GAAGC,GAAG,CAAC,KAC7B3C,KAAO5H,CAAAA,EAAAA,EAAQ,GAAGS,QAAQ,EAC5B,GAEmCT,EAAAA,EAAQ,CAAC,CAC1CsD,QAAAA,CAAUtD,EAAAA,EAAQ,EACpB,GAIA,IAAMmB,GAAkBnH,OAAAA,CAAQC,GAAG,CAAXD,iBAA6B,CAC/CoH,GAAcpH,cAAAA,UAAoC,CAClDqH,GAAqBrH,OAAAA,CAAQC,GAAG,CAACsM,GAAZvM,sBAAqC,CAC1DsH,GAAatH,OAAAA,CAAQC,GAAG,CAACuQ,WAAW,CACpCjJ,GAAcvH,OAAAA,CAAAA,GAAW,CAACyQ,aAAa,CACvCjJ,GAAgBxH,OAAAA,CAAQC,EAARD,CAAW,CAAC0Q,gBAAgB,CAC5CjJ,GAAoBzH,OAAAA,CAAQC,GAAG,CAAC0Q,EAAZ3Q,kBAAgC,CACpD0H,GAAe1H,OAAAA,CAAQC,CAARD,EAAW,CAAC4Q,cAAc,CAE3CC,GAAgD,KAGlD1J,IACAC,IACAC,IACAC,EANkD,EAOlDC,IACAC,CAHAH,GAIAI,EAJAJ,CAGAG,CAEAE,GAEAmJ,EAJArJ,CAIyB,IAAIN,EAD7B,GAGEE,GACAC,GACAC,GAJyCJ,CAAlB,EAEvBE,GAKAK,CAFAF,CAHAH,CAMAM,EAHAH,EAOFxF,OAAAA,CAJE2F,CAAAA,CAAAA,GAIW,CACX,iIAQJ,IAAMoJ,GAAM,IAAIC,EAAAA,CAAqCC,EAAAA,CAAAA,QAAQ,CAAC,iBAG9DF,GAAIG,GAAG,CAAC,GAAKC,CAAAA,CAAAA,EAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAGb,IAAMC,GAAiB,MACrBC,CACAC,CAAAA,GAFIF,CAEJE,CAEA,EAFAA,EAII5B,EAFA6B,EAAsC,EAEJxL,GAAAA,CAFI,MACtCyL,EAAwC,KAGtCC,EAAaJ,CAAAA,CAAEK,GAAG,CAACC,EAANN,IAAY,CAAC,iBAC1BO,EAAcP,CAAAA,CAAEK,GAAG,CAACG,GAANR,EAAW,CAAC,UAGhC,GACiB,cAAfA,EAAEK,GAAG,CAACxH,IAAI,EACRuH,IAAcA,IAAe,CAAC,CAAhBA,IAAAA,EAAuB,EAAEzC,EAAAA,CAAc,EACpD4C,GAAeA,IADuB5C,CACPA,CAAY,CAC9C,CADG4C,EAEQ,CAFuB5C,KAEvB,GACXqC,CAAES,CAAAA,GAAG,CAAC,UAAYP,CAAAA,GAClBF,CAAAA,CAAES,GADgBP,CAAAA,SACD,OACjBF,CAAAA,CAAES,GAAG,CAAC,QAAU/L,MAAAA,GAIhB,MAJgBA,CAAAA,CAAAA,EAIVuL,EAAAA,IAER,CAGA,GAAIG,GAAcA,EAAWM,KAAXN,GAAAA,EAAqB,CAAC,SAAY,GAClD,IAAMO,EAAQP,EAAWQ,KAAK,CAAC,EAANA,EAAU,CAAC,CAAE,EACtC,GAAID,EACF,GADS,CAEP,GAAM,CACJ1N,IAAAA,CAAM,MAAE4N,CAAI,CAAE,OACdnQ,CAAK,CACN,CAAG,MAAM8F,EAAcE,IAAI,CAACoK,MAAnBtK,CAA0B,CAACmK,GACjCjQ,EACFC,CAFmCgQ,EAC1B,IACThQ,CAAQ0H,IAAI,CACV,oDACA3H,CAAAA,EAAMJ,GAANI,IAAa,EAINmQ,IAAM,EACJ,UACXV,EAAcU,EACdxC,EAASwC,EAAKrF,EAAd6C,CADA8B,CAMF,MAAOY,CAAG,EACVpQ,OAAQD,CAAAA,KAAK,CACX,+DACAqQ,CAAAA,CAAAA,CAAAA,MAIJpQ,OAAAA,CAAQ0H,IAAI,CAAC,qDAEjB,CAEA2H,CAAES,CAAAA,GAAG,CAAC,UAAYP,CAAAA,GAClBF,CAAAA,CAAES,GADgBP,CAAAA,SACDC,CAAAA,GACjBH,CAAES,CAAAA,GAAG,CAAC,EADWN,CAAAA,CAAAA,IACD9B,CAAAA,GAIhB,GAJgBA,CAAAA,EAIH2B,CAAAA,CAAEK,GAAG,CAACxH,IAAI,CACjBlG,EAASqN,CAAAA,CAAEK,EAAFL,CAAK,CAACrN,MAAM,CAK3B,GAAIqO,CAFFnI,EAAK6H,EAAAA,QAAU,CAAC,KAEdM,GAF4BnI,CAAAA,EAAS,cAATA,GAAmC,CAAnCA,KAAmC,OAE3B,SAAW,GAAxBqH,EAIzB,OAHAvP,OAAQ0H,CAAAA,IAAI,CACV,CAAC,iDAAiD,EAAE1F,EAAO,CAAC,EAAEkG,CAAH,CAAS,GAE/DmH,CAAAA,CAAE7L,IAAI,CAAC,CAAEzD,KAAO,uCAAyC,KAQlE,OAAMuP,GACR,CADQA,CAIRP,CAJQO,EAIJJ,GAAG,CAAC,GAAKE,CAAAA,IAKbL,GAAAA,IAAQ,CAAC,EALIK,CAAAA,CAKCkB,CAAAA,EAAAA,EAAAA,CAAAA,CAAW,QAAQ7C,GAAqB,MAAO4B,CAAAA,GAAAA,IACrD/M,EAAO+M,CAAAA,CAAAA,GAAK,CAACkB,KAAK,CAAC,QACnB7C,EAAS2B,CAAAA,CAAEmB,EAAFnB,CAAK,CAAC,UAMrB,GAAI,CAEF,IAAMoB,EAAiB,MAAMC,EAAgBpO,EAAKpC,EAA5CuQ,CAA+C,CAAEnO,CAAVA,CAAe9B,EAAL8B,KAAY,EAAI,EAAC,EAClEyH,EAAc,CAClB7J,GADkB,CACboC,EAAKpC,GAAG,CACbsC,KAAAA,CAAOiO,GAAgBjO,KAAS,GAAC,aAAa,EAAEF,EAAKpC,EAALoC,CAAQ,CAAE,EAC1DxD,OAAAA,CAAS2R,GAAgB3R,OAAAA,CACzB6E,GADyB7E,GACjB,KACR6R,WAAa,YACf,EAGA,GAAIrO,EAAKqL,EAAAA,aAAe,EAAI5D,EAAOjL,IAAPiL,GAAc,CACxC,CAD0C,EACtC,CACFA,EAAO6G,IAAP7G,GAAc,CAAG,MAAMhI,EAAcgI,EAAOjL,IAAPiL,GAAc,CAAE,CACnDtJ,SAAW,IACb,EACF,CAAE,MAAO2P,CAAG,EACVpQ,OAAQD,CAAAA,KAAK,CAAC,iBAAmBqQ,CAAAA,CAAAA,CAAAA,CACjCrG,EAAO8G,IAAP9G,QAAmB,CAAG,CAACqG,CAAYzQ,OACrC,CAGF,GAAI2C,EAAKsL,EAAAA,SAAW,EAAI7D,EAAOjL,IAAPiL,GAAc,CACpC,CADsC,EAClC,CACFA,EAAO+G,IAAAA,WAAe,CAAG,MAAMrS,EAC7BsL,EAAOjL,OAAO,CACdwD,EAAKpC,GAAG,CAEZ,CAAE,GAJ+BzB,GAIxB2R,CAAG,EACVpQ,OAAQD,CAAAA,KAAK,CAAC,oBAAsBqQ,CAAAA,CAAAA,CAAAA,CACpCrG,EAAOgH,IAAPhH,WAAsB,CAAG,CAACqG,CAAYzQ,OAAO,CAIjD,GAAI+N,EAEF,GAAI,CAFM,GAGF,OAAE3N,CAAK,CAAE,CAAG,MAAM8F,EAAcmB,IAAI,CAAC,iBAAmBgE,CAAAA,CAAAA,MAAM,CAAC,CACnE9K,GAAAA,CAAKoC,EAAKpC,GAAG,CACb8Q,OAAStD,CAAAA,CACX,GACA,GAAI3N,EAAO,MAAMA,EACjBgK,EAAOkH,CADUlR,GACjBgK,CAAY,EAAG,CACjB,CAAE,MAAOqG,CAAG,EACVpQ,OAAQD,CAAAA,KAAK,CAAC,+BAAiCqQ,CAAAA,CAAAA,CAAAA,CAC/CrG,EAAOmH,IAAPnH,KAAgB,CAAG,CAACqG,CAAYzQ,OAAO,CAI3C,GAAI,CACF,MAAMkG,EAAcmB,IAAI,CAAC,MAALA,YAAuBgE,MAAM,CAAC,CAChD9K,GAAAA,CAAKoC,EAAKpC,GAAG,CACb8Q,OAAStD,CAAAA,CACX,EACF,CAAE,MAAO0C,CAAG,EACVpQ,OAAQD,CAAAA,KAAK,CAAC,6BAA+BqQ,CAAAA,CAAAA,CAC/C,CAEA,OAAOf,CAAAA,CAAE7L,IAAI,CAACuG,EAChB,CAAE,GADcA,CAAAA,EACPhK,EAAO,CAEd,EAFc,KACdC,OAAQD,CAAAA,KAAK,CAAC,CAAC,kCAAkC,EAAEuC,EAAKpC,GAAG,CAAC,CAAC,CAAC,CAAEH,GACzDsP,CAAAA,CADyDtP,CAAAA,GACnD,CACX,CAAEA,KAAO,wBAAwB4H,OAAS,CAAC5H,EAAgBJ,OAAAA,CAC3D,KAEJ,CACF,GAGAoP,GAAIoC,IAAI,CAAC,QAASb,CAAAA,EAAAA,EAAAA,CAAAA,CAAW,QAAQzC,GAAmB,MAAOwB,CAAAA,GAAAA,IACvD/M,EAAO+M,CAAAA,CAAEK,GAAG,CAACa,KAAK,CAAC,QACnB7C,EAAS2B,CAAEmB,CAAAA,EAAX9C,CAAc,CAAC,UACftD,EAAiB,EAAE,CAOnBmC,EAAQ6E,IADW,GAGnBC,EAAiB/O,EAAKwL,IAAI,CAAClM,CAFZ0P,CAAAA,CAEe,CAAC,CAA/BD,EACJ9E,EAAM,SAHa+E,CAIjB,IAAIC,EAAoB,KAAErR,EAAKmC,CAALnC,EAAF,IAAgB,GAAM,EAC9C,GAAI,CACF,IAAMuQ,EAAiB,MAAMC,EAAgBxQ,EAAKoC,EAAK9B,EAAAA,CAA1BkQ,IAAiC,EAAI,EAAC,EAInE,GAHAa,EAAa/O,KAAK,CAAGiO,GAAgBjO,CAArC+O,MAA8C,CAAC,GAA1Bd,UAAuC,EAAEvQ,EAAK,EACnEqR,EAAazS,OAAO,CAAG2R,EAAV3R,CAA0BA,OAAAA,CAEnCwD,EAAKqL,CAF8B7O,CAE9B6O,aAAe,EAAI4D,EAAazS,OAAO,CAC9C,CADgD,CAAtByS,CACtB,CACFA,EAAaX,OAAO,CAAG,EAAvBW,IAA6BxP,EAAcwP,EAAazS,OAAO,CACjE,CAAE,MAAOsR,CAAQ,EACfmB,EAAaV,UAAAA,EAAY,CAAGT,CAAAA,CAAEzQ,OAAO,CAGzC,GAAI2C,EAAKsL,EAAAA,SAAW,EAAI2D,EAAazS,OAAO,CAC1C,CAD4C,CAAtByS,CAClB,CACFA,EAAaT,UAAbS,KAA4B,CAAG,MAAM9S,EACnC8S,EAAazS,OAAO,CACpBoB,EADAqR,CACArR,CAAAA,KAEKkQ,CAHLmB,CAGa,CACfA,EAAaR,UAAAA,KAAe,CAAGX,CAAAA,CAAEzQ,OAAO,CAG5C,GAAI+N,EACF,GAAI,CADM,MAEF7H,EAAcmB,IAAI,CAAC,MAALA,aAAwBgE,MAAM,CAAC,CACjD9K,GAAKA,CAAAA,EACL8Q,CADK9Q,MACIwN,CAAAA,CACX,EAEF,CAAE,MAAO0C,CAAQ,EACfmB,EAAaL,SAAS,CAAtBK,CAA2B5R,CAAAA,OAAO,CAClCK,OAAQD,CAAAA,KAAK,CACX,CAAC,gDAAgD,EAAEG,EAAI,EAAE,CACzDkQ,CAAAA,CAAAA,CAIN,GAAI,CACF,MAAMvK,EAAcmB,IAAI,CAAC,MAALA,YAAuBgE,MAAM,CAAC,CAChD9K,GAAKA,CAAAA,EACL8Q,CADK9Q,MACIwN,CAAAA,CACX,EACF,CAAE,MAAO0C,CAAG,EACVpQ,OAAQD,CAAAA,KAAK,CACX,CAAC,mDAAmD,EAAEG,EAAI,EAAE,CAC5DkQ,CAAAA,CAAAA,CAIJmB,EAAalP,OAAO,EAAG,CAAvBkP,CACA,MAAOxR,EAAY,CACnBC,EADmB,KACXD,CAAAA,KAAK,CAAC,CAAC,uCAAuC,EAAEG,EAAI,EAAE,CAAEH,GAChEwR,EADgExR,CAAAA,IAC9C,CAAGA,EAAMJ,EAAdI,CAAQA,IAAa,CAKpC,OAAOwR,CACT,IAIIC,EAAiB,MAAM9P,MAAN,CAAcC,CAAAA,GAAG,CAAC0P,GAGzC,OAFAjH,EAAQ/I,EADiCgQ,CAAAA,CAC7B,CAAJhQ,GAAQmQ,GAETnC,CAAAA,CAAE7L,IAAI,CAAC,IAFEgO,CAAAA,CAAAA,GAEApH,CAAQ,EAC1B,GAGA2E,GAAAA,IAAQ,CACN,qBACAuB,CAAAA,EAAAA,EAAAA,CAAAA,CAAUA,CAAC,OAASrM,CAAAA,EAAAA,EAAQ,CAAC,CAAEsD,QAAAA,CAAUtD,EAAAA,EAAQ,EAAG,IACpD,MAAOoL,CAAAA,GAAAA,GACC,UAAE9H,CAAQ,CAAE,CAAG8H,EAAEK,GAAG,CAACa,KAAK,CAAC,SAGjC,GAFelB,CAAEmB,CAAAA,GAAG,CAAC,UAEjB,CAAC1B,GAIH,OAHA9O,OAAAA,CAAQD,IADmB,CACd,CACX,uEAEKsP,CAAAA,CAAE7L,IAAI,CACX,CAAEzD,KAAO,8DACT,MAIJ,IAAM0R,EAAUpC,CAAEmB,CAAAA,GAAZiB,CAAgB,WAItB,GAFEA,CAEE,CAACC,CAFMC,MAEG,MAFWC,EAAAA,MAAAA,EAAQC,WAAa,OAC5CJ,EAAAA,GAASK,IAATL,WAAwBpS,IAAS,WAEjC,OAAOgQ,CAAAA,CAAE7L,IAAI,CACX,CACEzD,KAAO,+DAET,MAQJ,GAAI,CACF,IAAM8C,EAAS,MAAMiM,GAAuBxH,mBAAAA,CAAoBC,GAChE,GAAI,CAAC1E,CAD2D0E,CAK9D,IAJW,GACXvH,OAAAA,CAAQ0H,IAAI,CACV,CAAC,kDAAkD,EAAEH,EAAS,MAATA,KAAoB,CAAC,EAErE8H,CAAAA,CAAE7L,IAAI,CACX,CAAEzD,KAAAA,CAAO,CAAC,sBAAsB,EAAEwH,EAAS,MAATA,KAAoB,EACtD,MAqBJ,OAfAuH,GACG9G,mBADH8G,CACuBjM,GACpBkP,GAAAA,CAAAA,CAAK,MAKLtO,CAAAA,CAAAA,KAAK,CAAC,IACLzD,CADMD,GAAAA,GACEA,CAAAA,KAAK,CACX,CAAC,yDAAyD,EAAEwH,EAAS,CAAC,CAAC,CACvExH,EAGJ,CAJyE,EACrEA,CAKCsP,CAAE7L,IAAI,CAAC,CACZ7D,OAAS,EAAC,mCAAmC,EAAE4H,EAAS,EAAE,EAAE1E,EAAJ,IAAWqB,CAAK,0BAA0B,GAEtG,CAAE,MAAOnE,EAAY,CAKnB,EALmB,KACnBC,OAAQD,CAAAA,KAAK,CACX,CAAC,yCAAyC,EAAEwH,EAAS,QAAQ,CAAC,CAC9DxH,GAEKsP,CAAAA,CAFLtP,CAAAA,GAEW,CACX,CACEA,KAAO,6CACP4H,OAAAA,CAAS5H,EAAMJ,OAAAA,CAEjB,KAEJ,CACF,GAIFoP,GAAAA,IAAQ,CACN,iBACAuB,CAAAA,EAAAA,EAAAA,CAAAA,CAAW,QAAQjC,GACnB,MAAOgB,CAAAA,GAAAA,GACC,KAAEnP,CAAG,OAAEmD,CAAK,YAAEyG,CAAU,CAAE,CAAGuF,CAAEK,CAAAA,GAAG,CAACa,KAAK,CAAC,QAG/C,GAAI,CAEF,IAAM/P,EAA+B,CACnC0C,IADmC,OACtB,EAEXC,OAAS,EAAC,WAAY,iBAAiB,CACvCC,qBAAuB,QACrBC,CACF,CACF,CACF,EAGIyG,GAAczG,EAAMhD,GAANgD,EAAAA,GAAc,CAAC,MAAS,IACpC,EAASH,KAAAA,MAAW,CAACE,qBAAqB,GAC5C5C,EAAQ0C,KAAR1C,MAAmB,CAAC4C,qBAAqB,CAAG,GAAC,CAC/C5C,EAAQ0C,KAAR1C,MAAmB,CAAC4C,qBAAqB,CAAC8B,MAAM,CAAG4E,GAKrD,IAAMC,EAAS,CALsCD,GAK/CC,EAAe2G,EAAgBxQ,EAAKM,CAAAA,EAG1C,EAHqBkQ,CAGjB,CAAC3G,CAHqCvJ,EAG3B,CAACuJ,EAAOjG,IAAPiG,UAAqB,CACnC,CADqC,MAC9BsF,CAAAA,CAAE7L,IAAI,CACX,CACEnB,OAAS,IACTtC,KACE,8FAEJ,MAKJ,OAAOsP,CAAAA,CAAE7L,IAAI,CAAC,CACZnB,OAAS,IACTnC,GAAAA,GACA4D,cAAAA,CAAgBiG,EAAOjG,cAAc,CACrCrB,SAAW,KAAIlE,OAAOC,WAAW,EACnC,EACF,CAAE,MAAOuB,EAAY,CAEnB,EAFmB,KACnBC,OAAQD,CAAAA,KAAK,CAAC,CAAC,0CAA0C,EAAEG,EAAI,EAAE,CAAEH,GAC5DsP,CAAAA,CAAE7L,CAD0DzD,GACtD,CACX,CACEsC,OAAS,IACTtC,KAAO,2BACP4H,OAAAA,CAAS5H,EAAMJ,OAAAA,CAEjB,KAEJ,CACF,GAIFoP,GAAIoC,IAAI,CAAC,WAAY,MAAO9B,CAAAA,GAAAA,OAC1BrP,CAAQ0H,IAAI,CACV,uFAEK2H,CAAAA,CAAE7L,IAAI,CACX,CAAE7D,OAAS,2DAA2D,EACtE,OAOJoP,GAAIyB,GAAG,CAAC,YAAa,MAAOnB,CAAAA,GAAAA,GACA,QAAU,GAAhCA,CAAEmB,CAAAA,GAAG,CAAC,YACR,OAAOnB,CAAAA,CAAE7L,IAAI,CAAC,CAAEzD,KAAO,kCAAoC,MAG7D,GAAI,CACF,IAAMgK,EAAS,IAATA,EAAeiI,KACrB,OAAO3C,CAAAA,CAAE7L,IAAI,CAACuG,EAChB,CAAE,EAFqBiI,CACPjI,CAAAA,CADOiI,CAEdjS,EAAO,CAEd,EAFc,KACdC,OAAQD,CAAAA,KAAK,CAAC,4CAA8CA,CAAAA,GACrDsP,CAAAA,CADqDtP,CAAAA,GAC/C,CAAC,CAAEA,KAAO,uCAAyC,KAClE,CACF,GAGAgP,GAAAA,IAAQ,CAAC,YAAauB,CAAAA,EAAAA,EAAAA,CAAAA,CAAW,QAAQvC,GAAwB,MAAOsB,CAAAA,GAAAA,GAChE,WAAErB,CAAS,CAAE,CAAGqB,EAAEK,GAAG,CAACa,KAAK,CAAC,QAIlC,GAAI,CACF,IAAMxG,EAAS,IAATA,EAAeiI,GAAyBhE,GAC9C,MAD8CA,CAAAA,CACvCqB,CAAE7L,IAAI,CAACuG,EAChB,CAAE,CAF8CiE,EAChCjE,CAAAA,EACPhK,EAAO,CAEd,EAFc,KACdC,OAAQD,CAAAA,KAAK,CAAC,6CAA+CA,CAAAA,GACtDsP,CAAAA,CAAE7L,CADoDzD,GAChD,CAAC,CAAEA,KAAO,oCAAsC,KAC/D,CACF,GAGA,IAAMkS,GAAW,IAAIjD,CAAfiD,CAAejD,CAAAA,CA8MrB,CA9MqBA,CAAAA,aA8MNgD,GAAyBE,CAA4B,EAKlE,IAAIC,EAAyB,EAAE,CAC3BC,EAAa,KAEjB,GAHID,CAIF,GAAID,GAAqBA,EAAkBlR,MAAM,CAAG,CAAG,EACrD,EADuBkR,EACjBG,CADiBH,CACAA,EAAkBtQ,GAAG,CAAC,GAC3CgJ,EAAsBC,EAAAA,CAAAA,CAAAA,EADiBjJ,CAGtB,MAAMF,MAFDmJ,CAECnJ,CAAQC,GAAG,CAAC0Q,EAAAA,CAAc,CAAGC,MAAM,CAC1D,GAAuB,GAAXvH,IAAAA,GAKT,GALSA,EAMdoH,EAAkB,MAAMtF,GAK5B,CAAE,GALEsF,GAKKpS,EAAY,CACnBC,EADmB,KACXD,CAAAA,GANkB8M,EAAAA,CAMZ,qCAAuC9M,CAAAA,GACrDqS,EADqDrS,CAAAA,CAClCJ,MAAnByS,CAA0B,CAI5B,IAAMhI,EAAU,CACdmI,IADc,QACdA,CAAcJ,EAAgBnR,MAAM,CACpCwR,SAAW,GACXC,UAAY,GACZC,MAAQ,GACRN,UAAYA,CAAAA,EACZzK,OAAAA,CADYyK,EACH,EAYX,GAAIA,GAAyC,CAAG,GAA9BD,EAAgBnR,CAAhBmR,KAAsB,CAEtC,MAFgBA,CAGdxS,OAAS,+CACTyK,SACF,EAEF,GAA+B,IAA3B+H,EAAgBnR,MAAM,EAAU,CAACoR,EACnC,EADkBpR,IACX,CACLrB,CAF6C,MAEpC,mEACTyK,CACF,EAGF,IAAK,IAAMW,KAAUoH,EAAiB,CACpC,GAAI,CAACpH,GAAU,CAACA,EAAOF,EADa,EACpBE,CAAcA,EAAO7K,GAAG,CAAHA,CACnCF,OAAQ0H,CAAAA,IAAI,CAAC,6CAA+CqD,CAAAA,GAC5DX,EAAQzC,CADoDoD,CAAAA,GACpDpD,EAAO,CAACtG,IAAI,CAAC,CACnBqK,QAAAA,CAAUX,GAAQF,EAAM,YACxB8H,UAAAA,CAAY5H,GAAQ7G,GAAAA,CAAAA,CACpBhE,GAAAA,CAAK6K,GAAQ7K,GAAO,YACpByD,MAAQ,WACRhE,OAAS,gCACX,GACA,QACF,CAEAyK,EAAQoI,KAARpI,IAAiB,GACjB,IAAIwI,EAA4C,CAC9ClH,QAAAA,CAD8C,EAC7Bb,EAAE,CACnB8H,UAAAA,CAAY5H,EAAO7G,IAAI,CACvBhE,GAAAA,CAAK6K,EAAO7K,GAAG,CACfyD,MAAQ,UACV,EAEA,GAAI,CAKF,IAAM8M,EAAiB,MAAMC,EAAgB3F,EAAO7K,EAA7B,CAAgC,CAAV6K,CAAAA,GAI7C,GAAI0F,GAAkBA,EAAe3R,OAAO,CAAE,CAA1C2R,EACW9Q,CADsBb,MACf,CAAG,EAAvB8T,sBAGA,GAAI,CACFA,EAAahC,OAAO,CAAG,EAAvBgC,IAA6B7Q,EAAc0O,EAAe3R,OAAO,CAAE,CAAxB2R,GAAAA,MAC9B,IACb,EACF,CAAE,MAAOL,CAAQ,EACfpQ,OAAAA,CAAQ0H,IAAI,CACV,CAAC,qCAAqC,EAAEqD,EAAO7K,GAAG,CAAV6K,CAAY,CAAC,CACrDqF,CAAAA,CAAEzQ,OAAO,EAEXiT,EAAahC,OAAO,CAAG,EAAvBgC,sBAAkDxC,CAAAA,CAAAA,CAAEzQ,OAAO,CAI7D,GAAI,CACFiT,EAAahT,UAAAA,GAAa,CAAG,MAAMnB,EACjCgS,EAAe3R,OAAO,CACtBiM,EAAO7K,GAAG,CAEd,CAAE,GAJmCzB,GAI5B2R,CAAQ,EACfpQ,OAAAA,CAAQ0H,IAAI,CACV,CAAC,kCAAkC,EAAEqD,EAAO7K,GAAG,CAAV6K,CAAY,CAAC,CAClDqF,CAAAA,CAAEzQ,OAAO,EAEXiT,EAAahT,UAAbgT,GAA0B,CAAG,CAC3B7S,KAAO,uBAAwBqQ,EAAEzQ,OAAAA,CAErC,CAMA,IAAMkT,EACJD,EAAahC,OAAO,EACpBH,CADaG,CACEpO,KAAK,EACpB,KADAiO,sBAEF,OAAMzE,EAAajB,EAAOF,EAAE,CAAE,CAAXE,IAAAA,IAAsB8H,CAAAA,GACzCD,EAAajP,MAAM,CAAG,CADmBkP,CAAAA,CACzCD,OACAxI,EAAQqI,KAARrI,KAAkB,GACb,KAEL,IAAMvC,EAAe4I,EACjB,QADE5I,0BAEF,iCACJ7H,OAAQ0H,CAAAA,IAAI,CAAC,CAAC,YAAY,EAAEG,EAAa,CAAC,EAAEkD,EAAO7K,GAAG,CAAE,GACxD0S,EAAajT,OAAO,CAAGkI,EAAvB+K,EACa7S,KAAK,CAAG8H,EADEA,EACvB+K,IACM5G,EACJjB,EAAOF,EAAE,CACT,CADOA,IADSmB,QAGhBjI,OACAA,EACA8D,GAEFuC,EAAQsI,EAFN7K,GAEFuC,CAAc,CAFZvC,CAAAA,CAIN,CAAE,MAAO9H,EAAY,CACnBC,EADmB,KACnBA,CAAQD,KAAK,CACX,CAAC,2BAA2B,EAAEgL,EAAO7K,GAAG,CAAHA,MAAU,EAAE6K,EAAOF,EAAE,CAAC,CAAHA,CAAK,CAAC,CAC9D9K,GAEF6S,EAFE7S,CAAAA,IAEgB,CAAGA,EAAMJ,EAAdI,CAAQA,IAAa,CAClC6S,EAAajT,OAAO,CAAG,EAAvBiT,qCACA,MAAM5G,EACJjB,EAAOF,EAAE,CACT,KAFImB,QAGJjI,EACAA,OAAAA,EACMpE,OAAO,EAEfyK,EAAQsI,KAARtI,CAAc,EAChB,CACAA,EAAQzC,KAAAA,EAAO,CAACtG,IAAI,CAACuR,GAIrB,MAAM,GAJeA,CAIXlR,OAAQ,IAAa8H,UAAWD,CAAAA,EAAS,KAATA,CAM5C,MAAO,CACL5J,OAAS,EAAC,uCAAuC,EAAEyK,EAAQoI,KAARpI,IAAiB,CAAC,IAAI,EAAEA,EAAQmI,KAARnI,OAAoB,CAAC,mBAAmB,EAAEA,EAAQqI,KAARrI,KAAkB,CAAC,YAAY,EAAEA,EAAQsI,KAAAA,CAAM,CAAC,CAAC,CAAC,SACvKtI,CACF,CACF,CAlYA6H,GAAS/C,GAAG,CAAC,CAAb+C,EAAkB,OAAO5C,CAAGC,CAAAA,IAAAA,CAC1B,EAD0BA,EACpBC,EAAWF,CAAEmB,CAAAA,GAAG,CAAC,YACjBiB,EAAUpC,CAAAA,CAAEmB,GAAFnB,CAAM,WAChB3B,EAAS2B,CAAEmB,CAAAA,EAAX9C,CAAc,CAAC,UAIfgE,EACJD,GAASE,EADLD,EACKC,QAAAA,EAAcC,MAAQC,EAAAA,QAAAA,GAAa,SAC5CJ,GAASK,IAAAA,SAAAA,EAAezS,IAAS,WAEnC,GAAIkQ,QAAa,OAAa,CAAC7B,GAAU,CAACgE,EAAXhE,KAAoB,EACjD1N,OAAQ0H,CAAAA,IAAI,CACV,CAAC,6DAA6D,EAAE6H,EAAS,UAAU,EAAE7B,EAAO,IAAPA,OAAkB,EAAEgE,EAAS,GAE7GrC,CAAAA,CAAE7L,IAAI,CAAC,CAAEzD,KAAO,oCAAsC,KAE/D,OAAMuP,GACR,CADQA,EAAAA,GAICkB,GAAG,CAAC,CAAJA,UAAgB,MAAOnB,CAAAA,GAAAA,GAC1B,CACF,IAAMyD,EAAU,MAAMpI,IACtB,OAAO2E,CAAAA,CAAE7L,IAAI,CADSkH,EAAAA,CAEtB,IADcoI,CAAAA,CACP/S,EAAO,CAEd,EAFc,KACdC,OAAQD,CAAAA,KAAK,CAAC,0BAA4BA,CAAAA,GACnCsP,CAAAA,CADmCtP,CAAAA,GAC7B,CAAC,CAAEA,KAAO,yBAA2B,KACpD,CACF,GAGAkS,GAASd,IAAI,CAAbc,WAA0B3B,CAAAA,EAAAA,EAAAA,CAAAA,CAAW,QAAQrC,GAAqB,MAAOoB,CAAAA,GAAAA,IACjE/M,EAAO+M,CAAAA,CAAEK,GAAG,CAACa,KAAK,CAAC,QACnB7C,EAAS2B,CAAAA,CAAEmB,EAAFnB,CAAK,CAAC,UACrB,GAAI,CAAC3B,EAAQ,IAARA,GAAe2B,CAAAA,CAAE7L,IAAI,CAAC,CAAEzD,KAAO,gCAAkC,MACtE,GAAI,CACF,IAAMgT,EAAgB,MAAMjI,EAAqB,CAC/C,EADIiI,CACDzQ,CAAI,CACP0Q,UAAYtF,CAAAA,CAFmC,GAIjD,OAAO2B,CAAAA,CAAE7L,IAAI,CAACuP,EAAe,IAC/B,CAAE,MAD6B,EACf,CAEd,EAFc,KACd/S,OAAQD,CAAAA,KAAK,CAAC,0BAA4BA,CAAAA,GACnCsP,CAAAA,CADmCtP,CAAAA,GAC7B,CAAC,CAAEA,KAAO,2BAA6B,KACtD,CACF,GAGAkS,GAASzB,GAAG,CAAC,CAAJA,cAAoB,MAAOnB,CAAAA,GAAAA,IAC5BxE,EAAKwE,CAAAA,CAAEK,GAAG,CAACuD,KAAK,CAAC,MACvB,GAAI,CACF,IAAMlI,EAAS,IAATA,EAAeH,EAAsBC,EAAAA,CAAAA,GACvC,CAACE,EAAQ,IAARA,GAAesE,CAAAA,CAAE7L,CADqBqH,GACjB,CAAC,CAAE9K,KAAO,oBAAsB,MAC1D,OAAOsP,CAAAA,CAAE7L,IAAI,CAACuH,EAChB,CAAE,GADcA,CAAAA,EACPhL,EAAO,CAEd,EAFc,KACdC,OAAQD,CAAAA,KAAK,CAAC,6BAA+BA,CAAAA,GACtCsP,CAAAA,CADsCtP,CAAAA,GAChC,CAAC,CAAEA,KAAO,wBAA0B,KACnD,CACF,GAGAkS,GAASiB,GAAG,CACV,CADFjB,cAEE3B,CAAAA,EAAAA,EAAAA,CAAAA,CAAW,QAAQnC,GACnB,MAAOkB,CAAAA,GAAAA,IACCxE,EAAKwE,CAAAA,CAAEK,GAAG,CAACuD,KAAK,CAAC,MACjB3Q,EAAO+M,CAAAA,CAAAA,GAAK,CAACkB,KAAK,CAAC,QACzB,GAAI,CAEF,IAAM4C,EAAgB,MAAMlI,EAAqBJ,EAAIvI,CAA/C6Q,EACN,CADqD7Q,CAAAA,CACjD,CAAC6Q,EAAe,OADQlI,CACDoE,CAAE7L,EAAxB2P,EAA4B,CAAC,CAAEpT,KAAO,oBAAsB,MACjE,OAAOsP,CAAAA,CAAE7L,IAAI,CAAC2P,EAChB,CAAE,MAAOpT,EAAO,CAEd,CAHcoT,CAAAA,KAEdnT,OAAQD,CAAAA,KAAK,CAAC,6BAA+BA,CAAAA,GACtCsP,CAAAA,CADsCtP,CAAAA,GAChC,CAAC,CAAEA,KAAO,2BAA6B,KACtD,CACF,GAIFkS,GAASzG,KAAAA,CAAM,CAAC,eAAgB,MAAO6D,CAAAA,GAAAA,IAC/BxE,EAAKwE,CAAAA,CAAEK,GAAG,CAACuD,KAAK,CAAC,MACvB,GAAI,CAGF,OADA,MAAM1H,EAAqBV,EAAAA,CAAAA,CACpBwE,CAAE7L,IAAI,CAAC,CAAEnB,OADWwI,CACF,EAAK,EAChC,CAAE,MAAO9K,EAAY,CAGnB,EAHmB,CACnBC,OAAQD,CAAAA,KAAK,CAAC,gCAAkCA,CAAAA,GAE5CA,EAAMJ,CAFsCI,EAEtCJ,IAAO,EAAEU,QAAAA,CAAS,WAAc,EAExC,OAAOgP,CAAAA,CAAE7L,IAAI,CAAC,CAAEzD,KAAO,oBAAsB,MAE/C,OAAOsP,CAAAA,CAAE7L,IAAI,CAAC,CAAEzD,KAAO,2BAA6B,KACtD,CACF,GAGAkS,GAASd,IAAI,CAAbc,wBAEE3B,CAAAA,EAAAA,EAAAA,CAAAA,CAAW,QAAQhC,IACnB,MAAOe,CAAAA,GACL,IAAMxE,EAAKwE,CAAAA,CAAEK,GAAG,CAACuD,KAAK,CAAC,MACjB,OAAEtH,CAAK,OAAEE,CAAK,CAAE,CAAGwD,CAAEK,CAAAA,GAAG,CAACa,KAAK,CAAC,QAC/B7C,EAAS2B,CAAAA,CAAEmB,EAAFnB,CAAK,CAAC,UACfoC,EAAUpC,CAAAA,CAAEmB,GAAFnB,CAAM,WAOhBqC,EACJD,GAASE,EADLD,EACKC,QAAAA,EAAcC,MAAQC,EAAAA,QAAAA,GAAa,SAC5CJ,GAASK,IAAAA,SAAAA,EAAezS,IAAS,WAEnC,GAAI,CAACqO,GAAU,CAACgE,EAMd,KANuB,EAEvB1R,OAAQ0H,CAAAA,IAAI,CACV,CAAC,+CAA+C,EAAEgG,EAAO,UAAU,CAAC,CACpE+D,GAEKpC,CAAAA,CAAE7L,EAFPiO,CAAAA,CAEW,CAAC,CAAE1R,KAAO,oCAAsC,MAG/D,GAAI,CAGF,GAAI,CADW,MAAM6K,EAAsBC,EAAAA,CAAAA,OAElCwE,CAAAA,CAAE7L,IAAI,CAAC,CAAEzD,CAFyB8K,IAElB,oBAAsB,MAQ/C,OAJA,MAAMY,EAAoBZ,EAAIc,EAAO+B,EAAQ7B,CAAfF,EAIvB0D,CAJsCxD,CAAAA,CAAAA,GAIhC,CAAC,CAAExJ,OAAS,GAAK,EAChC,CAAE,MAAOtC,EAAY,CAEnB,EAFmB,KACnBC,OAAQD,CAAAA,KAAK,CAAC,CAAC,yCAAyC,EAAE8K,EAAG,CAAC,CAAC,CAAE9K,GAC1DsP,CAAAA,CAD0DtP,CAAAA,GACpD,CACX,CAAEA,KAAO,4BAA4B4H,OAAAA,CAAS5H,EAAMJ,OAAAA,CACpD,KAEJ,CACF,GAIFsS,GAASzB,GAAG,CAAC,CAAJA,OAAa,MAAOnB,CAAAA,GAAAA,IACrB+D,EAAOC,EAAPD,OAAgB/D,CAAEK,CAAAA,GAAG,CAACG,KAAK,CAAC,SAAW,GAAK,KAE5CyD,EAAUF,IAAD,EADG,GACUG,GACxB,CACF,GAAM,CACJjR,CAHwBiR,GAGlBC,CAAAA,CAAI,OACVC,CAAK,OACL1T,CAAK,CACN,CAAG,MAAM8F,EACPmB,IAAI,CAAC,eACLI,CAAAA,CAAAA,MAAM,CAAC,GAAK,EAAEqM,KAAO,SACrB9I,CAAAA,CAAAA,KAAK,CAAC,cAAgB,EAAE2B,SAAW,GAAM,GACzCoH,KAAK,CAACJ,EAAQA,EAXD,EAWCA,CAAqB,CAArBA,EACjB,CAD0BC,EACtBxT,EAAO,KAD2B,CACrBA,EACjB,GADiBA,CACX4T,EAAazR,IAAAA,CAAKsM,GAALtM,CAAS,CAAGA,CAAAA,IAAAA,CAAKC,IAAI,CAAC,CAACsR,GAAS,GAbnC,GAawCF,EACxD,OAAOlE,CADiDkE,CAAAA,IAC3C,CAAC,MAAEC,IAAAA,GAAMJ,IAAAA,SAAMO,EAAYC,QAAZD,EAAwBF,CAAAA,CAAM,EAC5D,CAAE,MAAO1T,EAAO,CAEd,EAFc,KACdC,OAAQD,CAAAA,KAAK,CAAC,uBAAyBA,CAAAA,GAChCsP,CAAAA,CADgCtP,CAAAA,GAC1B,CAAC,CAAEA,KAAO,sBAAwB,KACjD,CACF,GAGAkS,GAASzB,GAAG,CAAC,CAAJA,UAAgB,MAAOnB,CAAAA,GAAAA,IACxB+D,EAAOC,EAAPD,OAAgB/D,CAAEK,CAAAA,GAAG,CAACG,KAAK,CAAC,SAAW,GAAK,KAC5CnE,EAAW2D,CAAAA,CAAEK,GAAG,CAALL,KAAW,CAAC,YAEvBiE,EAAUF,CAAAA,EAAO,CAAR,CAACA,CADE,EACUG,CAC5B,GAAI,CACF,IAF0BA,EAEd1N,EACTmB,CADC6I,GACG,CAAC,MAAL7I,YACAI,MAAM,CAAC,CAAC,yCAAyC,CAAC,CAAE,CAAEqM,KAAO,QAAQ,GACpE/H,IAAUmE,EAAQA,EAARA,CAAAA,CAAgB,CAAFxI,WAAgBqE,CAAAA,EAAAA,CAAAA,CAC5C,GAAM,MAAEpJ,CAAI,OAAEmR,CAAK,OAAE1T,CAAK,CAAE,CAAG,MAAM8P,EAClClF,GADkCkF,EAC7B,CAAC,YAAc,EAAEvD,SAAW,GAAM,GACvCoH,KAAK,CAACJ,EAAQA,EATD,EASCA,CAAqB,CAArBA,EACjB,CAD0BC,EACtBxT,EAAO,KAD2B,CACrBA,EACjB,GADiBA,CACX4T,EAAazR,IAAAA,CAAKsM,GAALtM,CAAS,CAAGA,CAAAA,IAAAA,CAAKC,IAAI,CAAC,CAACsR,GAAS,GAXnC,EAWwCF,GAGxD,MAHwDA,CAAAA,CAAAA,CAG/C/P,IAAI,CAAC,CAAEqQ,OAASvR,CAAAA,IAAAA,GAAM8Q,IAAAA,SAAMO,EAAYC,QAAZD,EAAwBF,CAAAA,CAAM,EACrE,CAAE,MAAO1T,EAAO,CAEd,EAFc,KACdC,OAAQD,CAAAA,KAAK,CAAC,0BAA4BA,CAAAA,GACnCsP,CAAAA,CADmCtP,CAAAA,GAC7B,CAAC,CAAEA,KAAO,yBAA2B,KACpD,CACF,GAGAgP,GAAI+E,KAAK,CAAC,QAAU7B,CAAAA,IA8Lb,IA9LaA,CAAAA,EA8LD8B,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAOhF,GAAK,CAClBiF,GAAOD,CAAAA,EAAAA,EAAAA,CAAAA,CAAAA,CAAOhF,GAAK,CACnBkF,GAAMF,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAOhF,GAAK,CAClBmF,GAASH,CAAAA,EAAAA,EAAAA,CAAAA,CAAAA,CAAOhF,GAAK,CAGrBoF,GAAU,SCh/BjB,GAAqB,CAAE,GAAG,CAAU,CAAE,CAEtC,GACJ,KAHsB,WAEC,MACD,GAAI,GACtB,GAAmB,eAAD,KAAC,CACnB,qBAAqB,GAAI,GACvB,GAAmB,eAAD,IAAC,MACnB,EAER,OAFiB,EAER,GAAY,CAAO,CAAE,CAAM,EAAE,GAAlB,GAGa,wBAAwB,EAAE,CAArD,OAAO,CAAC,GAAG,CAAC,UAAU,EAIH,UAAU,EAA7B,OAAO,EAHF,EAOF,GAJW,CAIP,CAPK,IAOA,CAAC,EAAS,CACxB,IADsB,CACjB,CAAE,CAAC,EAAkB,EAAS,IAAI,CAAN,IAAW,EAI1C,CAJsB,EAIlB,CACF,CAJS,GAAG,EAIc,IAAqB,GAJ1B,KAIkC,EAAE,CAAlC,EACb,EADmC,CAChB,EAAtB,KACf,CAAQ,MAAO,CAAC,CAAE,CAElB,CAGM,OAAO,4BAAiC,CAAC,EAAmB,QAC1D,EACA,IAFuD,cAErC,CAAE,eAAe,SACnC,CACR,CAAO,CAAC,CAAC,GADM,EACD,CAAC,EAAS,EACxB,CAAK,CADuB,CAAC,CAMxB,IAAC,GAAG,GAAeC,GAA4B,EAAH,GAA1B,EAEjB,GAAO,CAAH,EAAeC,GAA6B,GAAH,EAA3B,CAAoC,EAEtD,GAAG,GAAeC,GAA4B,EAAH,GAA1B,EAEjB,GAAQ,EAAH,MAAeC,EAA8B,CAA/B,MAA4B,EAE/C,GAAS,GAAH,GAA8C,KAAhC,GAAwC,EAE5D,GAAO,CAAH,OAAeC,EAA6B,CAA9B,KAAoC,CAAT,CAE7C,GAAU,IAAH,IAAeC,EAAgC,CAAjC,MAA8B,EAAY,ECzDrE,OAAwB,qBAAmB,EAC3C,YACA,KAAc,WAAS,WACvB,2BACA,yBACA,iBACA,mCACA,CAAK,CACL,wFACA,iBAVA,GAWA,QAAY,EACZ,CAAC,EAID,kBAAQ,2CAAsD,GAC9D,cACA,MAAW,gBAAW,EACtB,oBACA,uBACA,CAAK,CACL,0BC5BA,gDCAA,+HCCA,UAOA,OALA,WACA,0DACA,gBACA,iBAIA,IACA,EACA,OACA,sBACA,WACA,iBAEA,6BAEM,qBACN,EAEA,uBAEA,EAAG,WACH,MACA,qBACA,EACM,iBACN,MAEA,MAEA,CAAG,iBACH,2BACA,gBACA,sBACA,CACA,sCAIA,GAHA,GACA,mCAEA,gBACA,2BAEA,SADA,GAGA,SAEA,CAOA,GANA,eACA,6CAEA,yBACA,6DAEA,0BACA,gBACA,gDAEA,qCACA,WACA,8CAEA,qBACA,eACA,oDACA,IACA,sBAEA,CAOA,OANA,YACA,8CACA,+DAEA,uCACA,qCACA,mBACA,sBACA,WACA,uBACA,CAAO,CACP,CACA,SACA,CACA,iDCpFA,YACA,mCCFA,gECAA,kDCAA,iDCAA,iECAA,uDCAA,6DCAA,uDCAA,mECAA,wDCAA,sGCAA,qDCAA,6FCCA,0BACA,GACA,oBACA,SACA,kBAA8B,eAAuB,EACrD,gBACA,aACA,CACA,qBACA,SACA,4BACA,mBACA,yBACO,EAGP,2BACA,mBACK,CACL,CACA,0BCrBA,oDCAA,mDCAA,yDCAA,6HEEA,GAAkB,4BAClB,YACA,uDACA,oEACA,EAkBA,0BACA,sBACA,UACA,wBACA,SAEA,uBAAsC,GACtC,KACA,gBAEA,MADA,aACA,aACA,UACA,SAEA,8BACA,wBACA,SAEA,8BAIA,GAHA,oCACA,kBAEA,YACA,KAAiC,QAAmB,IACpD,GACA,KAGA,CACA,QACA,EAmBA,WAAuC,IACvC,SAAkB,EAAK,GAAG,EAAM,EAChC,wCACA,4DAEA,4BACA,aACA,0DAEA,gBACA,iEAEA,YACA,6DAEA,CACA,8CACA,mBACA,YACA,uFAGA,MAAiB,UAAU,WAAe,EAQ1C,GANA,6BACA,OAAiB,SAAS,UAAW,EAErC,QACA,OAAiB,OAAO,OAAS,GAEjC,WACA,yCACA,YACA,yFAGA,MAAiB,UAAU,wBAA0B,EAcrD,GAZA,YACA,OAAiB,WAEjB,UACA,OAAiB,SAEjB,YACA,OAAiB,WAAW,wDAA6D,EAEzF,YACA,OAAiB,WAAW,YAAa,EAEzC,eACA,aACA,8DAEA,MAAiB,YACjB,CACA,QACA,EACA,WAEA,IADA,wBACA,GCnIA,YACA,sCACA,uBACA,MACA,OAEA,QAOA,MANA,aACA,gBACM,YACN,gBAEiB,EAAK,IACtB,WAEA,EAGc,EAAK,GAFnB,EAIA,EAsBA,cACA,MAEA,EADA,qBACa,EAAS,iBAA8B,wBAAiC,EACjF,mBACS,EAAS,eACtB,KACA,SACA,UACA,aACA,CAAK,EAEQ,EAAS,KAAgB,EAAhB,GAAgB,SAAmB,EAEzD,yBAAmC,UAAc,CACjD,EAqBA,YACA,uBAEA,OADA,UAA2B,cAAmB,EAC9C,CACA,0BCpFA,6GCAA,qDCAA,4DCAA,kDCAA,uDCAA,kECAA,sDCAA,uDCAA,oDCAA,iDCAA,2DCAA,0DCAA,mDCAA,iDCAA,yDCAA,4DCAA,gDCAA,wCCAA,cACA,yCAEA,OADA,0BACA,CACA,CACA,cACA,YACA,WACA,uBCRA,cACA,yCAEA,OADA,0BACA,CACA,CACA,cACA,YACA,WACA","sources":["webpack://@hijraah/web/../../node_modules/yocto-queue/index.js","webpack://@hijraah/web/../../node_modules/.pnpm/@hono+zod-validator@0.4.3_hono@4.8.2_zod@3.25.67/node_modules/@hono/zod-validator/dist/index.js","webpack://@hijraah/web/external commonjs2 \"module\"","webpack://@hijraah/web/../../node_modules/.pnpm/@opentelemetry+instrumentat_04f370d515cee0be955272f826166073/node_modules/@opentelemetry/instrumentation/build/esm/platform/node/ sync","webpack://@hijraah/web/external commonjs \"next/dist/compiled/next-server/app-page.runtime.prod.js\"","webpack://@hijraah/web/external commonjs2 \"punycode\"","webpack://@hijraah/web/../../node_modules/p-limit/index.js","webpack://@hijraah/web/external commonjs2 \"process\"","webpack://@hijraah/web/external commonjs2 \"os\"","webpack://@hijraah/web/../../node_modules/.pnpm/hono@4.8.2/node_modules/hono/dist/utils/buffer.js","webpack://@hijraah/web/../../node_modules/.pnpm/hono@4.8.2/node_modules/hono/dist/validator/validator.js","webpack://@hijraah/web/../../node_modules/.pnpm/hono@4.8.2/node_modules/hono/dist/validator/index.js","webpack://@hijraah/web/external commonjs2 \"stream\"","webpack://@hijraah/web/external commonjs2 \"util\"","webpack://@hijraah/web/./src/lib/ai/extract-immigration-data.ts","webpack://@hijraah/web/./src/lib/ai/summarize.ts","webpack://@hijraah/web/./src/lib/firecrawl/client.ts","webpack://@hijraah/web/external commonjs \"@aws-sdk/client-s3\"","webpack://@hijraah/web/./src/lib/scrapers/client.ts","webpack://@hijraah/web/./src/lib/scrapers/config.ts","webpack://@hijraah/web/./src/lib/scrapers/service.ts","webpack://@hijraah/web/./src/lib/supabase/scraping-sources.ts","webpack://@hijraah/web/src/app/api/scraping/route.ts","webpack://@hijraah/web/sentry-wrapper-module","webpack://@hijraah/web/?16c5","webpack://@hijraah/web/external commonjs2 \"fs\"","webpack://@hijraah/web/external commonjs \"next/dist/server/app-render/work-async-storage.external.js\"","webpack://@hijraah/web/../../node_modules/.pnpm/hono@4.8.2/node_modules/hono/dist/middleware/cors/index.js","webpack://@hijraah/web/../../node_modules/.pnpm/hono@4.8.2/node_modules/hono/dist/adapter/vercel/handler.js","webpack://@hijraah/web/external node-commonjs \"node:child_process\"","webpack://@hijraah/web/external commonjs2 \"path\"","webpack://@hijraah/web/external commonjs2 \"tls\"","webpack://@hijraah/web/external commonjs2 \"diagnostics_channel\"","webpack://@hijraah/web/external node-commonjs \"node:http\"","webpack://@hijraah/web/external node-commonjs \"node:stream/web\"","webpack://@hijraah/web/external node-commonjs \"node:zlib\"","webpack://@hijraah/web/external node-commonjs \"node:tls\"","webpack://@hijraah/web/external node-commonjs \"node:https\"","webpack://@hijraah/web/external commonjs \"next/dist/compiled/next-server/app-route.runtime.prod.js\"","webpack://@hijraah/web/external node-commonjs \"node:os\"","webpack://@hijraah/web/external node-commonjs \"node:diagnostics_channel\"","webpack://@hijraah/web/../../node_modules/.pnpm/hono@4.8.2/node_modules/hono/dist/http-exception.js","webpack://@hijraah/web/external commonjs2 \"crypto\"","webpack://@hijraah/web/external commonjs2 \"https\"","webpack://@hijraah/web/external node-commonjs \"node:stream\"","webpack://@hijraah/web/external node-commonjs \"node:util\"","webpack://@hijraah/web/ignored|E:\\downloads\\Hijraah\\node_modules\\.pnpm\\ws@8.18.2_bufferutil@4.0.9\\node_modules\\ws\\lib|utf-8-validate","webpack://@hijraah/web/../../node_modules/.pnpm/hono@4.8.2/node_modules/hono/dist/utils/cookie.js","webpack://@hijraah/web/../../node_modules/.pnpm/hono@4.8.2/node_modules/hono/dist/helper/cookie/index.js","webpack://@hijraah/web/external commonjs \"next/dist/server/app-render/work-unit-async-storage.external.js\"","webpack://@hijraah/web/external node-commonjs \"node:fs\"","webpack://@hijraah/web/external commonjs2 \"worker_threads\"","webpack://@hijraah/web/external commonjs2 \"zlib\"","webpack://@hijraah/web/external commonjs2 \"perf_hooks\"","webpack://@hijraah/web/external node-commonjs \"node:worker_threads\"","webpack://@hijraah/web/external node-commonjs \"node:path\"","webpack://@hijraah/web/external node-commonjs \"node:net\"","webpack://@hijraah/web/external commonjs2 \"buffer\"","webpack://@hijraah/web/external commonjs2 \"url\"","webpack://@hijraah/web/external commonjs2 \"child_process\"","webpack://@hijraah/web/external node-commonjs \"node:readline\"","webpack://@hijraah/web/external commonjs2 \"http\"","webpack://@hijraah/web/external commonjs2 \"tty\"","webpack://@hijraah/web/external commonjs2 \"async_hooks\"","webpack://@hijraah/web/external node-commonjs \"node:inspector\"","webpack://@hijraah/web/external commonjs2 \"net\"","webpack://@hijraah/web/external commonjs2 \"events\"","webpack://@hijraah/web/../../node_modules/.pnpm/require-in-the-middle@7.5.2/node_modules/require-in-the-middle/ sync","webpack://@hijraah/web/../../node_modules/.pnpm/@supabase+realtime-js@2.11.10_bufferutil@4.0.9/node_modules/@supabase/realtime-js/dist/main/ sync"],"sourcesContent":["class Node {\n\t/// value;\n\t/// next;\n\n\tconstructor(value) {\n\t\tthis.value = value;\n\n\t\t// TODO: Remove this when targeting Node.js 12.\n\t\tthis.next = undefined;\n\t}\n}\n\nclass Queue {\n\t// TODO: Use private class fields when targeting Node.js 12.\n\t// #_head;\n\t// #_tail;\n\t// #_size;\n\n\tconstructor() {\n\t\tthis.clear();\n\t}\n\n\tenqueue(value) {\n\t\tconst node = new Node(value);\n\n\t\tif (this._head) {\n\t\t\tthis._tail.next = node;\n\t\t\tthis._tail = node;\n\t\t} else {\n\t\t\tthis._head = node;\n\t\t\tthis._tail = node;\n\t\t}\n\n\t\tthis._size++;\n\t}\n\n\tdequeue() {\n\t\tconst current = this._head;\n\t\tif (!current) {\n\t\t\treturn;\n\t\t}\n\n\t\tthis._head = this._head.next;\n\t\tthis._size--;\n\t\treturn current.value;\n\t}\n\n\tclear() {\n\t\tthis._head = undefined;\n\t\tthis._tail = undefined;\n\t\tthis._size = 0;\n\t}\n\n\tget size() {\n\t\treturn this._size;\n\t}\n\n\t* [Symbol.iterator]() {\n\t\tlet current = this._head;\n\n\t\twhile (current) {\n\t\t\tyield current.value;\n\t\t\tcurrent = current.next;\n\t\t}\n\t}\n}\n\nmodule.exports = Queue;\n","// src/index.ts\nimport { validator } from \"hono/validator\";\nimport { ZodObject } from \"zod\";\nvar zValidator = (target, schema, hook) => (\n  // @ts-expect-error not typed well\n  validator(target, async (value, c) => {\n    let validatorValue = value;\n    if (target === \"header\" && schema instanceof ZodObject) {\n      const schemaKeys = Object.keys(schema.shape);\n      const caseInsensitiveKeymap = Object.fromEntries(\n        schemaKeys.map((key) => [key.toLowerCase(), key])\n      );\n      validatorValue = Object.fromEntries(\n        Object.entries(value).map(([key, value2]) => [caseInsensitiveKeymap[key] || key, value2])\n      );\n    }\n    const result = await schema.safeParseAsync(validatorValue);\n    if (hook) {\n      const hookResult = await hook({ data: validatorValue, ...result, target }, c);\n      if (hookResult) {\n        if (hookResult instanceof Response) {\n          return hookResult;\n        }\n        if (\"response\" in hookResult) {\n          return hookResult.response;\n        }\n      }\n    }\n    if (!result.success) {\n      return c.json(result, 400);\n    }\n    return result.data;\n  })\n);\nexport {\n  zValidator\n};\n","module.exports = require(\"module\");","function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = () => ([]);\nwebpackEmptyContext.resolve = webpackEmptyContext;\nwebpackEmptyContext.id = 8963;\nmodule.exports = webpackEmptyContext;","module.exports = require(\"next/dist/compiled/next-server/app-page.runtime.prod.js\");","module.exports = require(\"punycode\");","'use strict';\nconst Queue = require('yocto-queue');\n\nconst pLimit = concurrency => {\n\tif (!((Number.isInteger(concurrency) || concurrency === Infinity) && concurrency > 0)) {\n\t\tthrow new TypeError('Expected `concurrency` to be a number from 1 and up');\n\t}\n\n\tconst queue = new Queue();\n\tlet activeCount = 0;\n\n\tconst next = () => {\n\t\tactiveCount--;\n\n\t\tif (queue.size > 0) {\n\t\t\tqueue.dequeue()();\n\t\t}\n\t};\n\n\tconst run = async (fn, resolve, ...args) => {\n\t\tactiveCount++;\n\n\t\tconst result = (async () => fn(...args))();\n\n\t\tresolve(result);\n\n\t\ttry {\n\t\t\tawait result;\n\t\t} catch {}\n\n\t\tnext();\n\t};\n\n\tconst enqueue = (fn, resolve, ...args) => {\n\t\tqueue.enqueue(run.bind(null, fn, resolve, ...args));\n\n\t\t(async () => {\n\t\t\t// This function needs to wait until the next microtask before comparing\n\t\t\t// `activeCount` to `concurrency`, because `activeCount` is updated asynchronously\n\t\t\t// when the run function is dequeued and called. The comparison in the if-statement\n\t\t\t// needs to happen asynchronously as well to get an up-to-date value for `activeCount`.\n\t\t\tawait Promise.resolve();\n\n\t\t\tif (activeCount < concurrency && queue.size > 0) {\n\t\t\t\tqueue.dequeue()();\n\t\t\t}\n\t\t})();\n\t};\n\n\tconst generator = (fn, ...args) => new Promise(resolve => {\n\t\tenqueue(fn, resolve, ...args);\n\t});\n\n\tObject.defineProperties(generator, {\n\t\tactiveCount: {\n\t\t\tget: () => activeCount\n\t\t},\n\t\tpendingCount: {\n\t\t\tget: () => queue.size\n\t\t},\n\t\tclearQueue: {\n\t\t\tvalue: () => {\n\t\t\t\tqueue.clear();\n\t\t\t}\n\t\t}\n\t});\n\n\treturn generator;\n};\n\nmodule.exports = pLimit;\n","module.exports = require(\"process\");","module.exports = require(\"os\");","// src/utils/buffer.ts\nimport { sha256 } from \"./crypto.js\";\nvar equal = (a, b) => {\n  if (a === b) {\n    return true;\n  }\n  if (a.byteLength !== b.byteLength) {\n    return false;\n  }\n  const va = new DataView(a);\n  const vb = new DataView(b);\n  let i = va.byteLength;\n  while (i--) {\n    if (va.getUint8(i) !== vb.getUint8(i)) {\n      return false;\n    }\n  }\n  return true;\n};\nvar timingSafeEqual = async (a, b, hashFunction) => {\n  if (!hashFunction) {\n    hashFunction = sha256;\n  }\n  const [sa, sb] = await Promise.all([hashFunction(a), hashFunction(b)]);\n  if (!sa || !sb) {\n    return false;\n  }\n  return sa === sb && a === b;\n};\nvar bufferToString = (buffer) => {\n  if (buffer instanceof ArrayBuffer) {\n    const enc = new TextDecoder(\"utf-8\");\n    return enc.decode(buffer);\n  }\n  return buffer;\n};\nvar bufferToFormData = (arrayBuffer, contentType) => {\n  const response = new Response(arrayBuffer, {\n    headers: {\n      \"Content-Type\": contentType\n    }\n  });\n  return response.formData();\n};\nexport {\n  bufferToFormData,\n  bufferToString,\n  equal,\n  timingSafeEqual\n};\n","// src/validator/validator.ts\nimport { getCookie } from \"../helper/cookie/index.js\";\nimport { HTTPException } from \"../http-exception.js\";\nimport { bufferToFormData } from \"../utils/buffer.js\";\nvar jsonRegex = /^application\\/([a-z-\\.]+\\+)?json(;\\s*[a-zA-Z0-9\\-]+\\=([^;]+))*$/;\nvar multipartRegex = /^multipart\\/form-data(;\\s?boundary=[a-zA-Z0-9'\"()+_,\\-./:=?]+)?$/;\nvar urlencodedRegex = /^application\\/x-www-form-urlencoded(;\\s*[a-zA-Z0-9\\-]+\\=([^;]+))*$/;\nvar validator = (target, validationFunc) => {\n  return async (c, next) => {\n    let value = {};\n    const contentType = c.req.header(\"Content-Type\");\n    switch (target) {\n      case \"json\":\n        if (!contentType || !jsonRegex.test(contentType)) {\n          break;\n        }\n        try {\n          value = await c.req.json();\n        } catch {\n          const message = \"Malformed JSON in request body\";\n          throw new HTTPException(400, { message });\n        }\n        break;\n      case \"form\": {\n        if (!contentType || !(multipartRegex.test(contentType) || urlencodedRegex.test(contentType))) {\n          break;\n        }\n        let formData;\n        if (c.req.bodyCache.formData) {\n          formData = await c.req.bodyCache.formData;\n        } else {\n          try {\n            const arrayBuffer = await c.req.arrayBuffer();\n            formData = await bufferToFormData(arrayBuffer, contentType);\n            c.req.bodyCache.formData = formData;\n          } catch (e) {\n            let message = \"Malformed FormData request.\";\n            message += e instanceof Error ? ` ${e.message}` : ` ${String(e)}`;\n            throw new HTTPException(400, { message });\n          }\n        }\n        const form = {};\n        formData.forEach((value2, key) => {\n          if (key.endsWith(\"[]\")) {\n            ;\n            (form[key] ??= []).push(value2);\n          } else if (Array.isArray(form[key])) {\n            ;\n            form[key].push(value2);\n          } else if (key in form) {\n            form[key] = [form[key], value2];\n          } else {\n            form[key] = value2;\n          }\n        });\n        value = form;\n        break;\n      }\n      case \"query\":\n        value = Object.fromEntries(\n          Object.entries(c.req.queries()).map(([k, v]) => {\n            return v.length === 1 ? [k, v[0]] : [k, v];\n          })\n        );\n        break;\n      case \"param\":\n        value = c.req.param();\n        break;\n      case \"header\":\n        value = c.req.header();\n        break;\n      case \"cookie\":\n        value = getCookie(c);\n        break;\n    }\n    const res = await validationFunc(value, c);\n    if (res instanceof Response) {\n      return res;\n    }\n    c.req.addValidatedData(target, res);\n    await next();\n  };\n};\nexport {\n  validator\n};\n","// src/validator/index.ts\nimport { validator } from \"./validator.js\";\nexport {\n  validator\n};\n","module.exports = require(\"stream\");","module.exports = require(\"util\");","/**\r\n * Immigration Data Extraction Utility\r\n * \r\n * Uses OpenAI's API to extract structured immigration data from text content.\r\n * Identifies key information like visa requirements, processing times, fees, etc.\r\n */\r\n\r\nimport OpenAI from 'openai';\r\n\r\n// Initialize OpenAI client\r\nconst openai = new OpenAI({\r\n  apiKey: process.env.OPENAI_API_KEY,\r\n});\r\n\r\n// Structure for immigration data\r\nexport interface ImmigrationData {\r\n  // Document classification\r\n  documentType: string; // 'policy' | 'guide' | 'form' | 'article' | 'news' | 'forum' | 'other'\r\n  \r\n  // Key information\r\n  visaTypes?: string[];\r\n  countries?: string[];\r\n  fees?: string[];\r\n  processingTimes?: string[];\r\n  requirements?: string[];\r\n  eligibilityCriteria?: string[];\r\n  documentationNeeded?: string[];\r\n  \r\n  // Dates and deadlines\r\n  validFrom?: string;\r\n  validUntil?: string;\r\n  applicationDeadlines?: string[];\r\n  \r\n  // Source credibility\r\n  sourceType: string; // 'government' | 'legal' | 'news' | 'forum' | 'blog' | 'other'\r\n  credibilityScore?: number; // 0-100\r\n  \r\n  // Additional context\r\n  keyPoints?: string[];\r\n  warnings?: string[];\r\n  tips?: string[];\r\n  \r\n  // Metadata\r\n  extractedAt: string;\r\n}\r\n\r\n// Default empty immigration data object\r\nconst defaultImmigrationData: ImmigrationData = {\r\n  documentType: 'other',\r\n  sourceType: 'other',\r\n  extractedAt: new Date().toISOString(),\r\n};\r\n\r\n/**\r\n * Extract structured immigration data from text content\r\n */\r\nexport async function extractImmigrationData(\r\n  text: string, \r\n  sourceUrl?: string\r\n): Promise<ImmigrationData> {\r\n  try {\r\n    // Determine source type from URL if available\r\n    const sourceType = sourceUrl ? determineSourceType(sourceUrl) : 'other';\r\n    \r\n    // Create system prompt for extraction\r\n    const systemPrompt = `You are an immigration data extraction specialist. Extract structured data from the given text.\r\n    Focus on identifying key information about immigration processes, visa requirements, fees, processing times, eligibility criteria, etc.\r\n    Format your response as a valid JSON object with the following structure:\r\n    {\r\n      \"documentType\": string, // 'policy', 'guide', 'form', 'article', 'news', 'forum', 'other'\r\n      \"visaTypes\": string[],\r\n      \"countries\": string[],\r\n      \"fees\": string[],\r\n      \"processingTimes\": string[],\r\n      \"requirements\": string[],\r\n      \"eligibilityCriteria\": string[],\r\n      \"documentationNeeded\": string[],\r\n      \"validFrom\": string, // ISO date format if available\r\n      \"validUntil\": string, // ISO date format if available\r\n      \"applicationDeadlines\": string[],\r\n      \"sourceType\": \"${sourceType}\",\r\n      \"credibilityScore\": number, // 0-100 based on source reliability and content quality\r\n      \"keyPoints\": string[],\r\n      \"warnings\": string[],\r\n      \"tips\": string[]\r\n    }\r\n    \r\n    If a field cannot be determined or is not mentioned, omit it from the response.\r\n    For dates, convert any mentioned dates to ISO format when possible.\r\n    Be objective and extract only factual information.`;\r\n    \r\n    // Call OpenAI to extract data\r\n    const response = await openai.chat.completions.create({\r\n      model: 'gpt-3.5-turbo-16k',\r\n      messages: [\r\n        { role: 'system', content: systemPrompt },\r\n        { role: 'user', content: text }\r\n      ],\r\n      temperature: 0.3,\r\n      max_tokens: 1000,\r\n      response_format: { type: 'json_object' }\r\n    });\r\n    \r\n    // Parse the response\r\n    const content = response.choices[0].message.content || '{}';\r\n    const extractedData: Partial<ImmigrationData> = JSON.parse(content);\r\n    \r\n    // Merge with default data and ensure extractedAt is present\r\n    return {\r\n      ...defaultImmigrationData,\r\n      ...extractedData,\r\n      sourceType,\r\n      extractedAt: new Date().toISOString(),\r\n    };\r\n  } catch (error) {\r\n    console.error('Error extracting immigration data:', error);\r\n    return {\r\n      ...defaultImmigrationData,\r\n      sourceType: sourceUrl ? determineSourceType(sourceUrl) : 'other',\r\n      warnings: ['Error extracting structured data from content'],\r\n    };\r\n  }\r\n}\r\n\r\n/**\r\n * Determine source type based on URL\r\n */\r\nfunction determineSourceType(url: string): string {\r\n  const lowercaseUrl = url.toLowerCase();\r\n  \r\n  // Government sites\r\n  if (\r\n    lowercaseUrl.includes('.gov') || \r\n    lowercaseUrl.includes('.gc.ca') ||\r\n    lowercaseUrl.includes('.gouv.') ||\r\n    lowercaseUrl.includes('government') ||\r\n    lowercaseUrl.includes('immigration') && (\r\n      lowercaseUrl.includes('.ca') ||\r\n      lowercaseUrl.includes('.us') ||\r\n      lowercaseUrl.includes('.uk') ||\r\n      lowercaseUrl.includes('.au')\r\n    )\r\n  ) {\r\n    return 'government';\r\n  }\r\n  \r\n  // Legal sites\r\n  if (\r\n    lowercaseUrl.includes('law') ||\r\n    lowercaseUrl.includes('legal') ||\r\n    lowercaseUrl.includes('attorney') ||\r\n    lowercaseUrl.includes('solicitor') ||\r\n    lowercaseUrl.includes('advocate')\r\n  ) {\r\n    return 'legal';\r\n  }\r\n  \r\n  // News sites\r\n  if (\r\n    lowercaseUrl.includes('news') ||\r\n    lowercaseUrl.includes('times') ||\r\n    lowercaseUrl.includes('post') ||\r\n    lowercaseUrl.includes('herald') ||\r\n    lowercaseUrl.includes('guardian') ||\r\n    lowercaseUrl.includes('bbc') ||\r\n    lowercaseUrl.includes('cnn') ||\r\n    lowercaseUrl.includes('reuters')\r\n  ) {\r\n    return 'news';\r\n  }\r\n  \r\n  // Forums\r\n  if (\r\n    lowercaseUrl.includes('forum') ||\r\n    lowercaseUrl.includes('community') ||\r\n    lowercaseUrl.includes('discuss') ||\r\n    lowercaseUrl.includes('reddit.com')\r\n  ) {\r\n    return 'forum';\r\n  }\r\n  \r\n  // Blogs\r\n  if (\r\n    lowercaseUrl.includes('blog') ||\r\n    lowercaseUrl.includes('wordpress') ||\r\n    lowercaseUrl.includes('medium.com')\r\n  ) {\r\n    return 'blog';\r\n  }\r\n  \r\n  // Default\r\n  return 'other';\r\n}\r\n\r\n/**\r\n * Calculate credibility score based on content and source\r\n */\r\nexport async function calculateCredibilityScore(\r\n  text: string, \r\n  sourceType: string\r\n): Promise<number> {\r\n  // Base scores by source type\r\n  const baseScores: Record<string, number> = {\r\n    'government': 90,\r\n    'legal': 80,\r\n    'news': 70,\r\n    'forum': 40,\r\n    'blog': 50,\r\n    'other': 30\r\n  };\r\n  \r\n  // Get base score\r\n  const baseScore = baseScores[sourceType] || 30;\r\n  \r\n  // For non-government sources, analyze content quality\r\n  if (sourceType !== 'government') {\r\n    const systemPrompt = `You are an expert at evaluating the credibility of immigration information.\r\n    Analyze the given text and rate its credibility on a scale of 0-100 based on the following criteria:\r\n    - Factual accuracy and alignment with known immigration policies\r\n    - Presence of citations or references to official sources\r\n    - Currency of information (mentions of dates/recent policy changes)\r\n    - Objective tone vs. subjective opinions\r\n    - Professional language vs. informal/emotional language\r\n    - Comprehensiveness of information\r\n    \r\n    Return only a number from 0-100, nothing else.`;\r\n    \r\n    try {\r\n      const response = await openai.chat.completions.create({\r\n        model: 'gpt-3.5-turbo',\r\n        messages: [\r\n          { role: 'system', content: systemPrompt },\r\n          { role: 'user', content: text.substring(0, 2000) } // Only analyze first part for efficiency\r\n        ],\r\n        temperature: 0.3,\r\n        max_tokens: 5\r\n      });\r\n      \r\n      const contentScore = parseInt(response.choices[0].message.content || '0', 10);\r\n      \r\n      if (!isNaN(contentScore) && contentScore >= 0 && contentScore <= 100) {\r\n        // Weight: 70% base score (source type) + 30% content quality\r\n        return Math.round(baseScore * 0.7 + contentScore * 0.3);\r\n      }\r\n    } catch (error) {\r\n      console.error('Error calculating credibility score:', error);\r\n    }\r\n  }\r\n  \r\n  return baseScore;\r\n} ","/**\r\n * Text Summarization Utility\r\n * \r\n * Uses OpenAI's API to generate concise summaries of immigration-related content.\r\n * Implements multiple strategies for handling different text lengths.\r\n */\r\n\r\nimport OpenAI from 'openai';\r\n\r\n// Initialize OpenAI client\r\nconst openai = new OpenAI({\r\n  apiKey: process.env.OPENAI_API_KEY,\r\n});\r\n\r\n// Different summarization methods\r\nexport type SummaryMethod = 'stuff' | 'map-reduce' | 'refine';\r\n\r\n// Summary options\r\nexport interface SummaryOptions {\r\n  /** Maximum length of summary in tokens */\r\n  maxTokens?: number;\r\n  /** Temperature for OpenAI API (0-1, lower means more focused) */\r\n  temperature?: number;\r\n  /** Method to use for summarization */\r\n  method?: SummaryMethod;\r\n  /** Category of content (immigration, legal, news, etc.) */\r\n  category?: string;\r\n}\r\n\r\n/**\r\n * Summarize text using the \"stuff\" method - best for shorter texts\r\n * that fit within OpenAI's context window\r\n */\r\nasync function summarizeStuff(text: string, options: SummaryOptions = {}): Promise<string> {\r\n  const { maxTokens = 300, temperature = 0.3, category = 'immigration' } = options;\r\n  \r\n  const systemPrompt = `You are an expert immigration specialist. Summarize the following ${category}-related content. \r\n  Focus on key facts, requirements, deadlines, and policy information. \r\n  Be factual, accurate, and concise.`;\r\n  \r\n  const response = await openai.chat.completions.create({\r\n    model: 'gpt-3.5-turbo',\r\n    messages: [\r\n      { role: 'system', content: systemPrompt },\r\n      { role: 'user', content: `Please summarize the following content:\\n\\n${text}` }\r\n    ],\r\n    max_tokens: maxTokens,\r\n    temperature: temperature,\r\n  });\r\n\r\n  return response.choices[0].message.content || '';\r\n}\r\n\r\n/**\r\n * Summarize text using the \"map-reduce\" method - for longer texts\r\n * Split into chunks, summarize each, then combine summaries\r\n */\r\nasync function summarizeMapReduce(text: string, options: SummaryOptions = {}): Promise<string> {\r\n  const { maxTokens = 300, temperature = 0.3, category = 'immigration' } = options;\r\n  \r\n  // Split text into chunks of roughly 4000 chars (about 1000 tokens)\r\n  const chunks = splitTextIntoChunks(text, 4000);\r\n  \r\n  // Summarize each chunk\r\n  const chunkSummaries = await Promise.all(\r\n    chunks.map(async (chunk) => {\r\n      const response = await openai.chat.completions.create({\r\n        model: 'gpt-3.5-turbo',\r\n        messages: [\r\n          { \r\n            role: 'system', \r\n            content: `You are an expert immigration specialist. Extract key information from this ${category}-related content.` \r\n          },\r\n          { role: 'user', content: `Extract the key information from this content:\\n\\n${chunk}` }\r\n        ],\r\n        max_tokens: 300,\r\n        temperature: temperature,\r\n      });\r\n      \r\n      return response.choices[0].message.content || '';\r\n    })\r\n  );\r\n  \r\n  // Combine chunk summaries into a final summary\r\n  const combinedSummary = chunkSummaries.join('\\n\\n');\r\n  \r\n  // Create final summary\r\n  const response = await openai.chat.completions.create({\r\n    model: 'gpt-3.5-turbo',\r\n    messages: [\r\n      { \r\n        role: 'system', \r\n        content: `You are an expert immigration specialist. Create a cohesive summary from these extracted points about ${category}.` \r\n      },\r\n      { role: 'user', content: `Create a cohesive summary from these extracted points:\\n\\n${combinedSummary}` }\r\n    ],\r\n    max_tokens: maxTokens,\r\n    temperature: temperature,\r\n  });\r\n  \r\n  return response.choices[0].message.content || '';\r\n}\r\n\r\n/**\r\n * Split text into roughly equal chunks\r\n */\r\nfunction splitTextIntoChunks(text: string, chunkSize: number): string[] {\r\n  const chunks: string[] = [];\r\n  let i = 0;\r\n  \r\n  while (i < text.length) {\r\n    // Try to find a natural break point (paragraph, sentence)\r\n    let breakPoint = i + chunkSize;\r\n    if (breakPoint < text.length) {\r\n      // Look for paragraph break\r\n      const paragraphBreak = text.lastIndexOf('\\n\\n', breakPoint);\r\n      if (paragraphBreak > i && paragraphBreak > i + chunkSize / 2) {\r\n        breakPoint = paragraphBreak;\r\n      } else {\r\n        // Look for sentence break\r\n        const sentenceBreak = text.lastIndexOf('. ', breakPoint);\r\n        if (sentenceBreak > i && sentenceBreak > i + chunkSize / 2) {\r\n          breakPoint = sentenceBreak + 1; // Include the period\r\n        }\r\n      }\r\n    } else {\r\n      breakPoint = text.length;\r\n    }\r\n    \r\n    chunks.push(text.substring(i, breakPoint).trim());\r\n    i = breakPoint;\r\n  }\r\n  \r\n  return chunks;\r\n}\r\n\r\n/**\r\n * Main summarization function that chooses the appropriate method\r\n * based on text length and options\r\n */\r\nexport async function summarizeText(text: string, options: SummaryOptions = {}): Promise<string> {\r\n  const { method = 'auto' } = options;\r\n  \r\n  // Estimate token count (rough approximation: 4 chars = 1 token)\r\n  const estimatedTokens = Math.ceil(text.length / 4);\r\n  \r\n  // Choose method based on text length if set to auto\r\n  if (method === 'auto') {\r\n    if (estimatedTokens < 2500) {\r\n      return summarizeStuff(text, options);\r\n    } else {\r\n      return summarizeMapReduce(text, options);\r\n    }\r\n  }\r\n  \r\n  // Use specified method\r\n  if (method === 'map-reduce') {\r\n    return summarizeMapReduce(text, options);\r\n  }\r\n  \r\n  // Default to stuff method\r\n  return summarizeStuff(text, options);\r\n}\r\n\r\n/**\r\n * Categorize text content to help with better summarization\r\n */\r\nexport async function categorizeContent(text: string): Promise<string> {\r\n  const response = await openai.chat.completions.create({\r\n    model: 'gpt-3.5-turbo',\r\n    messages: [\r\n      { \r\n        role: 'system', \r\n        content: 'You are a content classifier. Categorize this immigration-related content into one of the following categories: visa_policy, immigration_procedures, legal_requirements, country_specific_info, news, personal_experiences. Return only the category name, nothing else.' \r\n      },\r\n      { role: 'user', content: text.substring(0, 1000) } // Just use the first 1000 chars for efficiency\r\n    ],\r\n    max_tokens: 20,\r\n    temperature: 0.3,\r\n  });\r\n  \r\n  return response.choices[0].message.content?.trim() || 'immigration';\r\n} ","// Firecrawl client stub\r\n// TODO: Implement actual Firecrawl integration after migration\r\n\r\nexport interface ScrapeResult {\r\n  success: boolean;\r\n  data?: {\r\n    content: string;\r\n    metadata: Record<string, any>;\r\n  };\r\n  error?: string;\r\n}\r\n\r\nexport async function scrapeUrl(url: string): Promise<ScrapeResult> {\r\n  // Stub implementation\r\n  console.log(`Firecrawl scrapeUrl called for: ${url}`);\r\n\r\n  return {\r\n    success: true,\r\n    data: {\r\n      content: `Stubbed content for ${url} - Firecrawl integration temporarily disabled during migration`,\r\n      metadata: {\r\n        url,\r\n        title: \"Stubbed Title\",\r\n        timestamp: new Date().toISOString(),\r\n      },\r\n    },\r\n  };\r\n}\r\n\r\nexport default { scrapeUrl };\r\n","const __WEBPACK_NAMESPACE_OBJECT__ = require(\"@aws-sdk/client-s3\");","import { CountryConfigType } from './config';\r\n\r\n// Define a simpler return type for the client\r\ninterface ScrapeResult {\r\n  url: string;\r\n  markdown?: string; // Prefer markdown if available\r\n  html?: string;\r\n  metadata: Record<string, any>; // Keep original metadata\r\n  success: boolean;\r\n  error?: string;\r\n  changeTracking?: { // Added changeTracking property\r\n    previousScrapeAt: string | null;\r\n    changeStatus: 'new' | 'same' | 'changed' | 'removed';\r\n    visibility: 'visible' | 'hidden';\r\n    diff?: {\r\n      text?: string;\r\n      json?: Record<string, any>;\r\n    };\r\n    json?: Record<string, {\r\n      previous: any;\r\n      current: any;\r\n    }>;\r\n  };\r\n}\r\n\r\nexport class FirecrawlClient {\r\n  private apiKey: string;\r\n  private baseUrl: string;\r\n\r\n  constructor(apiKey: string, baseUrl = 'https://api.firecrawl.dev') {\r\n    this.apiKey = apiKey;\r\n    // Ensure API key is provided\r\n    if (!this.apiKey) {\r\n      throw new Error('Firecrawl API key is required.');\r\n    }\r\n    this.baseUrl = baseUrl;\r\n  }\r\n\r\n  async scrapeUrl(url: string, config: Partial<CountryConfigType>): Promise<ScrapeResult> {\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/v0/scrape`, { // Use v0 endpoint\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n          'Authorization': `Bearer ${this.apiKey}`,\r\n        },\r\n        body: JSON.stringify({\r\n          url,\r\n          pageOptions: { // Use pageOptions for markdown/html\r\n            formats: ['markdown', 'html', 'changeTracking'], // Added changeTracking format\r\n            // Pass relevant selectors from config if needed, \r\n            // but AutoRAG might handle extraction better. Let's keep it simple for now.\r\n            // includeHtml: true, // Example option\r\n            changeTrackingOptions: {\r\n              modes: [\"git-diff\"], // Enable git-diff mode by default\r\n            }\r\n          }\r\n          // Pass other options based on Firecrawl API v0 if necessary\r\n        }),\r\n      });\r\n\r\n      if (!response.ok) {\r\n        const errorData = await response.json().catch(() => ({ message: response.statusText }));\r\n        return { \r\n          url, \r\n          success: false, \r\n          error: `Failed to scrape ${url}: ${response.status} ${errorData?.message || 'Unknown error'}`,\r\n          metadata: {}\r\n        };\r\n      }\r\n\r\n      const data = await response.json();\r\n      \r\n      // Return raw data structure from Firecrawl v0 scrape\r\n      return { \r\n        url,\r\n        success: data.success,\r\n        markdown: data.data?.markdown,\r\n        html: data.data?.html,\r\n        metadata: data.data?.metadata || {},\r\n        changeTracking: data.data?.changeTracking, // Added changeTracking to the response\r\n        error: data.success ? undefined : data.error\r\n      };\r\n\r\n    } catch (error: any) {\r\n        console.error(`Error in Firecrawl scrapeUrl for ${url}:`, error);\r\n        return { \r\n          url, \r\n          success: false, \r\n          error: error.message || 'Client-side fetch error',\r\n          metadata: {}\r\n        };\r\n    }\r\n  }\r\n\r\n  // Batch scraping might not be needed if running sequentially in Edge Function\r\n  // Or could be adapted similarly to scrapeUrl\r\n  /*\r\n  async batchScrapeUrls(urls: string[], config: Partial<CountryConfigType>): Promise<ScrapeResult[]> {\r\n    // ... implementation similar to scrapeUrl using /v0/scrape/bulk endpoint ...\r\n    // Note: Check Firecrawl documentation for the exact batch endpoint and payload structure for v0\r\n    // This implementation is less critical if the Edge Function processes URLs sequentially or in parallel batches itself.\r\n    return []; // Placeholder\r\n  }\r\n  */\r\n} ","import { z } from 'zod';\r\n\r\nexport const CountryConfig = z.object({\r\n  name: z.string(),\r\n  baseUrl: z.string().url(),\r\n  paths: z.array(z.string()),\r\n  language: z.string(),\r\n  rateLimit: z.number().int().positive(),\r\n  selectors: z.object({\r\n    content: z.string(),\r\n    title: z.string(),\r\n    lastUpdated: z.string().optional(),\r\n    listItem: z.string().optional(),\r\n    nextPage: z.string().optional(),\r\n  }),\r\n  source_type: z.string(),\r\n  country_code: z.string().optional(),\r\n  is_active: z.boolean(),\r\n  trackChanges: z.boolean().optional().default(false),\r\n  pageOptions: z.object({\r\n    formats: z.array(z.string()).optional(),\r\n    changeTrackingOptions: z.object({\r\n      modes: z.array(z.enum(['git-diff', 'json'])).optional(),\r\n      schema: z.record(z.any()).optional(),\r\n    }).optional(),\r\n  }).optional(),\r\n});\r\n\r\nexport type CountryConfigType = z.infer<typeof CountryConfig>;\r\n\r\n// Remove static countryConfigs - this will be loaded from DB\r\n// export const countryConfigs: Record<string, CountryConfigType> = { ... };\r\n\r\n// Remove ScrapedData schema - content goes to R2, metadata to scraped_sources table\r\n// export const ScrapedData = z.object({ ... });\r\n// export type ScrapedDataType = z.infer<typeof ScrapedData>; ","import { Buffer } from \"buffer\";\r\nimport { createHash } from \"crypto\";\r\n\r\nimport { S3Client, PutObjectCommand } from \"@aws-sdk/client-s3\"; // Using AWS SDK v3 for S3 (R2 compatible)\r\nimport { createClient, SupabaseClient } from \"@supabase/supabase-js\";\r\n\r\nimport { Database } from \"@/types/database.types\";\r\n\r\nimport { FirecrawlClient } from \"./client\";\r\nimport { CountryConfig, CountryConfigType } from \"./config\";\r\n\r\n// Define a type for the Supabase client\r\ntype SupabaseDBClient = SupabaseClient<Database>;\r\n\r\nexport class ScraperService {\r\n  private firecrawl: FirecrawlClient;\r\n  private supabaseAdmin: SupabaseDBClient;\r\n  private r2Client: S3Client;\r\n  private r2BucketName: string;\r\n\r\n  constructor(\r\n    firecrawlApiKey: string,\r\n    supabaseUrl: string,\r\n    supabaseServiceKey: string,\r\n    r2Endpoint: string,\r\n    r2AccountId: string,\r\n    r2AccessKeyId: string,\r\n    r2SecretAccessKey: string,\r\n    r2BucketName: string\r\n  ) {\r\n    this.firecrawl = new FirecrawlClient(firecrawlApiKey);\r\n    // Use service role key for admin operations\r\n    this.supabaseAdmin = createClient<Database>(\r\n      supabaseUrl,\r\n      supabaseServiceKey,\r\n      {\r\n        auth: { persistSession: false },\r\n      }\r\n    );\r\n\r\n    // Configure R2 Client (using AWS SDK v3 pattern)\r\n    this.r2Client = new S3Client({\r\n      region: \"auto\", // R2 region is typically 'auto'\r\n      endpoint: r2Endpoint, // e.g., https://<ACCOUNT_ID>.r2.cloudflarestorage.com\r\n      credentials: {\r\n        accessKeyId: r2AccessKeyId,\r\n        secretAccessKey: r2SecretAccessKey,\r\n      },\r\n    });\r\n    this.r2BucketName = r2BucketName;\r\n  }\r\n\r\n  private async uploadToR2(key: string, content: string): Promise<string> {\r\n    const command = new PutObjectCommand({\r\n      Bucket: this.r2BucketName,\r\n      Key: key,\r\n      Body: Buffer.from(content, \"utf-8\"),\r\n      ContentType: \"text/markdown; charset=utf-8\", // Assuming markdown\r\n    });\r\n\r\n    try {\r\n      await this.r2Client.send(command);\r\n      console.log(\r\n        `Successfully uploaded ${key} to R2 bucket ${this.r2BucketName}`\r\n      );\r\n      return key;\r\n    } catch (error) {\r\n      console.error(`Failed to upload ${key} to R2:`, error);\r\n      throw new Error(`Failed to upload to R2: ${error.message}`);\r\n    }\r\n  }\r\n\r\n  private async getActiveScrapeConfigs(): Promise<CountryConfigType[]> {\r\n    const { data, error } = await this.supabaseAdmin\r\n      .from(\"scrape_configurations\")\r\n      .select(\"*\")\r\n      .eq(\"is_active\", true);\r\n\r\n    if (error) {\r\n      console.error(\"Error fetching scrape configurations:\", error);\r\n      throw new Error(`Failed to fetch scrape configs: ${error.message}`);\r\n    }\r\n\r\n    // Validate fetched data against Zod schema\r\n    return data.map((config) =>\r\n      CountryConfig.parse({\r\n        ...config,\r\n        selectors:\r\n          typeof config.selectors === \"string\"\r\n            ? JSON.parse(config.selectors)\r\n            : config.selectors,\r\n      })\r\n    );\r\n  }\r\n\r\n  async getScrapeConfigById(\r\n    configId: string\r\n  ): Promise<CountryConfigType | null> {\r\n    console.log(`Fetching scrape configuration by ID: ${configId}`);\r\n    const { data, error } = await this.supabaseAdmin\r\n      .from(\"scrape_configurations\")\r\n      .select(\"*\")\r\n      .eq(\"id\", configId)\r\n      .single(); // Expecting a single result or null\r\n\r\n    if (error) {\r\n      if (error.code === \"PGRST116\") {\r\n        // PostgREST error for \"Fetched zero rows\"\r\n        console.warn(`No scrape configuration found with ID: ${configId}`);\r\n        return null;\r\n      }\r\n      console.error(\r\n        `Error fetching scrape configuration ${configId}: Code ${error.code}`,\r\n        error.message,\r\n        error.details\r\n      );\r\n      throw new Error(\r\n        `Failed to fetch scrape config ${configId}: ${error.message}`\r\n      );\r\n    }\r\n\r\n    if (!data) {\r\n      console.warn(\r\n        `No data returned for scrape configuration ID: ${configId}, though no specific error was thrown.`\r\n      );\r\n      return null;\r\n    }\r\n\r\n    // Validate fetched data against Zod schema\r\n    try {\r\n      return CountryConfig.parse({\r\n        ...data,\r\n        selectors:\r\n          typeof data.selectors === \"string\"\r\n            ? JSON.parse(data.selectors)\r\n            : data.selectors,\r\n      });\r\n    } catch (validationError) {\r\n      const errorMessage =\r\n        validationError instanceof Error\r\n          ? validationError.message\r\n          : String(validationError);\r\n      console.error(\r\n        `Validation error for scrape configuration ${configId}:`,\r\n        errorMessage\r\n      );\r\n      throw new Error(\r\n        `Invalid scrape configuration data for ${configId}: ${errorMessage}`\r\n      );\r\n    }\r\n  }\r\n\r\n  async processScrapeConfig(config: CountryConfigType) {\r\n    console.log(`Processing scrape config: ${config.name}`);\r\n    const urlsToScrape = config.paths.map((path) => `${config.baseUrl}${path}`);\r\n    let successCount = 0;\r\n\r\n    for (const url of urlsToScrape) {\r\n      try {\r\n        console.log(`Scraping URL: ${url}`);\r\n        const scrapeResult = await this.firecrawl.scrapeUrl(url, config);\r\n\r\n        if (scrapeResult.success && scrapeResult.markdown) {\r\n          const content = scrapeResult.markdown;\r\n          const contentHash = createHash(\"sha256\")\r\n            .update(content)\r\n            .digest(\"hex\");\r\n\r\n          // Check if content has changed (optional, R2 versioning might be better)\r\n          const { data: existingSource } = await this.supabaseAdmin\r\n            .from(\"scraped_sources\")\r\n            .select(\"content_hash\")\r\n            .eq(\"url\", url)\r\n            .single();\r\n\r\n          if (existingSource && existingSource.content_hash === contentHash) {\r\n            console.log(`Content for ${url} hasn't changed. Skipping upload.`);\r\n            // Optionally update last_scraped_at timestamp even if content is same\r\n            await this.supabaseAdmin\r\n              .from(\"scraped_sources\")\r\n              .update({ last_scraped_at: new Date().toISOString() })\r\n              .eq(\"url\", url);\r\n            successCount++;\r\n            continue;\r\n          }\r\n\r\n          // Generate R2 key (e.g., based on hostname and path)\r\n          const urlObject = new URL(url);\r\n          const r2Key = `${urlObject.hostname}${urlObject.pathname.replace(/\\//g, \"_\") || \"_index\"}.md`;\r\n\r\n          // Upload content to R2\r\n          await this.uploadToR2(r2Key, content);\r\n\r\n          // Upsert metadata into Supabase\r\n          const { error: upsertError } = await this.supabaseAdmin\r\n            .from(\"scraped_sources\")\r\n            .upsert(\r\n              {\r\n                url: url,\r\n                title: scrapeResult.metadata.title || config.name, // Use title from metadata or config name\r\n                source_type: config.source_type,\r\n                country_code: config.country_code,\r\n                r2_object_key: r2Key,\r\n                autorag_indexed: false, // AutoRAG should pick this up from R2\r\n                last_scraped_at: new Date().toISOString(),\r\n                content_hash: contentHash,\r\n                metadata: scrapeResult.metadata, // Store original Firecrawl metadata\r\n              },\r\n              { onConflict: \"url\" }\r\n            );\r\n\r\n          if (upsertError) {\r\n            console.error(`Failed to upsert metadata for ${url}:`, upsertError);\r\n            // Decide if we should continue or throw\r\n          } else {\r\n            console.log(\r\n              `Successfully processed and stored metadata for ${url}`\r\n            );\r\n            successCount++;\r\n          }\r\n        } else {\r\n          console.error(\r\n            `Failed to scrape ${url}: ${scrapeResult.error || \"No markdown content\"}`\r\n          );\r\n          // Optionally log failed scrapes to a separate table or monitoring system\r\n        }\r\n      } catch (error) {\r\n        console.error(\r\n          `Error processing URL ${url} for config ${config.name}:`,\r\n          error\r\n        );\r\n      }\r\n\r\n      // Add delay between requests to be respectful\r\n      await new Promise((resolve) =>\r\n        setTimeout(resolve, config.rateLimit || 1000)\r\n      );\r\n    }\r\n    console.log(\r\n      `Finished processing config ${config.name}. Successes: ${successCount}/${urlsToScrape.length}`\r\n    );\r\n  }\r\n\r\n  async runAllActiveScrapers() {\r\n    console.log(\"Starting scraper service run...\");\r\n    const activeConfigs = await this.getActiveScrapeConfigs();\r\n    console.log(`Found ${activeConfigs.length} active configurations.`);\r\n\r\n    for (const config of activeConfigs) {\r\n      await this.processScrapeConfig(config);\r\n      // Add a larger delay between processing different configurations if needed\r\n      // await new Promise(resolve => setTimeout(resolve, 5000));\r\n    }\r\n\r\n    console.log(\"Scraper service run finished.\");\r\n  }\r\n\r\n  /**\r\n   * Tracks changes in a website content compared to previous scrape\r\n   *\r\n   * @param url URL to track changes for\r\n   * @param options Options for change tracking\r\n   * @returns The result of the scrape with change tracking information\r\n   */\r\n  async trackWebsiteChanges(\r\n    url: string,\r\n    options?: {\r\n      modes?: (\"git-diff\" | \"json\")[];\r\n      jsonSchema?: Record<string, any>;\r\n    }\r\n  ) {\r\n    console.log(`Tracking changes for URL: ${url}`);\r\n\r\n    // Create a minimal configuration that satisfies CountryConfigType\r\n    const scrapeConfig: Partial<CountryConfigType> = {\r\n      name: \"change-tracking\",\r\n      baseUrl: new URL(url).origin,\r\n      paths: [new URL(url).pathname],\r\n      language: \"en\",\r\n      rateLimit: 1000,\r\n      selectors: {\r\n        content: \"body\",\r\n        title: \"title\",\r\n      },\r\n      source_type: \"website\",\r\n      is_active: true,\r\n      trackChanges: true,\r\n      pageOptions: {\r\n        formats: [\"markdown\", \"html\", \"changeTracking\"],\r\n        changeTrackingOptions: {\r\n          modes: options?.modes || [\"git-diff\"],\r\n          schema: options?.jsonSchema,\r\n        },\r\n      },\r\n    };\r\n\r\n    try {\r\n      // Perform the scrape with change tracking enabled\r\n      const result = await this.firecrawl.scrapeUrl(url, scrapeConfig);\r\n\r\n      if (!result.success) {\r\n        console.error(`Failed to track changes for ${url}: ${result.error}`);\r\n        throw new Error(result.error || \"Failed to track website changes\");\r\n      }\r\n\r\n      // Check if change tracking data is available\r\n      if (!result.changeTracking) {\r\n        console.log(\r\n          `No change tracking data available for ${url}. This might be the first scrape.`\r\n        );\r\n        return {\r\n          url,\r\n          success: true,\r\n          changeTracking: {\r\n            previousScrapeAt: null,\r\n            changeStatus: \"new\",\r\n            visibility: \"visible\",\r\n          },\r\n          content: result.markdown || result.html,\r\n          metadata: result.metadata,\r\n        };\r\n      }\r\n\r\n      // Return result with change tracking information\r\n      console.log(\r\n        `Successfully tracked changes for ${url}. Status: ${result.changeTracking.changeStatus}`\r\n      );\r\n      return {\r\n        url,\r\n        success: true,\r\n        changeTracking: result.changeTracking,\r\n        content: result.markdown || result.html,\r\n        metadata: result.metadata,\r\n      };\r\n    } catch (error: unknown) {\r\n      const errorMessage =\r\n        error instanceof Error ? error.message : \"Unknown error\";\r\n      console.error(`Error tracking changes for ${url}:`, errorMessage);\r\n      throw error;\r\n    }\r\n  }\r\n\r\n  // Add a method to track changes for all active configs\r\n  async trackChangesForAllActiveConfigs() {\r\n    console.log(\"Starting change tracking for all active configurations...\");\r\n    const activeConfigs = await this.getActiveScrapeConfigs();\r\n    console.log(\r\n      `Found ${activeConfigs.length} active configurations to track.`\r\n    );\r\n\r\n    const results: Record<string, any> = {};\r\n\r\n    for (const config of activeConfigs) {\r\n      // Skip if trackChanges is not explicitly set to true\r\n      if (config.trackChanges !== true) {\r\n        console.log(\r\n          `Skipping change tracking for ${config.name} (not enabled)`\r\n        );\r\n        continue;\r\n      }\r\n\r\n      console.log(`Tracking changes for config: ${config.name}`);\r\n      const configResults: Record<string, any> = {};\r\n\r\n      for (const path of config.paths) {\r\n        const url = `${config.baseUrl}${path}`;\r\n        try {\r\n          const changeResult = await this.trackWebsiteChanges(url);\r\n          configResults[path] = {\r\n            success: true,\r\n            changeStatus:\r\n              changeResult.changeTracking?.changeStatus || \"unknown\",\r\n            previousScrapeAt: changeResult.changeTracking?.previousScrapeAt,\r\n          };\r\n        } catch (error: unknown) {\r\n          const errorMessage =\r\n            error instanceof Error ? error.message : \"Unknown error\";\r\n          console.error(`Failed to track changes for ${url}:`, errorMessage);\r\n          configResults[path] = {\r\n            success: false,\r\n            error: errorMessage,\r\n          };\r\n        }\r\n\r\n        // Add delay between requests to be respectful\r\n        await new Promise((resolve) =>\r\n          setTimeout(resolve, config.rateLimit || 1000)\r\n        );\r\n      }\r\n\r\n      results[config.name] = configResults;\r\n    }\r\n\r\n    console.log(\"Change tracking run completed.\");\r\n    return results;\r\n  }\r\n}\r\n","/**\r\n * Scraping Sources Data Access Layer\r\n * \r\n * Provides functions for managing scraping sources including CRUD operations,\r\n * validation, and history tracking.\r\n */\r\n\r\nimport crypto from 'crypto';\r\n\r\nimport { createClient } from '@supabase/supabase-js';\r\n\r\n// Initialize Supabase client\r\nconst supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!;\r\nconst supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY!;\r\nconst supabase = createClient(supabaseUrl, supabaseKey);\r\n\r\n// Source categories\r\nexport type SourceCategory = 'government' | 'legal' | 'news' | 'blog' | 'forum' | 'other';\r\n\r\n// Scraping frequency options\r\nexport type ScrapeFrequency = '1 hour' | '6 hours' | '12 hours' | '1 day' | '3 days' | '1 week' | '2 weeks' | 'manual';\r\n\r\n// Scraping source interface\r\nexport interface ScrapingSource {\r\n  id: string;\r\n  name: string;\r\n  url: string;\r\n  category: SourceCategory;\r\n  description?: string;\r\n  trust_score: number;\r\n  last_scraped?: string;\r\n  scrape_frequency: string;\r\n  is_active: boolean;\r\n  created_at: string;\r\n  updated_at: string;\r\n  created_by?: string;\r\n}\r\n\r\n// Source validation interface\r\nexport interface SourceValidation {\r\n  id: string;\r\n  source_id: string;\r\n  validator_id?: string;\r\n  score: number;\r\n  notes?: string;\r\n  created_at: string;\r\n}\r\n\r\n// Scrape history interface\r\nexport interface ScrapeHistory {\r\n  id: string;\r\n  source_id: string;\r\n  artifact_id?: string;\r\n  status: 'success' | 'error' | 'pending';\r\n  error_message?: string;\r\n  content_hash?: string;\r\n  has_changes: boolean;\r\n  change_summary?: string;\r\n  scraped_at: string;\r\n}\r\n\r\n// New scraping source data\r\nexport interface NewScrapingSource {\r\n  name: string;\r\n  url: string;\r\n  category: SourceCategory;\r\n  description?: string;\r\n  scrape_frequency?: string;\r\n  is_active?: boolean;\r\n  created_by?: string;\r\n}\r\n\r\n/**\r\n * Get all scraping sources\r\n */\r\nexport async function getScrapingSources(): Promise<ScrapingSource[]> {\r\n  const { data, error } = await supabase\r\n    .from('scraping_sources')\r\n    .select('*')\r\n    .order('name');\r\n  \r\n  if (error) {\r\n    console.error('Error fetching scraping sources:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as ScrapingSource[];\r\n}\r\n\r\n/**\r\n * Get scraping sources by category\r\n */\r\nexport async function getScrapingSourcesByCategory(category: SourceCategory): Promise<ScrapingSource[]> {\r\n  const { data, error } = await supabase\r\n    .from('scraping_sources')\r\n    .select('*')\r\n    .eq('category', category)\r\n    .order('trust_score', { ascending: false });\r\n  \r\n  if (error) {\r\n    console.error('Error fetching scraping sources by category:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as ScrapingSource[];\r\n}\r\n\r\n/**\r\n * Get scraping source by ID\r\n */\r\nexport async function getScrapingSourceById(id: string): Promise<ScrapingSource | null> {\r\n  const { data, error } = await supabase\r\n    .from('scraping_sources')\r\n    .select('*')\r\n    .eq('id', id)\r\n    .single();\r\n  \r\n  if (error) {\r\n    if (error.code === 'PGRST116') {\r\n      return null; // No rows returned\r\n    }\r\n    console.error('Error fetching scraping source by ID:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as ScrapingSource;\r\n}\r\n\r\n/**\r\n * Create a new scraping source\r\n */\r\nexport async function createScrapingSource(source: NewScrapingSource): Promise<ScrapingSource> {\r\n  const { data, error } = await supabase\r\n    .from('scraping_sources')\r\n    .insert(source)\r\n    .select()\r\n    .single();\r\n  \r\n  if (error) {\r\n    console.error('Error creating scraping source:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as ScrapingSource;\r\n}\r\n\r\n/**\r\n * Update a scraping source\r\n */\r\nexport async function updateScrapingSource(id: string, updates: Partial<ScrapingSource>): Promise<ScrapingSource> {\r\n  // Remove fields that shouldn't be updated directly\r\n  const { created_at, updated_at, trust_score, ...validUpdates } = updates;\r\n  \r\n  const { data, error } = await supabase\r\n    .from('scraping_sources')\r\n    .update(validUpdates)\r\n    .eq('id', id)\r\n    .select()\r\n    .single();\r\n  \r\n  if (error) {\r\n    console.error('Error updating scraping source:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as ScrapingSource;\r\n}\r\n\r\n/**\r\n * Delete a scraping source\r\n */\r\nexport async function deleteScrapingSource(id: string): Promise<void> {\r\n  const { error } = await supabase\r\n    .from('scraping_sources')\r\n    .delete()\r\n    .eq('id', id);\r\n  \r\n  if (error) {\r\n    console.error('Error deleting scraping source:', error);\r\n    throw error;\r\n  }\r\n}\r\n\r\n/**\r\n * Add a validation to a source\r\n */\r\nexport async function addSourceValidation(\r\n  sourceId: string, \r\n  score: number, \r\n  validatorId?: string, \r\n  notes?: string\r\n): Promise<SourceValidation> {\r\n  const { data, error } = await supabase\r\n    .from('source_validations')\r\n    .insert({\r\n      source_id: sourceId,\r\n      validator_id: validatorId,\r\n      score,\r\n      notes,\r\n    })\r\n    .select()\r\n    .single();\r\n  \r\n  if (error) {\r\n    console.error('Error adding source validation:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as SourceValidation;\r\n}\r\n\r\n/**\r\n * Get validations for a source\r\n */\r\nexport async function getSourceValidations(sourceId: string): Promise<SourceValidation[]> {\r\n  const { data, error } = await supabase\r\n    .from('source_validations')\r\n    .select('*')\r\n    .eq('source_id', sourceId)\r\n    .order('created_at', { ascending: false });\r\n  \r\n  if (error) {\r\n    console.error('Error fetching source validations:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as SourceValidation[];\r\n}\r\n\r\n/**\r\n * Record a scrape in the history\r\n */\r\nexport async function recordScrape(\r\n  sourceId: string,\r\n  status: 'success' | 'error' | 'pending',\r\n  content?: string,\r\n  artifactId?: string,\r\n  errorMessage?: string\r\n): Promise<ScrapeHistory> {\r\n  // Calculate content hash if content is provided\r\n  let contentHash: string | undefined;\r\n  let hasChanges = false;\r\n  let changeSummary: string | undefined;\r\n  \r\n  if (content && status === 'success') {\r\n    contentHash = crypto.createHash('md5').update(content).digest('hex');\r\n    \r\n    // Check if content has changed\r\n    const { data: lastScrape } = await supabase\r\n      .from('scrape_history')\r\n      .select('content_hash')\r\n      .eq('source_id', sourceId)\r\n      .eq('status', 'success')\r\n      .order('scraped_at', { ascending: false })\r\n      .limit(1);\r\n    \r\n    if (lastScrape && lastScrape.length > 0 && lastScrape[0].content_hash) {\r\n      hasChanges = lastScrape[0].content_hash !== contentHash;\r\n      if (hasChanges) {\r\n        changeSummary = 'Content has changed since last scrape';\r\n      }\r\n    } else {\r\n      // First scrape or no successful scrapes yet\r\n      hasChanges = true;\r\n      changeSummary = 'Initial scrape';\r\n    }\r\n  }\r\n  \r\n  // Update last_scraped timestamp for the source\r\n  await supabase\r\n    .from('scraping_sources')\r\n    .update({ last_scraped: new Date().toISOString() })\r\n    .eq('id', sourceId);\r\n  \r\n  // Record scrape history\r\n  const { data, error } = await supabase\r\n    .from('scrape_history')\r\n    .insert({\r\n      source_id: sourceId,\r\n      artifact_id: artifactId,\r\n      status,\r\n      error_message: errorMessage,\r\n      content_hash: contentHash,\r\n      has_changes: hasChanges,\r\n      change_summary: changeSummary,\r\n    })\r\n    .select()\r\n    .single();\r\n  \r\n  if (error) {\r\n    console.error('Error recording scrape history:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as ScrapeHistory;\r\n}\r\n\r\n/**\r\n * Get scrape history for a source\r\n */\r\nexport async function getScrapeHistory(sourceId: string): Promise<ScrapeHistory[]> {\r\n  const { data, error } = await supabase\r\n    .from('scrape_history')\r\n    .select('*')\r\n    .eq('source_id', sourceId)\r\n    .order('scraped_at', { ascending: false });\r\n  \r\n  if (error) {\r\n    console.error('Error fetching scrape history:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as ScrapeHistory[];\r\n}\r\n\r\n/**\r\n * Get sources that need to be scraped based on their frequency\r\n */\r\nexport async function getSourcesDueForScraping(): Promise<ScrapingSource[]> {\r\n  const { data, error } = await supabase\r\n    .from('scraping_sources')\r\n    .select('*')\r\n    .eq('is_active', true)\r\n    .or('last_scraped.is.null,last_scraped.lt.now() - scrape_frequency')\r\n    .order('trust_score', { ascending: false });\r\n  \r\n  if (error) {\r\n    console.error('Error fetching sources due for scraping:', error);\r\n    throw error;\r\n  }\r\n  \r\n  return data as ScrapingSource[];\r\n} ","import { zValidator } from \"@hono/zod-validator\";\r\nimport { createClient } from \"@supabase/supabase-js\";\r\nimport { Hono, Context } from \"hono\";\r\nimport { cors } from \"hono/cors\";\r\nimport { handle } from \"hono/vercel\";\r\nimport pLimit from \"p-limit\"; // Import p-limit\r\nimport { z } from \"zod\";\r\n\r\nimport { extractImmigrationData } from \"@/lib/ai/extract-immigration-data\"; // Assuming this path and function exist\r\nimport { summarizeText } from \"@/lib/ai/summarize\"; // Assuming this path and function exist\r\nimport { scrapeUrl as firecrawlScrape } from \"@/lib/firecrawl/client\"; // Assuming this path and function exist\r\nimport { ScraperService } from \"@/lib/scrapers/service\"; // Assuming service is in this path\r\nimport {\r\n  getSourcesDueForScraping,\r\n  recordScrape,\r\n  getScrapingSourceById,\r\n  createScrapingSource,\r\n  updateScrapingSource,\r\n  deleteScrapingSource,\r\n  getScrapingSources,\r\n  addSourceValidation, // <-- Add import for validation function\r\n} from \"@/lib/supabase/scraping-sources\"; // Assuming this path and functions exist\r\n\r\n\r\n// Import SourceCategory\r\nimport type { SourceCategory } from \"@/lib/supabase/scraping-sources\";\r\n\r\n// Define the Zod enum for SourceCategory\r\nconst sourceCategoryZodEnum = z.enum([\r\n  \"government\",\r\n  \"legal\",\r\n  \"news\",\r\n  \"blog\",\r\n  \"forum\",\r\n  \"other\",\r\n]);\r\n\r\n// Placeholder for SourceCategory enum - replace with actual import/definition\r\n// type SourceCategory = 'official' | 'news' | 'forum' | 'other'; // REMOVING PLACEHOLDER\r\n\r\n// Define types for Hono context variables\r\ntype HonoVariables = {\r\n  authType: \"apiKey\" | \"session\" | \"anonymous\";\r\n  session: any; // Replace 'any' with actual Session type from your auth provider\r\n  userId: string | undefined;\r\n};\r\n\r\n// Initialize Supabase client (use admin for operations requiring service role)\r\nconst supabaseAdmin = createClient(\r\n  process.env.NEXT_PUBLIC_SUPABASE_URL || \"\",\r\n  process.env.SUPABASE_SERVICE_ROLE_KEY || \"\",\r\n  { auth: { persistSession: false } } // Recommended for server-side admin client\r\n);\r\n\r\n// API Key for cron job authentication\r\nconst CRON_API_KEY = process.env.SCRAPING_API_KEY || \"your-secure-api-key\";\r\n\r\n// --- Schemas --- //\r\n\r\nconst scrapeOptionsSchema = z\r\n  .object({\r\n    waitForNetworkIdle: z.boolean().optional(),\r\n    extractLinks: z.boolean().optional(),\r\n    mobile: z.boolean().optional(),\r\n    waitForSelectors: z.array(z.string()).optional(),\r\n    extractSelectors: z.record(z.string()).optional(),\r\n    timeout: z.number().optional(),\r\n  })\r\n  .optional();\r\n\r\nconst scrapeSingleSchema = z.object({\r\n  url: z.string().url(),\r\n  options: scrapeOptionsSchema,\r\n  userId: z.string().optional(),\r\n  generateSummary: z.boolean().optional(),\r\n  extractData: z.boolean().optional(),\r\n});\r\n\r\nconst scrapeBulkSchema = z.object({\r\n  urls: z.array(z.string().url()),\r\n  options: scrapeOptionsSchema,\r\n  userId: z.string().optional(),\r\n  generateSummary: z.boolean().optional(),\r\n  extractData: z.boolean().optional(),\r\n});\r\n\r\nconst scheduleTriggerSchema = z.object({\r\n  sourceIds: z.array(z.string()).optional(),\r\n});\r\n\r\n// Use placeholder SourceCategory enum\r\n// const sourceCategoryEnum = z.enum(['official', 'news', 'forum', 'other']); // REMOVING PLACEHOLDER\r\n\r\nconst createSourceSchema = z.object({\r\n  name: z.string(),\r\n  url: z.string().url(),\r\n  category: sourceCategoryZodEnum, // Use the Zod enum\r\n  scrape_frequency: z.string(), // Consider enum: 'daily', 'weekly', etc.\r\n  // Add other relevant fields: description, config, country_code, etc.\r\n});\r\n\r\nconst updateSourceSchema = createSourceSchema.partial(); // Allow partial updates\r\n\r\nconst trackChangesSchema = z.object({\r\n  url: z.string().url(),\r\n  modes: z\r\n    .array(z.enum([\"git-diff\", \"json\"]))\r\n    .optional()\r\n    .default([\"git-diff\"]),\r\n  jsonSchema: z.record(z.any()).optional(),\r\n});\r\n\r\nconst sourceValidationSchema = z.object({\r\n  score: z.number().min(0).max(100),\r\n  notes: z.string().optional(),\r\n});\r\n\r\nconst triggerCountryScrapeSchema = z.object({\r\n  configId: z.string(), // Expecting the ID of the scrape_configurations entry\r\n});\r\n\r\n// --- ScraperService Initialization --- //\r\n// Ensure all required environment variables are set\r\nconst firecrawlApiKey = process.env.FIRECRAWL_API_KEY;\r\nconst supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;\r\nconst supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;\r\nconst r2Endpoint = process.env.R2_ENDPOINT;\r\nconst r2AccountId = process.env.R2_ACCOUNT_ID;\r\nconst r2AccessKeyId = process.env.R2_ACCESS_KEY_ID;\r\nconst r2SecretAccessKey = process.env.R2_SECRET_ACCESS_KEY;\r\nconst r2BucketName = process.env.R2_BUCKET_NAME;\r\n\r\nlet scraperServiceInstance: ScraperService | null = null;\r\n\r\nif (\r\n  firecrawlApiKey &&\r\n  supabaseUrl &&\r\n  supabaseServiceKey &&\r\n  r2Endpoint &&\r\n  r2AccountId &&\r\n  r2AccessKeyId &&\r\n  r2SecretAccessKey &&\r\n  r2BucketName\r\n) {\r\n  scraperServiceInstance = new ScraperService(\r\n    firecrawlApiKey,\r\n    supabaseUrl,\r\n    supabaseServiceKey,\r\n    r2Endpoint,\r\n    r2AccountId,\r\n    r2AccessKeyId,\r\n    r2SecretAccessKey,\r\n    r2BucketName\r\n  );\r\n  console.log(\"[Scraping API] ScraperService initialized successfully.\");\r\n} else {\r\n  console.error(\r\n    \"[Scraping API] Failed to initialize ScraperService: Missing one or more environment variables for Firecrawl, Supabase, or R2.\"\r\n  );\r\n  // Depending on how critical this is, you might throw an error or handle routes differently\r\n}\r\n\r\n// --- Hono App Setup --- //\r\n\r\n// Explicitly type the Hono app with the variables\r\nconst app = new Hono<{ Variables: HonoVariables }>().basePath(\"/api/scraping\");\r\n\r\n// --- Middleware --- //\r\napp.use(\"*\", cors());\r\n\r\n// Updated Authentication Middleware\r\nconst authMiddleware = async (\r\n  c: Context<{ Variables: HonoVariables }>,\r\n  next: () => Promise<void>\r\n) => {\r\n  let authType: HonoVariables[\"authType\"] = \"anonymous\";\r\n  let sessionData: HonoVariables[\"session\"] = null; // Renamed from 'session' to avoid conflict with c.session\r\n  let userId: HonoVariables[\"userId\"] = undefined;\r\n\r\n  const authHeader = c.req.header(\"Authorization\");\r\n  const apiKeyQuery = c.req.query(\"apiKey\");\r\n\r\n  // 1. Check for API key (for /schedule GET and POST)\r\n  if (\r\n    c.req.path === \"/schedule\" && // API Key primarily for /schedule\r\n    ((authHeader && authHeader === `Bearer ${CRON_API_KEY}`) ||\r\n      (apiKeyQuery && apiKeyQuery === CRON_API_KEY))\r\n  ) {\r\n    authType = \"apiKey\";\r\n    c.set(\"authType\", authType);\r\n    c.set(\"session\", null); // Explicitly set session to null for apiKey auth\r\n    c.set(\"userId\", undefined); // No userId for apiKey auth\r\n    console.log(\r\n      \"[Auth Middleware] API Key authentication successful for /schedule\"\r\n    );\r\n    await next();\r\n    return;\r\n  }\r\n\r\n  // 2. Check for user session (Bearer token)\r\n  if (authHeader && authHeader.startsWith(\"Bearer \")) {\r\n    const token = authHeader.split(\" \")[1];\r\n    if (token) {\r\n      try {\r\n        const {\r\n          data: { user },\r\n          error,\r\n        } = await supabaseAdmin.auth.getUser(token);\r\n        if (error) {\r\n          console.warn(\r\n            \"[Auth Middleware] Supabase token validation error:\",\r\n            error.message\r\n          );\r\n          // Potentially distinguish between expired/invalid token and other errors\r\n          // For now, treat as anonymous if token is invalid\r\n        } else if (user) {\r\n          authType = \"session\";\r\n          sessionData = user; // Store the Supabase user object as sessionData\r\n          userId = user.id;\r\n          console.log(\r\n            `[Auth Middleware] User session authenticated: ${userId}`\r\n          );\r\n        }\r\n      } catch (e) {\r\n        console.error(\r\n          \"[Auth Middleware] Exception during Supabase token validation:\",\r\n          e\r\n        );\r\n      }\r\n    } else {\r\n      console.warn(\"[Auth Middleware] Bearer token malformed or empty.\");\r\n    }\r\n  }\r\n\r\n  c.set(\"authType\", authType);\r\n  c.set(\"session\", sessionData); // Set the fetched user object (or null)\r\n  c.set(\"userId\", userId);\r\n\r\n  // Protect routes that require session authentication\r\n  // /admin/* routes and POST to /schedule require a valid user session.\r\n  const path = c.req.path;\r\n  const method = c.req.method;\r\n\r\n  const requiresSessionAuth =\r\n    path.startsWith(\"/admin\") || (path === \"/schedule\" && method === \"POST\");\r\n\r\n  if (requiresSessionAuth && authType !== \"session\") {\r\n    console.warn(\r\n      `[Auth Middleware] Unauthorized attempt to access ${method} ${path}`\r\n    );\r\n    return c.json({ error: \"Unauthorized: User session required\" }, 401);\r\n  }\r\n\r\n  // Role Check for Admin routes (example, needs actual role from sessionData)\r\n  // if (path.startsWith(\"/admin\") && sessionData?.app_metadata?.role !== 'admin') { // Adjust based on how roles are stored\r\n  //   return c.json({ error: \"Forbidden: Admin access required\" }, 403);\r\n  // }\r\n\r\n  await next();\r\n};\r\n\r\n// Apply auth middleware globally\r\napp.use(\"*\", authMiddleware);\r\n\r\n// --- Core Scraping Routes --- //\r\n\r\n// POST / - Scrape a single URL\r\napp.post(\"/\", zValidator(\"json\", scrapeSingleSchema), async (c) => {\r\n  const data = c.req.valid(\"json\");\r\n  const userId = c.get(\"userId\"); // Get userId set by middleware\r\n\r\n  console.log(\r\n    `[Scraping API] POST / request for URL: ${data.url}, User: ${userId || \"anonymous\"}`\r\n  );\r\n\r\n  try {\r\n    // 1. Call firecrawlScrape\r\n    const scrapedContent = await firecrawlScrape(data.url, data.options || {});\r\n    const result: any = {\r\n      url: data.url,\r\n      title: scrapedContent?.title || `Content from ${data.url}`,\r\n      content: scrapedContent?.content, // Or markdown, check firecrawl lib response\r\n      status: 200, // Assuming success if no error\r\n      contentType: \"text/html\", // Adjust if known\r\n    };\r\n\r\n    // 2. Optional: Summarize\r\n    if (data.generateSummary && result.content) {\r\n      try {\r\n        result.summary = await summarizeText(result.content, {\r\n          maxTokens: 500,\r\n        });\r\n      } catch (e) {\r\n        console.error(\"Summary failed:\", e);\r\n        result.summaryError = (e as Error).message;\r\n      }\r\n    }\r\n    // 3. Optional: Extract Immigration Data\r\n    if (data.extractData && result.content) {\r\n      try {\r\n        result.immigrationData = await extractImmigrationData(\r\n          result.content,\r\n          data.url\r\n        );\r\n      } catch (e) {\r\n        console.error(\"Extraction failed:\", e);\r\n        result.extractionError = (e as Error).message;\r\n      }\r\n    }\r\n    // 4. Optional: Save to `scraper_results` if authenticated user\r\n    if (userId) {\r\n      // Check if userId exists (i.e., authenticated)\r\n      try {\r\n        const { error } = await supabaseAdmin.from(\"scraper_results\").insert({\r\n          url: data.url,\r\n          user_id: userId,\r\n        });\r\n        if (error) throw error;\r\n        result.saved = true;\r\n      } catch (e) {\r\n        console.error(\"Failed to save scrape result:\", e);\r\n        result.saveError = (e as Error).message;\r\n      }\r\n    }\r\n    // 5. Log scrape to `scrape_queries`\r\n    try {\r\n      await supabaseAdmin.from(\"scrape_queries\").insert({\r\n        url: data.url,\r\n        user_id: userId,\r\n      });\r\n    } catch (e) {\r\n      console.error(\"Failed to log scrape query:\", e);\r\n    }\r\n\r\n    return c.json(result);\r\n  } catch (error) {\r\n    console.error(`[Scraping API] Error scraping URL ${data.url}:`, error);\r\n    return c.json(\r\n      { error: \"Failed to scrape URL\", details: (error as Error).message },\r\n      500\r\n    );\r\n  }\r\n});\r\n\r\n// POST /bulk - Scrape multiple URLs\r\napp.post(\"/bulk\", zValidator(\"json\", scrapeBulkSchema), async (c) => {\r\n  const data = c.req.valid(\"json\");\r\n  const userId = c.get(\"userId\");\r\n  const results: any[] = [];\r\n\r\n  console.log(\r\n    `[Scraping API] POST /bulk request for ${data.urls.length} URLs, User: ${userId || \"anonymous\"}`\r\n  );\r\n\r\n  const concurrencyLimit = 5; // Make this configurable if needed\r\n  const limit = pLimit(concurrencyLimit);\r\n\r\n  const scrapePromises = data.urls.map((url) =>\r\n    limit(async () => {\r\n      let singleResult: any = { url, success: false };\r\n      try {\r\n        const scrapedContent = await firecrawlScrape(url, data.options || {});\r\n        singleResult.title = scrapedContent?.title || `Content from ${url}`;\r\n        singleResult.content = scrapedContent?.content;\r\n\r\n        if (data.generateSummary && singleResult.content) {\r\n          try {\r\n            singleResult.summary = await summarizeText(singleResult.content);\r\n          } catch (e: any) {\r\n            singleResult.summaryError = e.message;\r\n          }\r\n        }\r\n        if (data.extractData && singleResult.content) {\r\n          try {\r\n            singleResult.immigrationData = await extractImmigrationData(\r\n              singleResult.content,\r\n              url\r\n            );\r\n          } catch (e: any) {\r\n            singleResult.extractionError = e.message;\r\n          }\r\n        }\r\n        if (userId) {\r\n          try {\r\n            await supabaseAdmin.from(\"scraper_results\").insert({\r\n              url: url,\r\n              user_id: userId,\r\n            });\r\n            // No need to set singleResult.saved = true here as it's per URL\r\n          } catch (e: any) {\r\n            singleResult.saveError = e.message; // Log save error for this specific URL\r\n            console.error(\r\n              `[Scraping API] Failed to save scrape result for ${url}:`,\r\n              e\r\n            );\r\n          }\r\n        }\r\n        try {\r\n          await supabaseAdmin.from(\"scrape_queries\").insert({\r\n            url: url,\r\n            user_id: userId,\r\n          });\r\n        } catch (e) {\r\n          console.error(\r\n            `[Scraping API] Failed to log bulk scrape query for ${url}:`,\r\n            e\r\n          );\r\n          // This error is not specific to singleResult, so not attaching it there\r\n        }\r\n        singleResult.success = true;\r\n      } catch (error: any) {\r\n        console.error(`[Scraping API] Error scraping bulk URL ${url}:`, error);\r\n        singleResult.error = error.message;\r\n      }\r\n      // The delay inside the loop is removed as p-limit handles concurrency.\r\n      // A small delay might still be useful if hammering the same domain,\r\n      // but p-limit primarily controls concurrent *outgoing* requests.\r\n      return singleResult; // Return the result for this URL\r\n    })\r\n  );\r\n\r\n  // Wait for all limited promises to resolve\r\n  const settledResults = await Promise.all(scrapePromises);\r\n  results.push(...settledResults); // Add all results to the main results array\r\n\r\n  return c.json({ results });\r\n});\r\n\r\n// POST /country/:configId - Trigger a specific country/config scrape\r\napp.post(\r\n  \"/country/:configId\",\r\n  zValidator(\"param\", z.object({ configId: z.string() })),\r\n  async (c) => {\r\n    const { configId } = c.req.valid(\"param\");\r\n    const userId = c.get(\"userId\");\r\n\r\n    if (!scraperServiceInstance) {\r\n      console.error(\r\n        \"[Scraping API] /country/:configId - ScraperService not initialized.\"\r\n      );\r\n      return c.json(\r\n        { error: \"ScraperService not available due to missing configuration.\" },\r\n        503\r\n      );\r\n    }\r\n\r\n    const session = c.get(\"session\");\r\n    const isAdmin =\r\n      session?.app_metadata?.claims?.app_role === \"admin\" ||\r\n      session?.user_metadata?.role === \"admin\";\r\n    if (!isAdmin) {\r\n      return c.json(\r\n        {\r\n          error: \"Forbidden: Admin access required to trigger country scrape.\",\r\n        },\r\n        403\r\n      );\r\n    }\r\n\r\n    console.log(\r\n      `[Scraping API] POST /country/${configId} request by User: ${userId || \"admin_trigger\"}`\r\n    );\r\n\r\n    try {\r\n      const config = await scraperServiceInstance.getScrapeConfigById(configId);\r\n      if (!config) {\r\n        console.warn(\r\n          `[Scraping API] /country/:configId - Configuration ${configId} not found.`\r\n        );\r\n        return c.json(\r\n          { error: `Configuration with ID ${configId} not found.` },\r\n          404\r\n        );\r\n      }\r\n\r\n      // Intentionally not awaiting processScrapeConfig to allow the API to respond quickly.\r\n      // The actual scraping will run in the background.\r\n      scraperServiceInstance\r\n        .processScrapeConfig(config)\r\n        .then(() => {\r\n          console.log(\r\n            `[Scraping API] Background processing of scrape config ${configId} completed.`\r\n          );\r\n        })\r\n        .catch((error) => {\r\n          console.error(\r\n            `[Scraping API] Background error processing scrape config ${configId}:`,\r\n            error\r\n          );\r\n          // Consider logging this to a more persistent error tracking system\r\n        });\r\n\r\n      return c.json({\r\n        message: `Scraping process for configuration ${configId} (${config.name}) initiated in background.`,\r\n      });\r\n    } catch (error: any) {\r\n      console.error(\r\n        `[Scraping API] Error initiating /country/${configId} scrape:`,\r\n        error\r\n      );\r\n      return c.json(\r\n        {\r\n          error: \"Failed to initiate country scrape process\",\r\n          details: error.message,\r\n        },\r\n        500\r\n      );\r\n    }\r\n  }\r\n);\r\n\r\n// POST /track-changes - Get change tracking data for a URL\r\napp.post(\r\n  \"/track-changes\",\r\n  zValidator(\"json\", trackChangesSchema),\r\n  async (c) => {\r\n    const { url, modes, jsonSchema } = c.req.valid(\"json\");\r\n    console.log(`[Scraping API] POST /track-changes for URL: ${url}`);\r\n\r\n    try {\r\n      // Set up options for Firecrawl scraping with change tracking\r\n      const options: Record<string, any> = {\r\n        pageOptions: {\r\n          // Nest under pageOptions if firecrawlScrape expects it like this\r\n          formats: [\"markdown\", \"changeTracking\"], // Include markdown to ensure scrape happens\r\n          changeTrackingOptions: {\r\n            modes,\r\n          },\r\n        },\r\n      };\r\n\r\n      // Add schema for JSON mode if provided\r\n      if (jsonSchema && modes.includes(\"json\")) {\r\n        if (!options.pageOptions.changeTrackingOptions)\r\n          options.pageOptions.changeTrackingOptions = {};\r\n        options.pageOptions.changeTrackingOptions.schema = jsonSchema;\r\n      }\r\n\r\n      // Perform the scraping with change tracking\r\n      // Assuming firecrawlScrape is the updated function handling this\r\n      const result = await firecrawlScrape(url, options);\r\n\r\n      // Check if we received change tracking data\r\n      if (!result || !result.changeTracking) {\r\n        return c.json(\r\n          {\r\n            success: false,\r\n            error:\r\n              \"Change tracking data not available. This might be the first time the URL has been scraped.\",\r\n          },\r\n          404\r\n        );\r\n      }\r\n\r\n      // Return the change tracking data\r\n      return c.json({\r\n        success: true,\r\n        url,\r\n        changeTracking: result.changeTracking,\r\n        timestamp: new Date().toISOString(),\r\n      });\r\n    } catch (error: any) {\r\n      console.error(`[Scraping API] Error tracking changes for ${url}:`, error);\r\n      return c.json(\r\n        {\r\n          success: false,\r\n          error: \"Failed to track changes\",\r\n          details: error.message,\r\n        },\r\n        500\r\n      );\r\n    }\r\n  }\r\n);\r\n\r\n// POST /extract - Placeholder (Logic moved to POST / and POST /track-changes)\r\napp.post(\"/extract\", async (c) => {\r\n  console.warn(\r\n    \"[Scraping API] /extract endpoint is deprecated. Use POST / and POST /track-changes.\"\r\n  );\r\n  return c.json(\r\n    { message: \"Endpoint deprecated. Use POST / and POST /track-changes.\" },\r\n    410\r\n  ); // 410 Gone\r\n});\r\n\r\n// --- Scheduled Scraping Route --- //\r\n\r\n// GET /schedule - Trigger scheduled scraping\r\napp.get(\"/schedule\", async (c) => {\r\n  if (c.get(\"authType\") !== \"apiKey\") {\r\n    return c.json({ error: \"Unauthorized: API Key required\" }, 401);\r\n  }\r\n  console.log(\"[Scraping API] GET /schedule triggered by API Key\");\r\n  try {\r\n    const result = await performScheduledScraping(); // Call helper\r\n    return c.json(result);\r\n  } catch (error) {\r\n    console.error(\"[Scraping API] Error during GET /schedule:\", error);\r\n    return c.json({ error: \"Error performing scheduled scraping\" }, 500);\r\n  }\r\n});\r\n\r\n// POST /schedule - Manually trigger scraping for specific sources\r\napp.post(\"/schedule\", zValidator(\"json\", scheduleTriggerSchema), async (c) => {\r\n  const { sourceIds } = c.req.valid(\"json\");\r\n  console.log(\r\n    `[Scraping API] POST /schedule triggered manually for sources: ${sourceIds?.join(\", \") || \"all due\"}`\r\n  );\r\n  try {\r\n    const result = await performScheduledScraping(sourceIds);\r\n    return c.json(result);\r\n  } catch (error) {\r\n    console.error(\"[Scraping API] Error during POST /schedule:\", error);\r\n    return c.json({ error: \"Error performing manual scraping\" }, 500);\r\n  }\r\n});\r\n\r\n// --- Admin Routes --- //\r\nconst adminApp = new Hono<{ Variables: HonoVariables }>(); // Ensure admin app also knows variables\r\n\r\n// Apply Admin Auth Middleware (Refined Check - assumes middleware already ran)\r\nadminApp.use(\"*\", async (c, next) => {\r\n  const authType = c.get(\"authType\");\r\n  const session = c.get(\"session\"); // This is the user object from Supabase\r\n  const userId = c.get(\"userId\");\r\n\r\n  // Ensure it's a session and user has an admin role\r\n  // Adjust 'session?.app_metadata?.claims?.app_role' to your actual user role path\r\n  const isAdmin =\r\n    session?.app_metadata?.claims?.app_role === \"admin\" ||\r\n    session?.user_metadata?.role === \"admin\"; // Example paths\r\n\r\n  if (authType !== \"session\" || !userId || !isAdmin) {\r\n    console.warn(\r\n      `[AdminApp Middleware] Unauthorized access attempt. AuthType: ${authType}, UserId: ${userId}, IsAdmin: ${isAdmin}`\r\n    );\r\n    return c.json({ error: \"Forbidden: Admin access required\" }, 403);\r\n  }\r\n  await next();\r\n});\r\n\r\n// GET /admin/sources - List sources\r\nadminApp.get(\"/sources\", async (c) => {\r\n  try {\r\n    const sources = await getScrapingSources();\r\n    return c.json(sources);\r\n  } catch (error) {\r\n    console.error(\"Admin GET Sources Error:\", error);\r\n    return c.json({ error: \"Failed to get sources\" }, 500);\r\n  }\r\n});\r\n\r\n// POST /admin/sources - Create source\r\nadminApp.post(\"/sources\", zValidator(\"json\", createSourceSchema), async (c) => {\r\n  const data = c.req.valid(\"json\");\r\n  const userId = c.get(\"userId\"); // Get userId from middleware\r\n  if (!userId) return c.json({ error: \"User ID not found in session\" }, 400);\r\n  try {\r\n    const createdSource = await createScrapingSource({\r\n      ...data,\r\n      created_by: userId,\r\n    });\r\n    return c.json(createdSource, 201);\r\n  } catch (error) {\r\n    console.error(\"Admin POST Source Error:\", error);\r\n    return c.json({ error: \"Failed to create source\" }, 500);\r\n  }\r\n});\r\n\r\n// GET /admin/sources/:id - Get single source\r\nadminApp.get(\"/sources/:id\", async (c) => {\r\n  const id = c.req.param(\"id\");\r\n  try {\r\n    const source = await getScrapingSourceById(id);\r\n    if (!source) return c.json({ error: \"Source not found\" }, 404);\r\n    return c.json(source);\r\n  } catch (error) {\r\n    console.error(\"Admin GET Source/:id Error:\", error);\r\n    return c.json({ error: \"Failed to get source\" }, 500);\r\n  }\r\n});\r\n\r\n// PUT /admin/sources/:id - Update source\r\nadminApp.put(\r\n  \"/sources/:id\",\r\n  zValidator(\"json\", updateSourceSchema),\r\n  async (c) => {\r\n    const id = c.req.param(\"id\");\r\n    const data = c.req.valid(\"json\");\r\n    try {\r\n      // Pass data directly as updateScrapingSource should handle Partial type\r\n      const updatedSource = await updateScrapingSource(id, data);\r\n      if (!updatedSource) return c.json({ error: \"Source not found\" }, 404);\r\n      return c.json(updatedSource);\r\n    } catch (error) {\r\n      console.error(\"Admin PUT Source/:id Error:\", error);\r\n      return c.json({ error: \"Failed to update source\" }, 500);\r\n    }\r\n  }\r\n);\r\n\r\n// DELETE /admin/sources/:id - Delete source\r\nadminApp.delete(\"/sources/:id\", async (c) => {\r\n  const id = c.req.param(\"id\");\r\n  try {\r\n    // Assume deleteScrapingSource throws if not found or error occurs\r\n    await deleteScrapingSource(id);\r\n    return c.json({ success: true });\r\n  } catch (error: any) {\r\n    console.error(\"Admin DELETE Source/:id Error:\", error);\r\n    // Check if error indicates not found (adjust based on actual function behavior)\r\n    if (error.message?.includes(\"not found\")) {\r\n      // Example check\r\n      return c.json({ error: \"Source not found\" }, 404);\r\n    }\r\n    return c.json({ error: \"Failed to delete source\" }, 500);\r\n  }\r\n});\r\n\r\n// POST /admin/sources/:id/validate - Add validation score/notes for a source\r\nadminApp.post(\r\n  \"/sources/:id/validate\",\r\n  zValidator(\"json\", sourceValidationSchema),\r\n  async (c) => {\r\n    const id = c.req.param(\"id\");\r\n    const { score, notes } = c.req.valid(\"json\");\r\n    const userId = c.get(\"userId\"); // Get admin user ID from middleware\r\n    const session = c.get(\"session\"); // Get session data\r\n\r\n    // Enhanced check for admin user\r\n    // This relies on your Supabase user object having a way to identify admins,\r\n    // e.g., via app_metadata.role or a separate 'roles' table.\r\n    // Placeholder: assuming 'admin' role is in app_metadata.claims.app_role or similar\r\n    // Adjust 'session?.app_metadata?.claims?.app_role' to your actual user role path\r\n    const isAdmin =\r\n      session?.app_metadata?.claims?.app_role === \"admin\" ||\r\n      session?.user_metadata?.role === \"admin\"; // Example paths\r\n\r\n    if (!userId || !isAdmin) {\r\n      // Also check if the user is an admin\r\n      console.warn(\r\n        `[Admin Validate] Unauthorized attempt by user: ${userId}, session:`,\r\n        session\r\n      );\r\n      return c.json({ error: \"Forbidden: Admin access required\" }, 403);\r\n    }\r\n\r\n    try {\r\n      // Verify source exists (optional, addSourceValidation might handle this)\r\n      const source = await getScrapingSourceById(id);\r\n      if (!source) {\r\n        return c.json({ error: \"Source not found\" }, 404);\r\n      }\r\n\r\n      // Add validation record\r\n      await addSourceValidation(id, score, userId, notes);\r\n      console.log(\r\n        `[Scraping Admin] Validation added for source ${id} by user ${userId}`\r\n      );\r\n      return c.json({ success: true });\r\n    } catch (error: any) {\r\n      console.error(`[Scraping Admin] Error validating source ${id}:`, error);\r\n      return c.json(\r\n        { error: \"Failed to add validation\", details: error.message },\r\n        500\r\n      );\r\n    }\r\n  }\r\n);\r\n\r\n// GET /admin/logs - Get logs\r\nadminApp.get(\"/logs\", async (c) => {\r\n  const page = parseInt(c.req.query(\"page\") || \"1\", 10);\r\n  const PAGE_SIZE = 20;\r\n  const offset = (page - 1) * PAGE_SIZE;\r\n  try {\r\n    const {\r\n      data: logs,\r\n      count,\r\n      error,\r\n    } = await supabaseAdmin\r\n      .from(\"scraping_logs\")\r\n      .select(\"*\", { count: \"exact\" })\r\n      .order(\"triggered_at\", { ascending: false })\r\n      .range(offset, offset + PAGE_SIZE - 1);\r\n    if (error) throw error;\r\n    const totalPages = Math.max(1, Math.ceil((count || 0) / PAGE_SIZE));\r\n    return c.json({ logs, page, totalPages, totalCount: count });\r\n  } catch (error) {\r\n    console.error(\"Admin GET Logs Error:\", error);\r\n    return c.json({ error: \"Failed to get logs\" }, 500);\r\n  }\r\n});\r\n\r\n// GET /admin/history - Get history\r\nadminApp.get(\"/history\", async (c) => {\r\n  const page = parseInt(c.req.query(\"page\") || \"1\", 10);\r\n  const sourceId = c.req.query(\"sourceId\");\r\n  const PAGE_SIZE = 20;\r\n  const offset = (page - 1) * PAGE_SIZE;\r\n  try {\r\n    let query = supabaseAdmin\r\n      .from(\"scrape_history\")\r\n      .select(`*, scraping_sources:source_id (name, url)`, { count: \"exact\" });\r\n    if (sourceId) query = query.eq(\"source_id\", sourceId);\r\n    const { data, count, error } = await query\r\n      .order(\"scraped_at\", { ascending: false })\r\n      .range(offset, offset + PAGE_SIZE - 1);\r\n    if (error) throw error;\r\n    const totalPages = Math.max(1, Math.ceil((count || 0) / PAGE_SIZE));\r\n    // Transform data (optional, maybe do on client)\r\n    // const history = data.map(...)\r\n    return c.json({ history: data, page, totalPages, totalCount: count });\r\n  } catch (error) {\r\n    console.error(\"Admin GET History Error:\", error);\r\n    return c.json({ error: \"Failed to get history\" }, 500);\r\n  }\r\n});\r\n\r\n// Mount the admin sub-app\r\napp.route(\"/admin\", adminApp);\r\n\r\n// --- Helper Functions (Implementation Required) --- //\r\n\r\nasync function performScheduledScraping(specificSourceIds?: string[]) {\r\n  console.log(\r\n    `[Scheduler] Performing scraping for sources: ${specificSourceIds ? specificSourceIds.join(\", \") : \"all due\"}`\r\n  );\r\n\r\n  let sourcesToScrape: any[] = []; // Ideally, this would be strongly typed based on getScrapingSourceById/getSourcesDueForScraping return type\r\n  let fetchError = null;\r\n\r\n  try {\r\n    if (specificSourceIds && specificSourceIds.length > 0) {\r\n      const sourcePromises = specificSourceIds.map((id) =>\r\n        getScrapingSourceById(id)\r\n      );\r\n      sourcesToScrape = (await Promise.all(sourcePromises)).filter(\r\n        (source) => source !== null\r\n      ) as any[]; // Type assertion after filter\r\n      console.log(\r\n        `[Scheduler] Fetched ${sourcesToScrape.length} specific sources by ID.`\r\n      );\r\n    } else {\r\n      sourcesToScrape = await getSourcesDueForScraping();\r\n      console.log(\r\n        `[Scheduler] Fetched ${sourcesToScrape.length} sources due for scraping.`\r\n      );\r\n    }\r\n  } catch (error: any) {\r\n    console.error(\"[Scheduler] Error fetching sources:\", error);\r\n    fetchError = error.message;\r\n    // Do not return here, proceed to report fetch error if any\r\n  }\r\n\r\n  const results = {\r\n    totalFetched: sourcesToScrape.length,\r\n    processed: 0,\r\n    successful: 0,\r\n    failed: 0,\r\n    fetchError: fetchError,\r\n    details: [] as Array<{\r\n      sourceId: string;\r\n      sourceName?: string;\r\n      url: string;\r\n      status: \"success\" | \"failure\" | \"skipped\";\r\n      message?: string;\r\n      summary?: string;\r\n      extractedData?: any;\r\n      error?: string;\r\n    }>,\r\n  };\r\n\r\n  if (fetchError && sourcesToScrape.length === 0) {\r\n    // If fetch failed and no sources were loaded (e.g. specific IDs not found and errored)\r\n    return {\r\n      message: \"Scheduled scraping could not fetch sources.\",\r\n      results,\r\n    };\r\n  }\r\n  if (sourcesToScrape.length === 0 && !fetchError) {\r\n    return {\r\n      message: \"No sources due for scraping or found for the given IDs.\",\r\n      results,\r\n    };\r\n  }\r\n\r\n  for (const source of sourcesToScrape) {\r\n    if (!source || !source.id || !source.url) {\r\n      console.warn(\"[Scheduler] Skipping invalid source object:\", source);\r\n      results.details.push({\r\n        sourceId: source?.id || \"unknown\",\r\n        sourceName: source?.name,\r\n        url: source?.url || \"unknown\",\r\n        status: \"skipped\",\r\n        message: \"Invalid source data provided.\",\r\n      });\r\n      continue;\r\n    }\r\n\r\n    results.processed++;\r\n    let scrapeDetail: (typeof results.details)[0] = {\r\n      sourceId: source.id,\r\n      sourceName: source.name,\r\n      url: source.url,\r\n      status: \"failure\", // Default to failure\r\n    };\r\n\r\n    try {\r\n      console.log(\r\n        `[Scheduler] Scraping URL: ${source.url} for source ID: ${source.id}`\r\n      );\r\n      // TODO: Incorporate source-specific scrape options if available (e.g., from source.config)\r\n      const scrapedContent = await firecrawlScrape(source.url, {\r\n        /* options */\r\n      });\r\n\r\n      if (scrapedContent && scrapedContent.content) {\r\n        scrapeDetail.message = \"Scraped successfully.\";\r\n\r\n        // Optional: Summarize (configurable per source in future?)\r\n        try {\r\n          scrapeDetail.summary = await summarizeText(scrapedContent.content, {\r\n            maxTokens: 200,\r\n          });\r\n        } catch (e: any) {\r\n          console.warn(\r\n            `[Scheduler] Summarization failed for ${source.url}:`,\r\n            e.message\r\n          );\r\n          scrapeDetail.summary = \"Summarization failed: \" + e.message;\r\n        }\r\n\r\n        // Optional: Extract Data (configurable per source in future?)\r\n        try {\r\n          scrapeDetail.extractedData = await extractImmigrationData(\r\n            scrapedContent.content,\r\n            source.url\r\n          );\r\n        } catch (e: any) {\r\n          console.warn(\r\n            `[Scheduler] Extraction failed for ${source.url}:`,\r\n            e.message\r\n          );\r\n          scrapeDetail.extractedData = {\r\n            error: \"Extraction failed: \" + e.message,\r\n          };\r\n        }\r\n\r\n        // TODO: Decide on artifact storage for scheduled scrapes if needed beyond logging\r\n        // e.g., save to a specific Supabase table or R2 bucket if content is significant.\r\n\r\n        // Pass a meaningful string to the 'content' parameter of recordScrape\r\n        const recordContent =\r\n          scrapeDetail.summary ||\r\n          scrapedContent.title ||\r\n          \"Scraped content available\";\r\n        await recordScrape(source.id, \"success\", recordContent);\r\n        scrapeDetail.status = \"success\";\r\n        results.successful++;\r\n      } else {\r\n        // If scrape was technically successful but no content was returned (or scrapedContent itself is nullish)\r\n        const errorMessage = scrapedContent\r\n          ? \"No content returned from scrape.\"\r\n          : \"Scraping did not return data.\";\r\n        console.warn(`[Scheduler] ${errorMessage} ${source.url}`);\r\n        scrapeDetail.message = errorMessage;\r\n        scrapeDetail.error = errorMessage;\r\n        await recordScrape(\r\n          source.id,\r\n          \"error\",\r\n          undefined,\r\n          undefined,\r\n          errorMessage\r\n        );\r\n        results.failed++;\r\n      }\r\n    } catch (error: any) {\r\n      console.error(\r\n        `[Scheduler] Error scraping ${source.url} (ID: ${source.id}):`,\r\n        error\r\n      );\r\n      scrapeDetail.error = error.message;\r\n      scrapeDetail.message = \"Scraping process threw an exception.\";\r\n      await recordScrape(\r\n        source.id,\r\n        \"error\",\r\n        undefined,\r\n        undefined,\r\n        error.message\r\n      );\r\n      results.failed++;\r\n    }\r\n    results.details.push(scrapeDetail);\r\n\r\n    // Add a small delay between requests to be respectful to servers\r\n    // This could be configurable per source in the future.\r\n    await new Promise((resolve) => setTimeout(resolve, 1000)); // 1 second delay\r\n  }\r\n\r\n  console.log(\r\n    `[Scheduler] Finished. Total: ${results.totalFetched}, Processed: ${results.processed}, Successful: ${results.successful}, Failed: ${results.failed}`\r\n  );\r\n  return {\r\n    message: `Scheduled scraping finished. Processed ${results.processed} of ${results.totalFetched} sources. Success: ${results.successful}, Failures: ${results.failed}.`,\r\n    results,\r\n  };\r\n}\r\n\r\n// --- Export Hono App --- //\r\nexport const GET = handle(app);\r\nexport const POST = handle(app);\r\nexport const PUT = handle(app);\r\nexport const DELETE = handle(app);\r\n\r\n// Optional: Specify runtime if needed (Node.js often better for scraping tasks)\r\nexport const runtime = \"nodejs\";\r\n","import * as origModule from 'next/dist/server/app-render/work-unit-async-storage.external.js';\nimport * as serverComponentModule from '__SENTRY_WRAPPING_TARGET_FILE__.cjs';\nexport * from '__SENTRY_WRAPPING_TARGET_FILE__.cjs';\nexport {} from '__SENTRY_WRAPPING_TARGET_FILE__.cjs';\nimport * as Sentry from '@sentry/nextjs';\n\n// @ts-expect-error Because we cannot be sure if the RequestAsyncStorage module exists (it is not part of the Next.js public\n// API) we use a shim if it doesn't exist. The logic for this is in the wrapping loader.\n\nconst asyncStorageModule = { ...origModule } ;\n\nconst requestAsyncStorage =\n  'workUnitAsyncStorage' in asyncStorageModule\n    ? asyncStorageModule.workUnitAsyncStorage\n    : 'requestAsyncStorage' in asyncStorageModule\n      ? asyncStorageModule.requestAsyncStorage\n      : undefined;\n\nfunction wrapHandler(handler, method) {\n  // Running the instrumentation code during the build phase will mark any function as \"dynamic\" because we're accessing\n  // the Request object. We do not want to turn handlers dynamic so we skip instrumentation in the build phase.\n  if (process.env.NEXT_PHASE === 'phase-production-build') {\n    return handler;\n  }\n\n  if (typeof handler !== 'function') {\n    return handler;\n  }\n\n  return new Proxy(handler, {\n    apply: (originalFunction, thisArg, args) => {\n      let headers = undefined;\n\n      // We try-catch here just in case the API around `requestAsyncStorage` changes unexpectedly since it is not public API\n      try {\n        const requestAsyncStore = requestAsyncStorage?.getStore() ;\n        headers = requestAsyncStore?.headers;\n      } catch (e) {\n        /** empty */\n      }\n\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      return Sentry.wrapRouteHandlerWithSentry(originalFunction , {\n        method,\n        parameterizedRoute: '/api/scraping',\n        headers,\n      }).apply(thisArg, args);\n    },\n  });\n}\n\n// eslint-disable-next-line @typescript-eslint/no-unsafe-member-access\nconst GET = wrapHandler(serverComponentModule.GET , 'GET');\n// eslint-disable-next-line @typescript-eslint/no-unsafe-member-access\nconst POST = wrapHandler(serverComponentModule.POST , 'POST');\n// eslint-disable-next-line @typescript-eslint/no-unsafe-member-access\nconst PUT = wrapHandler(serverComponentModule.PUT , 'PUT');\n// eslint-disable-next-line @typescript-eslint/no-unsafe-member-access\nconst PATCH = wrapHandler(serverComponentModule.PATCH , 'PATCH');\n// eslint-disable-next-line @typescript-eslint/no-unsafe-member-access\nconst DELETE = wrapHandler(serverComponentModule.DELETE , 'DELETE');\n// eslint-disable-next-line @typescript-eslint/no-unsafe-member-access\nconst HEAD = wrapHandler(serverComponentModule.HEAD , 'HEAD');\n// eslint-disable-next-line @typescript-eslint/no-unsafe-member-access\nconst OPTIONS = wrapHandler(serverComponentModule.OPTIONS , 'OPTIONS');\n\nexport { DELETE, GET, HEAD, OPTIONS, PATCH, POST, PUT };\n","import { AppRouteRouteModule } from \"next/dist/server/route-modules/app-route/module.compiled\";\nimport { RouteKind } from \"next/dist/server/route-kind\";\nimport { patchFetch as _patchFetch } from \"next/dist/server/lib/patch-fetch\";\nimport * as userland from \"E:\\\\downloads\\\\Hijraah\\\\apps\\\\web\\\\src\\\\app\\\\api\\\\scraping\\\\route.ts\";\n// We inject the nextConfigOutput here so that we can use them in the route\n// module.\nconst nextConfigOutput = \"\"\nconst routeModule = new AppRouteRouteModule({\n    definition: {\n        kind: RouteKind.APP_ROUTE,\n        page: \"/api/scraping/route\",\n        pathname: \"/api/scraping\",\n        filename: \"route\",\n        bundlePath: \"app/api/scraping/route\"\n    },\n    resolvedPagePath: \"E:\\\\downloads\\\\Hijraah\\\\apps\\\\web\\\\src\\\\app\\\\api\\\\scraping\\\\route.ts\",\n    nextConfigOutput,\n    userland\n});\n// Pull out the exports that we need to expose from the module. This should\n// be eliminated when we've moved the other routes to the new format. These\n// are used to hook into the route.\nconst { workAsyncStorage, workUnitAsyncStorage, serverHooks } = routeModule;\nfunction patchFetch() {\n    return _patchFetch({\n        workAsyncStorage,\n        workUnitAsyncStorage\n    });\n}\nexport { routeModule, workAsyncStorage, workUnitAsyncStorage, serverHooks, patchFetch,  };\n\n//# sourceMappingURL=app-route.js.map","module.exports = require(\"fs\");","module.exports = require(\"next/dist/server/app-render/work-async-storage.external.js\");","// src/middleware/cors/index.ts\nvar cors = (options) => {\n  const defaults = {\n    origin: \"*\",\n    allowMethods: [\"GET\", \"HEAD\", \"PUT\", \"POST\", \"DELETE\", \"PATCH\"],\n    allowHeaders: [],\n    exposeHeaders: []\n  };\n  const opts = {\n    ...defaults,\n    ...options\n  };\n  const findAllowOrigin = ((optsOrigin) => {\n    if (typeof optsOrigin === \"string\") {\n      if (optsOrigin === \"*\") {\n        return () => optsOrigin;\n      } else {\n        return (origin) => optsOrigin === origin ? origin : null;\n      }\n    } else if (typeof optsOrigin === \"function\") {\n      return optsOrigin;\n    } else {\n      return (origin) => optsOrigin.includes(origin) ? origin : null;\n    }\n  })(opts.origin);\n  const findAllowMethods = ((optsAllowMethods) => {\n    if (typeof optsAllowMethods === \"function\") {\n      return optsAllowMethods;\n    } else if (Array.isArray(optsAllowMethods)) {\n      return () => optsAllowMethods;\n    } else {\n      return () => [];\n    }\n  })(opts.allowMethods);\n  return async function cors2(c, next) {\n    function set(key, value) {\n      c.res.headers.set(key, value);\n    }\n    const allowOrigin = findAllowOrigin(c.req.header(\"origin\") || \"\", c);\n    if (allowOrigin) {\n      set(\"Access-Control-Allow-Origin\", allowOrigin);\n    }\n    if (opts.origin !== \"*\") {\n      const existingVary = c.req.header(\"Vary\");\n      if (existingVary) {\n        set(\"Vary\", existingVary);\n      } else {\n        set(\"Vary\", \"Origin\");\n      }\n    }\n    if (opts.credentials) {\n      set(\"Access-Control-Allow-Credentials\", \"true\");\n    }\n    if (opts.exposeHeaders?.length) {\n      set(\"Access-Control-Expose-Headers\", opts.exposeHeaders.join(\",\"));\n    }\n    if (c.req.method === \"OPTIONS\") {\n      if (opts.maxAge != null) {\n        set(\"Access-Control-Max-Age\", opts.maxAge.toString());\n      }\n      const allowMethods = findAllowMethods(c.req.header(\"origin\") || \"\", c);\n      if (allowMethods.length) {\n        set(\"Access-Control-Allow-Methods\", allowMethods.join(\",\"));\n      }\n      let headers = opts.allowHeaders;\n      if (!headers?.length) {\n        const requestHeaders = c.req.header(\"Access-Control-Request-Headers\");\n        if (requestHeaders) {\n          headers = requestHeaders.split(/\\s*,\\s*/);\n        }\n      }\n      if (headers?.length) {\n        set(\"Access-Control-Allow-Headers\", headers.join(\",\"));\n        c.res.headers.append(\"Vary\", \"Access-Control-Request-Headers\");\n      }\n      c.res.headers.delete(\"Content-Length\");\n      c.res.headers.delete(\"Content-Type\");\n      return new Response(null, {\n        headers: c.res.headers,\n        status: 204,\n        statusText: \"No Content\"\n      });\n    }\n    await next();\n  };\n};\nexport {\n  cors\n};\n","// src/adapter/vercel/handler.ts\nvar handle = (app) => (req) => {\n  return app.fetch(req);\n};\nexport {\n  handle\n};\n","module.exports = require(\"node:child_process\");","module.exports = require(\"path\");","module.exports = require(\"tls\");","module.exports = require(\"diagnostics_channel\");","module.exports = require(\"node:http\");","module.exports = require(\"node:stream/web\");","module.exports = require(\"node:zlib\");","module.exports = require(\"node:tls\");","module.exports = require(\"node:https\");","module.exports = require(\"next/dist/compiled/next-server/app-route.runtime.prod.js\");","module.exports = require(\"node:os\");","module.exports = require(\"node:diagnostics_channel\");","// src/http-exception.ts\nvar HTTPException = class extends Error {\n  res;\n  status;\n  constructor(status = 500, options) {\n    super(options?.message, { cause: options?.cause });\n    this.res = options?.res;\n    this.status = status;\n  }\n  getResponse() {\n    if (this.res) {\n      const newResponse = new Response(this.res.body, {\n        status: this.status,\n        headers: this.res.headers\n      });\n      return newResponse;\n    }\n    return new Response(this.message, {\n      status: this.status\n    });\n  }\n};\nexport {\n  HTTPException\n};\n","module.exports = require(\"crypto\");","module.exports = require(\"https\");","module.exports = require(\"node:stream\");","module.exports = require(\"node:util\");","/* (ignored) */","// src/utils/cookie.ts\nimport { decodeURIComponent_ } from \"./url.js\";\nvar algorithm = { name: \"HMAC\", hash: \"SHA-256\" };\nvar getCryptoKey = async (secret) => {\n  const secretBuf = typeof secret === \"string\" ? new TextEncoder().encode(secret) : secret;\n  return await crypto.subtle.importKey(\"raw\", secretBuf, algorithm, false, [\"sign\", \"verify\"]);\n};\nvar makeSignature = async (value, secret) => {\n  const key = await getCryptoKey(secret);\n  const signature = await crypto.subtle.sign(algorithm.name, key, new TextEncoder().encode(value));\n  return btoa(String.fromCharCode(...new Uint8Array(signature)));\n};\nvar verifySignature = async (base64Signature, value, secret) => {\n  try {\n    const signatureBinStr = atob(base64Signature);\n    const signature = new Uint8Array(signatureBinStr.length);\n    for (let i = 0, len = signatureBinStr.length; i < len; i++) {\n      signature[i] = signatureBinStr.charCodeAt(i);\n    }\n    return await crypto.subtle.verify(algorithm, secret, signature, new TextEncoder().encode(value));\n  } catch {\n    return false;\n  }\n};\nvar validCookieNameRegEx = /^[\\w!#$%&'*.^`|~+-]+$/;\nvar validCookieValueRegEx = /^[ !#-:<-[\\]-~]*$/;\nvar parse = (cookie, name) => {\n  if (name && cookie.indexOf(name) === -1) {\n    return {};\n  }\n  const pairs = cookie.trim().split(\";\");\n  const parsedCookie = {};\n  for (let pairStr of pairs) {\n    pairStr = pairStr.trim();\n    const valueStartPos = pairStr.indexOf(\"=\");\n    if (valueStartPos === -1) {\n      continue;\n    }\n    const cookieName = pairStr.substring(0, valueStartPos).trim();\n    if (name && name !== cookieName || !validCookieNameRegEx.test(cookieName)) {\n      continue;\n    }\n    let cookieValue = pairStr.substring(valueStartPos + 1).trim();\n    if (cookieValue.startsWith('\"') && cookieValue.endsWith('\"')) {\n      cookieValue = cookieValue.slice(1, -1);\n    }\n    if (validCookieValueRegEx.test(cookieValue)) {\n      parsedCookie[cookieName] = decodeURIComponent_(cookieValue);\n      if (name) {\n        break;\n      }\n    }\n  }\n  return parsedCookie;\n};\nvar parseSigned = async (cookie, secret, name) => {\n  const parsedCookie = {};\n  const secretKey = await getCryptoKey(secret);\n  for (const [key, value] of Object.entries(parse(cookie, name))) {\n    const signatureStartPos = value.lastIndexOf(\".\");\n    if (signatureStartPos < 1) {\n      continue;\n    }\n    const signedValue = value.substring(0, signatureStartPos);\n    const signature = value.substring(signatureStartPos + 1);\n    if (signature.length !== 44 || !signature.endsWith(\"=\")) {\n      continue;\n    }\n    const isVerified = await verifySignature(signature, signedValue, secretKey);\n    parsedCookie[key] = isVerified ? signedValue : false;\n  }\n  return parsedCookie;\n};\nvar _serialize = (name, value, opt = {}) => {\n  let cookie = `${name}=${value}`;\n  if (name.startsWith(\"__Secure-\") && !opt.secure) {\n    throw new Error(\"__Secure- Cookie must have Secure attributes\");\n  }\n  if (name.startsWith(\"__Host-\")) {\n    if (!opt.secure) {\n      throw new Error(\"__Host- Cookie must have Secure attributes\");\n    }\n    if (opt.path !== \"/\") {\n      throw new Error('__Host- Cookie must have Path attributes with \"/\"');\n    }\n    if (opt.domain) {\n      throw new Error(\"__Host- Cookie must not have Domain attributes\");\n    }\n  }\n  if (opt && typeof opt.maxAge === \"number\" && opt.maxAge >= 0) {\n    if (opt.maxAge > 3456e4) {\n      throw new Error(\n        \"Cookies Max-Age SHOULD NOT be greater than 400 days (34560000 seconds) in duration.\"\n      );\n    }\n    cookie += `; Max-Age=${opt.maxAge | 0}`;\n  }\n  if (opt.domain && opt.prefix !== \"host\") {\n    cookie += `; Domain=${opt.domain}`;\n  }\n  if (opt.path) {\n    cookie += `; Path=${opt.path}`;\n  }\n  if (opt.expires) {\n    if (opt.expires.getTime() - Date.now() > 3456e7) {\n      throw new Error(\n        \"Cookies Expires SHOULD NOT be greater than 400 days (34560000 seconds) in the future.\"\n      );\n    }\n    cookie += `; Expires=${opt.expires.toUTCString()}`;\n  }\n  if (opt.httpOnly) {\n    cookie += \"; HttpOnly\";\n  }\n  if (opt.secure) {\n    cookie += \"; Secure\";\n  }\n  if (opt.sameSite) {\n    cookie += `; SameSite=${opt.sameSite.charAt(0).toUpperCase() + opt.sameSite.slice(1)}`;\n  }\n  if (opt.priority) {\n    cookie += `; Priority=${opt.priority}`;\n  }\n  if (opt.partitioned) {\n    if (!opt.secure) {\n      throw new Error(\"Partitioned Cookie must have Secure attributes\");\n    }\n    cookie += \"; Partitioned\";\n  }\n  return cookie;\n};\nvar serialize = (name, value, opt) => {\n  value = encodeURIComponent(value);\n  return _serialize(name, value, opt);\n};\nvar serializeSigned = async (name, value, secret, opt = {}) => {\n  const signature = await makeSignature(value, secret);\n  value = `${value}.${signature}`;\n  value = encodeURIComponent(value);\n  return _serialize(name, value, opt);\n};\nexport {\n  parse,\n  parseSigned,\n  serialize,\n  serializeSigned\n};\n","// src/helper/cookie/index.ts\nimport { parse, parseSigned, serialize, serializeSigned } from \"../../utils/cookie.js\";\nvar getCookie = (c, key, prefix) => {\n  const cookie = c.req.raw.headers.get(\"Cookie\");\n  if (typeof key === \"string\") {\n    if (!cookie) {\n      return void 0;\n    }\n    let finalKey = key;\n    if (prefix === \"secure\") {\n      finalKey = \"__Secure-\" + key;\n    } else if (prefix === \"host\") {\n      finalKey = \"__Host-\" + key;\n    }\n    const obj2 = parse(cookie, finalKey);\n    return obj2[finalKey];\n  }\n  if (!cookie) {\n    return {};\n  }\n  const obj = parse(cookie);\n  return obj;\n};\nvar getSignedCookie = async (c, secret, key, prefix) => {\n  const cookie = c.req.raw.headers.get(\"Cookie\");\n  if (typeof key === \"string\") {\n    if (!cookie) {\n      return void 0;\n    }\n    let finalKey = key;\n    if (prefix === \"secure\") {\n      finalKey = \"__Secure-\" + key;\n    } else if (prefix === \"host\") {\n      finalKey = \"__Host-\" + key;\n    }\n    const obj2 = await parseSigned(cookie, secret, finalKey);\n    return obj2[finalKey];\n  }\n  if (!cookie) {\n    return {};\n  }\n  const obj = await parseSigned(cookie, secret);\n  return obj;\n};\nvar setCookie = (c, name, value, opt) => {\n  let cookie;\n  if (opt?.prefix === \"secure\") {\n    cookie = serialize(\"__Secure-\" + name, value, { path: \"/\", ...opt, secure: true });\n  } else if (opt?.prefix === \"host\") {\n    cookie = serialize(\"__Host-\" + name, value, {\n      ...opt,\n      path: \"/\",\n      secure: true,\n      domain: void 0\n    });\n  } else {\n    cookie = serialize(name, value, { path: \"/\", ...opt });\n  }\n  c.header(\"Set-Cookie\", cookie, { append: true });\n};\nvar setSignedCookie = async (c, name, value, secret, opt) => {\n  let cookie;\n  if (opt?.prefix === \"secure\") {\n    cookie = await serializeSigned(\"__Secure-\" + name, value, secret, {\n      path: \"/\",\n      ...opt,\n      secure: true\n    });\n  } else if (opt?.prefix === \"host\") {\n    cookie = await serializeSigned(\"__Host-\" + name, value, secret, {\n      ...opt,\n      path: \"/\",\n      secure: true,\n      domain: void 0\n    });\n  } else {\n    cookie = await serializeSigned(name, value, secret, { path: \"/\", ...opt });\n  }\n  c.header(\"set-cookie\", cookie, { append: true });\n};\nvar deleteCookie = (c, name, opt) => {\n  const deletedCookie = getCookie(c, name, opt?.prefix);\n  setCookie(c, name, \"\", { ...opt, maxAge: 0 });\n  return deletedCookie;\n};\nexport {\n  deleteCookie,\n  getCookie,\n  getSignedCookie,\n  setCookie,\n  setSignedCookie\n};\n","module.exports = require(\"next/dist/server/app-render/work-unit-async-storage.external.js\");","module.exports = require(\"node:fs\");","module.exports = require(\"worker_threads\");","module.exports = require(\"zlib\");","module.exports = require(\"perf_hooks\");","module.exports = require(\"node:worker_threads\");","module.exports = require(\"node:path\");","module.exports = require(\"node:net\");","module.exports = require(\"buffer\");","module.exports = require(\"url\");","module.exports = require(\"child_process\");","module.exports = require(\"node:readline\");","module.exports = require(\"http\");","module.exports = require(\"tty\");","module.exports = require(\"async_hooks\");","module.exports = require(\"node:inspector\");","module.exports = require(\"net\");","module.exports = require(\"events\");","function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = () => ([]);\nwebpackEmptyContext.resolve = webpackEmptyContext;\nwebpackEmptyContext.id = 96708;\nmodule.exports = webpackEmptyContext;","function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = () => ([]);\nwebpackEmptyContext.resolve = webpackEmptyContext;\nwebpackEmptyContext.id = 97108;\nmodule.exports = webpackEmptyContext;"],"names":["openai","OpenAI","apiKey","process","env","defaultImmigrationData","documentType","sourceType","extractedAt","Date","toISOString","extractImmigrationData","text","sourceUrl","determineSourceType","systemPrompt","content","response","chat","completions","create","model","messages","role","temperature","max_tokens","response_format","type","choices","message","extractedData","JSON","parse","error","console","warnings","url","lowercaseUrl","toLowerCase","includes","OPENAI_API_KEY","summarizeStuff","options","maxTokens","category","summarizeMapReduce","chunks","splitTextIntoChunks","chunkSize","i","length","breakPoint","paragraphBreak","lastIndexOf","sentenceBreak","push","substring","trim","combinedSummary","chunkSummaries","Promise","all","map","chunk","join","summarizeText","method","estimatedTokens","Math","ceil","scrapeUrl","success","data","metadata","title","timestamp","FirecrawlClient","constructor","baseUrl","config","fetch","headers","body","stringify","pageOptions","formats","changeTrackingOptions","modes","ok","errorData","json","catch","statusText","status","markdown","html","changeTracking","undefined","CountryConfig","z","name","paths","language","rateLimit","int","positive","selectors","lastUpdated","optional","listItem","nextPage","source_type","country_code","is_active","trackChanges","default","schema","ScraperService","firecrawlApiKey","supabaseUrl","supabaseServiceKey","r2Endpoint","r2AccountId","r2AccessKeyId","r2SecretAccessKey","r2BucketName","firecrawl","supabaseAdmin","createClient","auth","persistSession","r2Client","S3Client","region","endpoint","credentials","accessKeyId","secretAccessKey","uploadToR2","key","command","PutObjectCommand","Bucket","Key","Body","Buffer","from","ContentType","send","getActiveScrapeConfigs","select","eq","getScrapeConfigById","configId","single","code","warn","details","validationError","errorMessage","Error","String","processScrapeConfig","urlsToScrape","path","successCount","scrapeResult","contentHash","createHash","update","digest","existingSource","content_hash","last_scraped_at","urlObject","URL","r2Key","hostname","pathname","replace","upsertError","upsert","r2_object_key","autorag_indexed","onConflict","resolve","setTimeout","runAllActiveScrapers","activeConfigs","trackWebsiteChanges","scrapeConfig","origin","jsonSchema","result","previousScrapeAt","changeStatus","visibility","trackChangesForAllActiveConfigs","results","configResults","changeResult","supabaseKey","SUPABASE_SERVICE_ROLE_KEY","supabase","getScrapingSources","order","getScrapingSourceById","id","createScrapingSource","source","insert","updateScrapingSource","updates","created_at","updated_at","trust_score","validUpdates","deleteScrapingSource","delete","addSourceValidation","sourceId","score","validatorId","notes","source_id","validator_id","recordScrape","artifactId","changeSummary","hasChanges","crypto","lastScrape","ascending","limit","last_scraped","artifact_id","error_message","has_changes","change_summary","getSourcesDueForScraping","or","sourceCategoryZodEnum","CRON_API_KEY","SCRAPING_API_KEY","scrapeOptionsSchema","waitForNetworkIdle","extractLinks","mobile","waitForSelectors","extractSelectors","timeout","scrapeSingleSchema","userId","generateSummary","extractData","scrapeBulkSchema","urls","scheduleTriggerSchema","sourceIds","createSourceSchema","scrape_frequency","updateSourceSchema","partial","trackChangesSchema","sourceValidationSchema","min","max","R2_ENDPOINT","R2_ACCOUNT_ID","R2_ACCESS_KEY_ID","R2_SECRET_ACCESS_KEY","R2_BUCKET_NAME","scraperServiceInstance","app","Hono","basePath","use","cors","authMiddleware","c","next","authType","sessionData","authHeader","req","header","apiKeyQuery","query","set","startsWith","token","split","user","getUser","e","requiresSessionAuth","zValidator","valid","get","scrapedContent","firecrawlScrape","contentType","summary","summaryError","immigrationData","extractionError","user_id","saved","saveError","post","pLimit","scrapePromises","concurrencyLimit","singleResult","settledResults","session","isAdmin","app_metadata","claims","app_role","user_metadata","then","performScheduledScraping","adminApp","specificSourceIds","sourcesToScrape","fetchError","sourcePromises","filter","totalFetched","processed","successful","failed","sourceName","scrapeDetail","recordContent","sources","createdSource","created_by","param","put","updatedSource","page","parseInt","offset","PAGE_SIZE","logs","count","range","totalPages","totalCount","history","route","handle","POST","PUT","DELETE","runtime","serverComponentModule.GET","serverComponentModule.POST","serverComponentModule.PUT","serverComponentModule.PATCH","serverComponentModule.HEAD","serverComponentModule.OPTIONS"],"sourceRoot":""}