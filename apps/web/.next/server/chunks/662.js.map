{"version":3,"file":"662.js","mappings":"+fAeO,SAASA,EACdC,EAAqB,MAAM,EAE3B,IAAMC,EAAMC,wBAAoC,CAChD,GADoDA,CAAwB,EAE1E,GADQ,GACF,MACJ,0EAIJ,IAAMC,EACK,YAATH,EACIE,QAAQE,GAAG,CAACC,oBAAoB,EAChCH,QAAQE,GAAG,CAACE,yBAAyB,CACrCJ,QAAQE,GAAG,CAACG,iBAAiB,EAC7BL,2JAAyC,CAE/C,GAAI,CAACC,EACH,GADQ,GACF,MACJ,CAAC,SAAS,EAAEH,EAAK,6CAA6C,CAAC,EAInE,MAAOQ,CAAAA,EAAAA,EAAAA,YAAAA,CAAsBA,CAACP,EAAKE,EAAK,CACtCM,KAAM,CACJC,kBAAkB,EAClBC,gBAAgB,CAClB,CACF,EACF,CCpCO,MAAMC,EAIXC,YAAYC,CAAc,CAAE,MAHpBC,QAAAA,CAAWhB,EAAqB,WAItC,IAAI,CAACe,EAJgCf,IAI1B,CAAGe,CAChB,CAEA,MAAME,gBACJC,CAAc,CACdC,EAAQ,CAAC,CACsB,CAC/B,GAAM,MAAEC,CAAI,OAAEC,CAAK,CAAE,CAAG,MAAM,IAAI,CAACL,QAAQ,CACxCM,IAAI,CAAC,sBACLC,MAAM,CAAC,cACPC,EAAE,CAAC,UAAWN,GACdO,KAAK,CAAC,aAAc,CAAEC,WAAW,CAAM,GACvCP,KAAK,CAAC,IAET,GAAIE,GAAS,CAACD,EAAM,MAAO,EAAE,CAE7B,IAAMO,EAAWP,EAAKQ,GAAG,CAAC,GAAOC,EAAEC,UAAU,EAAEC,IAAI,CAAC,MAE9CC,EAAW,MAAM,IAAI,CAACjB,MAAM,CAACkB,IAAI,CAACC,WAAW,CAACC,MAAM,CAAC,CACzDC,MAAO,cACPC,SAAU,CACR,CACEpC,KAAM,SACNqC,QACE,iLACJ,EACA,CAAErC,KAAM,OAAQqC,QAASX,EAASY,KAAK,CAAC,EAAG,IAAM,EAClD,CACDC,gBAAiB,CAAEC,KAAM,aAAc,CACzC,GAEA,GAAI,CACF,OAAOC,KAAKC,KAAK,CAACX,EAASY,OAAO,CAAC,EAAE,CAACC,OAAO,CAACP,OAAO,EAAI,KAC3D,CAAE,KAAM,CACN,MAAO,EAAE,CAEb,CACF,CC3CO,MAAMQ,EAGX,MAAMC,eAAe7B,CAAc,CAA+B,CAChE,GAAM,MAAEE,CAAI,OAAEC,CAAK,CAAE,CAAG,MAAM,IAAI,CAACL,QAAQ,CACxCM,IAAI,CAAC,sBACLC,MAAM,CAAC,iCACPC,EAAE,CAAC,UAAWN,GACd8B,EAAE,CAAC,iBAAkB,GAExB,GAAI3B,GAAS,CAACD,EAAM,MAAO,EAAE,CAE7B,IAAM6B,EAAqC,CAAC,EAW5C,OATA7B,EAAK8B,OAAO,CAAC,IAEXC,CADcC,EAAIC,aAAa,EAAEC,MAAM,QAAU,IAC3CJ,OAAO,CAAC,IACZ,IAAM9C,EAAMmD,EAAEC,WAAW,GACrBpD,EAAIqD,MAAM,CAAG,GAAG,CACpBR,CAAU,CAAC7C,EAAI,CAAG,CAAC6C,CAAU,CAAC7C,EAAI,GAAI,GAAK,CAC7C,EACF,GAEOsD,OAAOC,OAAO,CAACV,GACnBrB,GAAG,CAAC,CAAC,CAACgC,EAAGC,EAAE,GAAM,EAAEC,QAASF,EAAGG,OAAQC,KAAKC,GAAG,CAACJ,EAAG,GAAG,GACtDK,IAAI,CAAC,CAACC,EAAGC,IAAMA,EAAEL,MAAM,CAAGI,EAAEJ,MAAM,CACvC,oBAzBQ/C,QAAAA,CAAWhB,EAAqB,WA0B1C,CC9BO,MDIkCA,ECAvCc,YAAYC,CAAc,CAAE,MAHpBC,QAAAA,CAAWhB,EAAqB,WAItC,IAAI,CAACe,EAJgCf,IAI1B,CAAGe,CAChB,CAEA,MAAMsD,SAASC,CAAa,CAAEnD,EAAQ,CAAC,CAA6B,CAElE,IAAIoD,EAAsB,EAAE,CAE5B,GAAI,CACFA,EAAY,OACJ,IAAI,CAACxD,MAAM,CAACyD,UAAU,CAACrC,MAAM,CAAC,CAClCC,MAAO,yBACPqC,MAAOH,CACT,GAAC,CACDlD,IAAI,CAAC,EAAE,CAACmD,SAAS,CACnB,MAAOG,EAAU,CACjB,KACEA,GAAKC,OAAS,mBACd,2BAA2BC,IAAI,CAACF,GAAK7B,SAAW,KAchD,CAbA,MAYAgC,QAAQxD,KAAK,CAAC,8BAA+BqD,GACtC,EAAE,CAXTG,QAAQC,IAAI,CACV,4EAEFP,EAAY,CACV,MAAM,IAAI,CAACxD,MAAM,CAACyD,UAAU,CAACrC,MAAM,CAAC,CAClCC,MAAO,yBACPqC,MAAOH,CACT,GAAC,CACDlD,IAAI,CAAC,EAAE,CAACmD,SAAS,CAQvB,GAAM,MAAEnD,CAAI,OAAEC,CAAK,CAAE,CAAG,MAAM,IAAI,CAACL,QAAQ,CAAC+D,GAAG,CAAC,yBAA0B,CACxEC,kBAAmBT,EACnBU,cAAe9D,CACjB,UAEA,GACE0D,IADS,IACDxD,KAAK,CAAC,qBAAsBA,EAAMwB,OAAO,EAC1C,EAAE,EAGJ,CAACzB,GAAQ,IAAIQ,GAAG,CAAC,GAAe,EACrC1B,CADqC,GAChCkD,EAAI8B,SAAS,CAClBC,SAAU/B,EAAI+B,QAAQ,EAAI,CAAC,EAC3BC,MAAOhC,EAAIiC,UAAU,CACvB,EACF,CACF,8CCbO,OAAMC,EAOXxE,YAAYyE,CAA8B,CAAEC,CAAoB,CAAE,CAChE,IAAI,CAACxE,QAAQ,CAAGuE,EAChB,IAAI,CAACxE,MAAM,CAAGyE,EAGd,IAAI,CAACC,UAAU,CAAG,GAAIC,CAAAA,GAAAA,EAASA,CAAGC,OAAQ,GAAI,GAG9C,IAAI,CAACC,WAAW,CAAG,IAAIC,EAAAA,EAAKA,CAAC,CAC3B3F,IAAKC,QAAQE,GAAG,CAACyF,kBAAkB,CACnCC,MAAO5F,QAAQE,GAAG,CAAC2F,oBAAoB,GAGzC,IAAI,CAACC,KAAK,CAAG,IAAIC,EAAAA,CAAKA,CAAC,CACrBhG,IAAKC,QAAQE,GAAG,CAAC8F,iBAAiB,CAClCJ,MAAO5F,QAAQE,GAAG,CAAC+F,mBACrB,EACF,CAEA,MAAMC,cAAcC,CAA8B,CAAiB,CACjE,GAAM,CAAEjF,MAAOkF,CAAQ,CAAE,CAAG,MAAM,IAAI,CAACvF,QAAQ,CAC5CM,IAAI,CAAC,iBACLkF,MAAM,CACL,CACEC,GAAIH,EAASI,UAAU,CACvBC,WAAYL,EAASM,SAAS,CAC9BC,SAAUP,EAASQ,OAAO,CAC1BC,OAAQ,WACV,EACA,CAAEC,WAAY,IAAK,GAGvB,GAAIT,EACF,MAAM,EADM,IACI,CAAC,iCAAiC,EAAEA,EAAS1D,OAAO,EAAE,EAGxE,IAAMoE,EAAiBX,EAASY,MAAM,CAACtF,GAAG,CAAC,GAAY,EACrD6E,GADqD,CACjD,EAAGH,EAASI,UAAU,CAAC,CAAC,EAAES,EAAMhC,QAAQ,CAACiC,UAAU,EAAE,CACzDC,YAAaf,EAASI,UAAU,CAChCpE,QAAS6E,EAAM7E,OAAO,CACtBiC,UAAW4C,EAAM5C,SAAS,CAC1B+C,YAAaH,EAAMhC,QAAQ,CAACiC,UAAU,CACtCjC,SAAUgC,EAAMhC,QAAQ,CAC1B,GAGA,IAAK,IAAIoC,EAAI,EAAGA,EAAIN,EAAexD,MAAM,CAAE8D,KAAKC,EAAW,CACzD,IAAMC,EAAQR,EAAe1E,KAAK,CAACgF,EAAGA,EAFtB,EAE0BC,GACpC,CAAEnG,MAAOqG,CAAU,CAAE,CAAG,MAAM,IAAI,CAAC1G,QAAQ,CAC9CM,IAAI,CAAC,4BACLqG,MAAM,CAACF,GACV,GAAIC,EACF,MAAM,IADQ,EACE,CAAC,+BAA+B,EAAEA,EAAW7E,OAAO,EAAE,CAE1E,CACF,CAEA,MAAc+E,eAAe1G,CAAc,CAA+B,CACxE,GAAM,MAAEE,CAAI,OAAEC,CAAK,CAAE,CAAG,MAAM,IAAI,CAACL,QAAQ,CACxCM,IAAI,CAAC,YACLC,MAAM,CACL,4FAEDC,EAAE,CAAC,KAAMN,GACT2G,MAAM,UAET,GACqB,IADV,QACsB,CAA3BxG,EAAMsD,IAAI,EAKdE,QAAQxD,KAAK,CAAC,CAAC,kCAAkC,EAAEH,EAAO,CAAC,CAAC,CAAEG,GAFrD,MAMX,EAKO,CACLoF,CANE,EAAO,EAMAA,EAAE,CACXqB,mBAAoB1G,EAAK2G,oBAAoB,CAC7CC,kBAAmB5G,EAAK6G,mBAAmB,CAC3CC,qBAAsB9G,EAAK+G,sBAAsB,CACjDC,iBAAkBhH,EAAKiH,iBAAiB,EAAI,EAAE,EATvC,IAWX,CAEA,MAAcC,gBACZhE,CAAa,CACbiE,CAAgC,CACN,CAC1B,IAAMC,EAAW,CAAC,iBAAiB,EACjCD,GAAa9B,IAAM,YACpB,CAAC,EAAEnC,EAAAA,CAAO,CACLmE,EAAe,IAAI,CAAChD,UAAU,CAACiD,GAAG,CAAkBF,GAE1D,GAAIC,EAEF,OAAOA,EAGT,GALkB,CAKZE,EAAiBJ,EACnB,CAAC;;sCAE6B,EAAEA,EAAYT,kBAAkB,EAAI,MAAM;gCAChD,EAAES,EAAYL,oBAAoB,EAAI,MAAM;6CAC/B,EACrCK,EAAYP,iBAAiB,EAAI,MAClC;kCAC2B,EAC1BO,EAAYH,gBAAgB,EAAErG,KAAK,OAAS,MAC7C;;oJAE6I,CAAC,CAC7I,mDAEE6G,EAAS,CAAC;;MAEd,EAAED,eAAe;;mBAEJ,EAAErE,EAAM;;;;;;IAMvB,CAAC,CAED,GAAI,CACF,IAAMtC,EAAW,MAAM,IAAI,CAACjB,MAAM,CAACkB,IAAI,CAACC,WAAW,CAACC,MAAM,CAAC,CACzDC,MAAO,SACPC,SAAU,CACR,CACEpC,KAAM,SACNqC,QACE,wFACJ,EACA,CAAErC,KAAM,OAAQqC,QAASsG,CAAO,EACjC,CACDpG,gBAAiB,CAAEC,KAAM,aAAc,CACzC,GAEMH,EAAUN,EAASY,OAAO,CAAC,EAAE,EAAEC,SAASP,QAC9C,GAAI,CAACA,EAAS,MAAM,MAAU,oCAC9B,IAAMuG,EAAgBnG,KAAKC,KAAK,CAACL,GAE3BwG,EAA0B,CAC9BC,gBAAiBF,EAAcE,eAAe,EAAIzE,EAClD0E,gBACEH,EAAcG,eAAe,EAAI1E,EAAMhB,KAAK,CAAC,KAAKvB,IAAI,CAAC,MAC3D,EAGA,OADA,IAAI,CAAC0D,UAAU,CAACwD,GAAG,CAACT,EAAUM,GACvBA,CACT,CAAE,MAAOzH,EAAO,CAKd,OAJAwD,QAAQC,IAAI,CACV,2DACAzD,GAEK,CACL0H,gBAAiBzE,EACjB0E,gBAAiB1E,EAAMhB,KAAK,CAAC,KAAKvB,IAAI,CAAC,MACzC,CACF,CACF,CAEA,MAAcmH,yBACZ5E,CAAa,CAC8B,CAC3C,GAAI,CACF,IAAMtC,EAAW,MAAM,IAAI,CAACjB,MAAM,CAACkB,IAAI,CAACC,WAAW,CAACC,MAAM,CAAC,CACzDC,MAAO,SACPC,SAAU,CACR,CACEpC,KAAM,SACNqC,QACE,+HACJ,EACA,CACErC,KAAM,OACNqC,QAAS,CAAC,+EAA+E,EAAEgC,EAAM;;;iHAGI,CAAC,EAEzG,CACD9B,gBAAiB,CAAEC,KAAM,aAAc,CACzC,GAEMH,EAAUN,EAASY,OAAO,CAAC,EAAE,EAAEC,SAASP,QAC9C,GAAI,CAACA,EAAS,MAAO,EAAE,CAGvB,OADeI,KAAKC,KAAK,CAACL,GACZ6G,QAAQ,EAAI,EAAE,CAC5B,MAAO9H,EAAO,CAEd,OADAwD,QAAQxD,KAAK,CAAC,wCAAyCA,GAChD,EAAE,CAEb,CAEA,MAAc+H,qBACZD,CAA0C,CACtB,CACpB,GAAwB,GAAG,CAAvBA,EAAS1F,MAAM,CACjB,MAAO,CAAE0F,SAAU,EAAE,CAAEE,cAAe,EAAE,EAG1C,IAAMC,EAAcH,EAASvH,GAAG,CAAC,GAAO2H,EAAEC,IAAI,EAE9C,GAAI,CAEF,GAAM,MAAEpI,CAAI,CAAEC,OAAK,CAAE,CAAG,MAAM,IAAI,CAACL,QAAQ,CAAC+D,GAAG,CAAC,oBAAqB,CACnE0E,eAAgBH,CAClB,GAEA,GAAIjI,EAEF,KAFS,EACTwD,QAAQxD,KAAK,CAAC,mCAAoCA,GAC3C,UAAE8H,EAAUE,cAAe,EAAE,EAGtC,IAAMA,EAAgB,CAACjI,GAAQ,IAAIQ,GAAG,CAAC,GAAe,EACpD8H,CADoD,kBAChCtG,EAAIsG,kBAAkB,EAAI,GAC9CC,mBAAoBvG,EAAIuG,kBAAkB,EAAI,GAC9ClH,KAAMW,EAAIwG,iBAAiB,CAC7B,GAEA,MAAO,UACLT,EACAE,eACF,CACF,CAAE,MAAO3E,EAAK,CAEZ,OADAG,QAAQxD,KAAK,CAAC,6BAA8BqD,GACrC,UAAEyE,EAAUE,cAAe,EAAE,CACtC,CACF,CAKA,MAAcQ,mBACZC,CAAiB,CACgB,CACjC,GAAM,MAAE1I,CAAI,OAAEC,CAAK,CAAE,CAAG,MAAM,IAAI,CAACL,QAAQ,CACxCM,IAAI,CAAC,mBACLC,MAAM,CAAC,iCACPC,EAAE,CAAC,aAAcsI,GACjBC,GAAG,CAAC,aAAc,IAAIC,OAAOC,WAAW,IACxCC,WAAW,GAEd,GAAI7I,EAEF,KAFS,EACTwD,QAAQC,IAAI,CAAC,sBAAuBzD,EAAMwB,OAAO,EAC1C,KAET,GAAI,CAACzB,EAAM,OAAO,KAElB,GAAI,CACF,IAAM8F,EAASxE,KAAKC,KAAK,CACvBvB,EAAK+I,gBAAgB,EAEjBC,EAAY1H,KAAKC,KAAK,CAACvB,EAAKiJ,WAAW,EAC7C,MAAO,QAAEnD,YAAQkD,CAAU,CAC7B,CAAE,KAAM,CACN,OAAO,IACT,CACF,CAKA,UAAkB9F,CAAa,CAAEgG,CAAqB,CAAU,CAC9D,IAAMC,EAAU7H,KAAK8H,SAAS,CAAC,OAAElG,EAAO,GAAGgG,CAAO,GAClD,OAAOG,IAAAA,UAAiB,CAAC,UAAUC,MAAM,CAACH,GAASI,MAAM,CAAC,MAC5D,CAEA,MAAcC,eAAed,CAAiB,CAAEhB,CAAuB,CAAE,CACvE,GAAI,CAEF,MAAM,IAAI,CAAC9H,QAAQ,CAACM,IAAI,CAAC,mBAAmBkF,MAAM,CAAC,CACjDqE,WAAYf,EACZhI,WAAY,GACZqI,iBAAkBzH,KAAK8H,SAAS,CAAC1B,EAAO5B,MAAM,EAC9CmD,YAAa3H,KAAK8H,SAAS,CAAC1B,EAAOsB,SAAS,EAC5CU,WAAY,IAAId,KAAKA,KAAKe,GAAG,GAAK,MAAgBd,CAAT,KAAK,KAAe,EAC/D,GAGA,MAAM,IAAI,CAAChE,KAAK,CAAC+E,KAAK,CACpB,CAAC,OAAO,EAAElB,EAAAA,CAAW,CACrB,KACApH,KAAK8H,SAAS,CAAC1B,GAEnB,CAAE,MAAOpE,EAAK,CACZG,QAAQC,IAAI,CAAC,oBAAqBJ,EACpC,CACF,CAGA,MAAMuG,OACJ3G,CAAa,CACbgG,EAAwB,CAAC,CAAC,CACA,CACRN,KAAKe,GAAG,GAC1B,IAAMjB,EAAY,IAAI,CAACoB,SAAS,CAAC5G,EAAOgG,GAExC,GAAI,CAEF,IAAMa,EAAW,CAAC,OAAO,EAAErB,EAAAA,CAAW,CAChCsB,EAAc,MAAM,IAAI,CAACnF,KAAK,CAACyC,GAAG,CAACyC,GACzC,GAAIC,EAGF,OAAOC,IAHQ,CAEK1I,KAAK,CAACyI,GAK5B,IAAMC,EAAS,MAAM,IAAI,CAACxB,kBAAkB,CAACC,GAC7C,GAAIuB,EAGF,MAHU,CAEV,MAAM,IAAI,CAACpF,KAAK,CAAC+E,KAAK,CAACG,EAAU,KAAMzI,KAAK8H,SAAS,CAACa,IAC/CA,EAIT,GAAM,OAAElK,EAAQ,EAAE,WAAEmK,EAAY,EAAG,CAAEpK,QAAM,CAAE,CAAGoJ,EAC5C/B,EAAkC,KAClCrH,IACFqH,EAAc,EADJ,IACU,IAAI,CAACX,cAAc,CAAC1G,EAAAA,EAG1C,GAAM,iBAAE6H,CAAe,CAAE,CAAG,MAAM,IAAI,CAACT,eAAe,CACpDhE,EACAiE,GAEIgD,EAAiB,OACf,IAAI,CAACxK,MAAM,CAACyD,UAAU,CAACrC,MAAM,CAAC,CAClCC,MAAO,yBACPqC,MAAOsE,CACT,GAAC,CACD3H,IAAI,CAAC,EAAE,CAACmD,SAAS,CAEnB,GAAI,CACF,IAAMiH,EAAUtK,EACZ,IAAI,CAAC0E,WAAW,CAAC6F,SAAS,CAAC,CAAC,KAAK,EAAEvK,EAAAA,CAAQ,EAC3C,IAAI,CAAC0E,WAAW,CAEd8F,EAAqB,MAAMF,EAAQlH,KAAK,CAAC,CAC7CqH,OAAQJ,EACRK,KAAM,EACNC,iBAAiB,CACnB,GAEA,GACEH,EAAmBjI,MAAM,CAAG,GAC5BiI,CAAkB,CAAC,EAAE,CAACtG,KAAK,CAAG,IAC9B,CAEA,IAAMqD,EAAeiD,CAAkB,CAAC,EAAE,CAACvG,QAAQ,CACnD,GAAIsD,GAAcK,OAChB,CADwB,MACjBL,EAAaK,MAAM,CAGhC,CAAE,MAAOgD,EAAkB,CACzBjH,QAAQC,IAAI,CAAC,6BAA8BgH,EAC7C,CAIA,IAAMhD,EAAS,MAAM,IAAI,CAACiD,mBAAmB,CAACzH,EAAOgG,EAGrD,OAAM,IAAI,CAACM,cAAc,CAACd,EAAWhB,GAGrC,IAAM0C,EAAUtK,EACZ,IAAI,CAAC0E,WAAW,CAAC6F,SAAS,CAAC,CAAC,KAAK,EAAEvK,EAAAA,CAAQ,EAC3C,IAAI,CAAC0E,WAAW,CAYpB,OAVA,MAAM4F,EAAQhF,MAAM,CAAC,CACnBC,GAAIqD,EACJ6B,OAAQJ,EACRpG,SAAU,OACRb,SACAwE,EACAkD,SAAUhC,KAAKe,GAAG,EACpB,CACF,GAEOjC,CACT,CAAE,MAAOzH,EAAO,CAEd,MADAwD,QAAQxD,KAAK,CAAC,oCAAqCA,GAC7CA,CACR,CACF,CAGA,MAAc0K,oBACZzH,CAAa,CACbgG,EAAwB,CAAC,CAAC,CACA,CAC1B,GAAM,OAAEnJ,EAAQ,EAAE,CAAEmK,YAAY,EAAG,CAAEpK,QAAM,CAAE,CAAGoJ,EAE5C/B,EAAkC,KAClCrH,IACFqH,EAAc,EADJ,IACU,IAAI,CAACX,cAAc,CAAC1G,EAAAA,EAI1C,GAAM,iBAAE6H,CAAe,CAAEC,iBAAe,CAAE,CAAG,MAAM,IAAI,CAACV,eAAe,CACrEhE,EACAiE,GAIIgD,EAAiB,CACrB,MAAM,IAAI,CAACxK,MAAM,CAACyD,UAAU,CAACrC,MAAM,CAAC,CAClCC,MAAO,yBACPqC,MAAOsE,CACT,GAAC,CACD3H,IAAI,CAAC,EAAE,CAACmD,SAAS,CAGb,CAAEnD,KAAM6K,CAAU,CAAE5K,MAAO6K,CAAW,CAAE,CAAG,MAAM,IAAI,CAAClL,QAAQ,CAAC+D,GAAG,CACtE,oBACA,CACEC,kBAAmBuG,EACnBY,aAAcnD,EACd/D,cAAe9D,EACfiL,uBAAwBd,CAC1B,GAGF,GAAIY,EACF,MAAM,KADS,CACC,CAAC,cAAc,EAAEA,EAAYrJ,OAAO,EAAE,EAYxD,IAAMwJ,EAA+CC,CATjBL,GAClC,IAQwDrK,GAAG,CAAC,GAAW,EACvEU,EADuE,MAC9DiK,EAAKjK,OAAO,CACrBiC,UAAW,EAAE,CACbY,SAAU,CACRyB,UAAW2F,EAAK5F,UAAU,CAC1BD,WAAY6F,EAAKlF,WAAW,CAC5BD,WAAYmF,EAAKjF,WAAW,EAEhC,GAGMkF,EAAoB,MAAM,IAAI,CAACtD,wBAAwB,CAAC5E,GACxD8F,EAAY,MAAM,IAAI,CAAChB,oBAAoB,CAACoD,GAG5CC,EAAiB,IAAIC,EAAe,IAAI,CAAC3L,MAAM,CAAZ2L,CACnCC,EAAS,MAAMF,EAAepI,QAAQ,CAACC,EAAO,GAG9CsI,EAAqB,IAAI,CAACC,iBAAiB,CAC/CR,EACA9D,GAYF,MAPyC,CACvCrB,OAHqB,CAGb4F,KAHmB,IAAI,CAACC,aAAa,CAACH,EAAoBtI,GAIlE8F,UAAWA,SACXuC,EACAK,YAAazE,CACf,CAGF,CAEA,MAAcwE,cACZ7F,CAAmC,CACnC5C,CAAa,CACyB,CACtC,GAAI4C,EAAOzD,MAAM,EAAI,EAAG,OAAOyD,EAI/B,IAAM+F,EAJiC,EAKpCrL,GAAG,CAAC,CAACsL,EAAG3F,IAAM,CAAC,QAAQ,EAAEA,EAAI,EAAE,GAAG,EAAE2F,EAAE5K,OAAO,CAACC,KAAK,CAAC,EAAG,KAAK,CAAC,CAAC,EAC9DR,IAAI,CAAC,QAEFC,EAAW,MAAM,IAAI,CAACjB,MAAM,CAACkB,IAAI,CAACC,WAAW,CAACC,MAAM,CAAC,CACzDC,MAAO,cACPC,SAAU,CACR,CAAEpC,KAAM,SAAUqC,QATP,CASgBsG,iNATkM,CAS3L,EAClC,CACE3I,KAAM,OACNqC,QAAS,CAAC,QAAQ,EAAEgC,EAAM;;AAEpC,EAAE2I,SAAS;;sBAEW,CAAC,EAEhB,CACDzK,gBAAiB,CAAEC,KAAM,aAAc,CACzC,GAEIhB,EAAkB,EAAE,CACxB,GAAI,CACF,IAAMa,EAAUN,EAASY,OAAO,CAAC,EAAE,EAAEC,SAASP,SAAW,GACzDb,EAAQiB,KAAKC,KAAK,CAACL,EACrB,CAAE,KAAM,CACN,OAAO4E,CACT,CAEA,IAAMiG,EAAyC,EAAE,CASjD,OARA1L,EAAMyB,OAAO,CAAC,IACZ,IAAMgK,EAAIhG,CAAM,CAACkG,EAAM,EAAE,CACrBF,GAAGC,EAAUE,IAAI,CAACH,EACxB,GAEAhG,EAAOhE,OAAO,CAAC,IACT,EAAWoK,QAAQ,CAACJ,IAAIC,EAAUE,IAAI,CAACH,EAC7C,GACOC,CACT,CAGQN,kBACN3F,CAAmC,CACnCqG,CAA2B,CACE,CAC7B,GAAI,CAACA,EAAS,OAAOrG,EAErB,IAAMsG,EAAoBD,EAAQvF,iBAAiB,EAAExE,cACrD,GAAI,CAACgK,EAAmB,OAAOtG,EAE/B,IAAIuG,EAAuC,CAAC,EACxCF,EAAQ9G,EAAE,EAAE,IACO5F,EAAqB,IAAI,CAACE,MAAM,EAE5CE,KAFgCJ,UAEjB,CAAC0M,EAAQ9G,EAAE,EAAEiH,IAAI,CAAEC,IACzCA,EAAUzK,OAAO,CAAC,IAChBuK,CAAY,CAAClG,EAAEqG,MAAM,CAACpK,WAAW,GAAG,CAAG+D,EAAEnC,KAAK,EAElD,GAIF,IAAIyI,EAAoC,CAAC,EAUzC,OATIN,GAAS9G,IAAI,IACK3D,IACZC,aAD6BD,CACf,CAACyK,EAAQ9G,EAAE,EAAEiH,IAAI,CAAC,IACtCI,EAAM5K,OAAO,CAAC,IACZ2K,CAAS,CAACE,EAAEjK,OAAO,CAAC,CAAGiK,EAAEhK,MAC3B,EACF,GAGKmD,EAAOhD,IAAI,CAAC,CAACC,EAAGC,KACrB,IAAM4J,KAAS,IAAI,CAACC,YAAY,CAAC9J,EAAGqJ,GAC9BU,KAAS,IAAI,CAACD,QADqC,IAAI,CAC5B7J,EAAGoJ,GAE9BW,EAAQ,IAAI,CAACC,SAAS,CAACjK,CAF4B,CAEzBsJ,GAF6B,EAG/C,IAAI,CAACW,SAAS,CAAChK,EAAGqJ,GAE1BY,EAAQ,IAAI,CAACD,SAAS,CAACjK,EAAG0J,GAGhC,OAAOK,EAASI,EAFF,IAAI,CAACF,CAEKG,QAASP,CAFJ5J,EAAGyJ,GAECG,GAASG,EAAQE,CAAAA,CAAI,EAE1D,CAEQJ,aAAa9G,CAAgC,CAAEqH,CAAe,CAAE,CACtE,OACErH,EAAM7E,OAAO,CAACkB,WAAW,GAAG8J,QAAQ,CAACkB,IACrCrH,EAAMhC,QAAQ,CAACyB,SAAS,EAAEpD,cAAc8J,SAASkB,EAErD,CAEQJ,UACNjH,CAAgC,CAChCsH,CAA6B,CAC7B,CACA,IAAK,IAAMrO,KAAOqO,EAChB,GAAItH,CADmB,CACb7E,OAAO,CAACkB,WAAW,GAAG8J,QAAQ,CAAClN,GAAM,OAAOqO,CAAK,CAACrO,EAAI,CAElE,OAAO,CACT,CACF,mJC3nBO,OAAMsO,EAGX5N,YAAY6N,CAAe,CAAE,CAI3B,IAAI,CAACC,GAAG,CAAGC,CAAAA,EAAAA,EAAAA,CAAAA,CAAYA,CAAC,CACtBF,OAAQA,CACV,EACF,CAKA,MAAMG,SACJxK,CAAa,CACbyK,CAAiC,CACjC/B,CAAgC,CAChC,CACA,IAAMgC,EAAU,IAAI,CAACC,YAAY,CAACF,EAAkB/B,GAC9CkC,EAAe,IAAI,CAACC,kBAAkB,CAACH,GAMvC5N,EAAO,IAAIgO,EAAAA,UAAUA,CA4B3B,OA1BAhO,EAAKiO,MAAM,CAAC3M,KAAKC,KAAK,CAACD,KAAK8H,SAAS,CAAC,kBAAEuE,CAAiB,KA0BlDjG,CAxBQ,MAAMwG,CAAAA,EAAAA,EAAAA,UAAAA,CAAUA,CAAC,CAE9BlN,MAAO,IAAI,CAACwM,GAAG,CAAC,UAChBvM,SAZ8B,CAC9B,CAAEpC,KAAM,SAAUqC,QAAS4M,CAAa,EACxC,CAAEjP,KAAM,OAAQqC,QAASgC,CAAM,EAChC,CAUCiL,SAAU,MAAOC,IACf,IAAMC,EAAgB,MAAM,IAAI,CAACC,gBAAgB,CAC/CF,EAAWG,IAAI,CACfX,GAEIY,EAAU,IAAI,CAACC,YAAY,CAACJ,EAAeV,GAC3Ce,EAAyC,SAA5BN,EAAWO,YAAY,CAAc,GAAM,GAQ9D3O,EAAKiO,MAAM,CAAC3M,KAAKC,KAAK,CAACD,KAAK8H,SAAS,CAACwF,CALpCL,KAAMF,EACNG,QAASA,EACTE,WAAYA,CACd,KAGA1O,EAAK6O,KAAK,EACZ,CACF,IAGcC,oBAAoB,EACpC,CAKA,aACEnB,CAAiC,CACjC/B,CAAgC,CAClB,CACd,IAAMmD,EAAepB,EAAiB7H,MAAM,CACzCtF,GAAG,CACF,CAACuF,EAAOiJ,IACN,CAAC,QAAQ,EAAEA,EAAQ,EAAE,GAAG,EAAEjJ,EAAMhC,QAAQ,CAACyB,SAAS,CAAC;AAAI,EACrDO,EAAM7E,OAAO,EACb,EAELP,IAAI,CAAC,eAEFqI,EAAY,CAAC;;UAEb,EACJ2E,EAAiB3E,SAAS,CAACjB,QAAQ,CAAC1F,MAAM,CAAG,EACzCsL,EAAiB3E,SAAS,CAACjB,QAAQ,CAChCvH,GAAG,CAAC,GAAO,GAAG2H,EAAEC,IAAI,CAAC,EAAE,EAAED,EAAE9G,IAAI,CAAC,CAAC,CAAC,EAClCV,IAAI,CAAC,MACR,OACL;eACU,EACTgN,EAAiB3E,SAAS,CAACf,aAAa,CAAC5F,MAAM,CAAG,EAC9CsL,EAAiB3E,SAAS,CAACf,aAAa,CACrCzH,GAAG,CACDyO,GACC,GAAGA,EAAE3G,kBAAkB,CAAC,IAAI,EAAE2G,EAAE5N,IAAI,CAAC,IAAI,EAAE4N,EAAE1G,kBAAkB,EAAE,EAEpE5H,IAAI,CAAC,MACR,OACL;IACD,CAAC,CAEGuO,EAAqB,6BACrBtD,IACFsD,GAAqB,CAAC,KADP;;wBAGG,EAAEtD,EAAYlF,kBAAkB,EAAI,gBAAgB;uBACrD,EAAEkF,EAAYhF,iBAAiB,EAAI,gBAAgB;qBACrD,EACb,CAACgF,EAAY5E,gBAAgB,EAAI,IAAIrG,IAAI,CAAC,OAAS,gBACpD;IACH,CAAC,EAGD,IAAMwO,EAAmB,GAAGJ,aAAa;AAAA;AAAI,EAAE/F,UAAU;AAAA;AAAI,EAAEkG,EAAAA,CAAoB,CAC7EE,EAAkB,IAAI,CAACC,kBAAkB,GAE/C,MAAO,kBAAEF,kBAAkBC,CAAgB,CAC7C,CAKA,oBAAqC,CACnC,OAAO,IAAIxG,OAAOC,WAAW,EAC/B,CAKA,mBAA0B+E,CAAqB,CAAU,CACvD,MAAO,CAAC;;;;;;wEAM4D,EAAEA,EAAQwB,eAAe,CAAC;;;;;IAK9F,EAAExB,EAAQuB,gBAAgB,CAAC;IAC3B,CAAC,CAMH,MAAcb,iBACZ1N,CAAgB,CAChBgN,CAAqB,CACJ,QACjB,GAA4C,GAAG,CAA9BhN,EAAS0O,IAAI,GAAGjN,MAAM,CAGhCzB,EAFE,oFAGX,CAKA,aACE2O,CAAoB,CACpB5B,CAAiC,CACH,CAC9B,IAEI6B,EAFEC,EAAmB,IAAIC,IACvBC,EAAQ,aAEd,KAAO,KAA6C,GAA5CH,EAAQG,EAAMC,IAAI,CAACL,EAAAA,CAAY,EACrCE,EAAiBI,GAAG,CAACC,SAASN,CAAK,CAAC,EAAE,CAAE,YAG1C,GAAiC,CAA7BC,EAAiBM,IAAI,CAEhBpC,EAAiB7H,MAAM,CAGzBkK,MAAM9P,IAAI,CAACuP,GACf3M,IAAI,CAAC,CAACC,EAAGC,IAAMD,EAAIC,GACnBxC,GAAG,CAAC,GAASmN,EAAiB7H,MAAM,CAACmK,EAAM,EAAE,EAC7CC,MAAM,CAACC,QACZ,CACF,CAFwB","sources":["webpack://@hijraah/web/./src/lib/utils/supabase-client.ts","webpack://@hijraah/web/./src/lib/rag/personalization/history-analyzer.ts","webpack://@hijraah/web/./src/lib/rag/personalization/preference-learner.ts","webpack://@hijraah/web/./src/lib/rag/retrieval/image-retriever.ts","webpack://@hijraah/web/./src/lib/rag/retrieval/hybrid-retriever.ts","webpack://@hijraah/web/./src/lib/rag/generation/context-generator.ts"],"sourcesContent":["import {\r\n  createClient as createSupabaseJsClient,\r\n  SupabaseClient,\r\n} from \"@supabase/supabase-js\";\r\n\r\n// Type definition for the possible roles a job might need\r\nexport type SupabaseRole = \"anon\" | \"service\";\r\n\r\n/**\r\n * Creates a Supabase client using environment variables.\r\n *\r\n * Usage in jobs:\r\n *   const supabase = createSupabaseClient(); // anon by default\r\n *   const admin = createSupabaseClient(\"service\");\r\n */\r\nexport function createSupabaseClient(\r\n  role: SupabaseRole = \"anon\"\r\n): SupabaseClient {\r\n  const url = process.env.NEXT_PUBLIC_SUPABASE_URL || process.env.SUPABASE_URL;\r\n  if (!url) {\r\n    throw new Error(\r\n      \"Supabase URL is missing. Set NEXT_PUBLIC_SUPABASE_URL or SUPABASE_URL.\"\r\n    );\r\n  }\r\n\r\n  const key =\r\n    role === \"service\"\r\n      ? process.env.SUPABASE_SERVICE_KEY ||\r\n        process.env.SUPABASE_SERVICE_ROLE_KEY\r\n      : process.env.SUPABASE_ANON_KEY ||\r\n        process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY;\r\n\r\n  if (!key) {\r\n    throw new Error(\r\n      `Supabase ${role} key is missing. Check environment variables.`\r\n    );\r\n  }\r\n\r\n  return createSupabaseJsClient(url, key, {\r\n    auth: {\r\n      autoRefreshToken: false,\r\n      persistSession: false,\r\n    },\r\n  });\r\n}\r\n","import { createSupabaseClient } from \"@/lib/utils/supabase-client\";\r\nimport { OpenAI } from \"openai\";\r\n\r\nexport interface HistoricalInterest {\r\n  entity: string;\r\n  score: number;\r\n}\r\n\r\nexport class QueryHistoryAnalyzer {\r\n  private supabase = createSupabaseClient(\"service\");\r\n  private openai: OpenAI;\r\n\r\n  constructor(openai: OpenAI) {\r\n    this.openai = openai;\r\n  }\r\n\r\n  async getTopInterests(\r\n    userId: string,\r\n    limit = 5\r\n  ): Promise<HistoricalInterest[]> {\r\n    const { data, error } = await this.supabase\r\n      .from(\"user_query_history\")\r\n      .select(\"query_text\")\r\n      .eq(\"user_id\", userId)\r\n      .order(\"created_at\", { ascending: false })\r\n      .limit(50);\r\n\r\n    if (error || !data) return [];\r\n\r\n    const combined = data.map((d) => d.query_text).join(\"\\n\");\r\n\r\n    const response = await this.openai.chat.completions.create({\r\n      model: \"gpt-4o-mini\",\r\n      messages: [\r\n        {\r\n          role: \"system\",\r\n          content:\r\n            \"Analyze the user's past queries and extract the top immigration-related entities (countries, visa types, programs). Return JSON array of {entity, score}. Score 1-5 importance.\",\r\n        },\r\n        { role: \"user\", content: combined.slice(0, 4000) },\r\n      ],\r\n      response_format: { type: \"json_object\" },\r\n    });\r\n\r\n    try {\r\n      return JSON.parse(response.choices[0].message.content ?? \"[]\");\r\n    } catch {\r\n      return [];\r\n    }\r\n  }\r\n}\r\n","import { createSupabaseClient } from \"@/lib/utils/supabase-client\";\r\n\r\nexport interface PreferenceWeight {\r\n  keyword: string;\r\n  weight: number; // 0-5\r\n}\r\n\r\nexport class PreferenceLearner {\r\n  private supabase = createSupabaseClient(\"service\");\r\n\r\n  async getUserWeights(userId: string): Promise<PreferenceWeight[]> {\r\n    const { data, error } = await this.supabase\r\n      .from(\"user_query_history\")\r\n      .select(\"feedback_text, feedback_score\")\r\n      .eq(\"user_id\", userId)\r\n      .gt(\"feedback_score\", 3);\r\n\r\n    if (error || !data) return [];\r\n\r\n    const wordCounts: Record<string, number> = {};\r\n\r\n    data.forEach((row) => {\r\n      const words = row.feedback_text?.split(/\\W+/) || [];\r\n      words.forEach((w: string) => {\r\n        const key = w.toLowerCase();\r\n        if (key.length < 4) return;\r\n        wordCounts[key] = (wordCounts[key] || 0) + 1;\r\n      });\r\n    });\r\n\r\n    return Object.entries(wordCounts)\r\n      .map(([k, v]) => ({ keyword: k, weight: Math.min(v, 5) }))\r\n      .sort((a, b) => b.weight - a.weight);\r\n  }\r\n}\r\n","import { createSupabaseClient } from \"@/lib/utils/supabase-client\";\r\nimport { OpenAI } from \"openai\";\r\nimport { RetrievedImage } from \"@/lib/rag/types\";\r\n\r\nexport class ImageRetriever {\r\n  private supabase = createSupabaseClient(\"service\");\r\n  private openai: OpenAI;\r\n\r\n  constructor(openai: OpenAI) {\r\n    this.openai = openai;\r\n  }\r\n\r\n  async retrieve(query: string, limit = 5): Promise<RetrievedImage[]> {\r\n    // Generate embedding. image-embedding-001 is deprecated – switch to text-embedding-3-small with fallback.\r\n    let embedding: number[] = [];\r\n\r\n    try {\r\n      embedding = (\r\n        await this.openai.embeddings.create({\r\n          model: \"text-embedding-3-small\",\r\n          input: query,\r\n        })\r\n      ).data[0].embedding;\r\n    } catch (err: any) {\r\n      if (\r\n        err?.code === \"model_not_found\" ||\r\n        /model .* does not exist/i.test(err?.message ?? \"\")\r\n      ) {\r\n        // Retry once with large model\r\n        console.warn(\r\n          \"text-embedding-3-small unavailable, retrying with text-embedding-3-large\"\r\n        );\r\n        embedding = (\r\n          await this.openai.embeddings.create({\r\n            model: \"text-embedding-3-large\",\r\n            input: query,\r\n          })\r\n        ).data[0].embedding;\r\n      } else {\r\n        console.error(\"Embedding generation failed\", err);\r\n        return [];\r\n      }\r\n    }\r\n\r\n    // Search image_embeddings by vector distance\r\n    const { data, error } = await this.supabase.rpc(\"match_image_embeddings\", {\r\n      p_query_embedding: embedding,\r\n      p_match_count: limit,\r\n    });\r\n\r\n    if (error) {\r\n      console.error(\"Image search error\", error.message);\r\n      return [];\r\n    }\r\n\r\n    return (data || []).map((row: any) => ({\r\n      url: row.image_url,\r\n      metadata: row.metadata || {},\r\n      score: row.similarity,\r\n    }));\r\n  }\r\n}\r\n","import { SupabaseClient } from \"@supabase/supabase-js\";\nimport OpenAI from \"openai\";\nimport {\n  RAGProcessedDocument,\n  RetrievalResult,\n  KGContext,\n  RAGProcessedDocumentChunk,\n} from \"@/lib/rag/types\";\nimport { UserProfile } from \"@/types/user\";\nimport NodeCache from \"node-cache\";\nimport { QueryHistoryAnalyzer } from \"@/lib/rag/personalization/history-analyzer\";\nimport { PreferenceLearner } from \"@/lib/rag/personalization/preference-learner\";\nimport { ImageRetriever } from \"@/lib/rag/retrieval/image-retriever\";\nimport { Index } from \"@upstash/vector\";\nimport { Redis } from \"@upstash/redis\";\nimport crypto from \"crypto\";\n\ninterface QueryOptions {\n  limit?: number;\n  threshold?: number;\n  userId?: string;\n}\n\ninterface SearchResult {\n  id: string;\n  document_id: string;\n  content: string;\n  score: number;\n}\n\ninterface UnderstoodQuery {\n  embedding_query: string;\n  full_text_query: string;\n}\n\n// New: strongly-typed row returned by search_rag_hybrid RPC\ninterface SearchRAGHybridRow {\n  chunk_id: string;\n  document_id: string;\n  chunk_index: number;\n  content: string;\n  source_url: string;\n  vector_similarity: number;\n  text_rank: number;\n  entities: unknown;\n  final_score: number;\n}\n\nexport class HybridRetriever {\n  private supabase: SupabaseClient;\n  private openai: OpenAI;\n  private queryCache: NodeCache;\n  private vectorCache: Index;\n  private redis: Redis;\n\n  constructor(supabaseClient: SupabaseClient, openaiClient: OpenAI) {\n    this.supabase = supabaseClient;\n    this.openai = openaiClient;\n\n    // Initialize cache with a 10-minute TTL for understood queries\n    this.queryCache = new NodeCache({ stdTTL: 600 });\n\n    // Initialize Upstash clients\n    this.vectorCache = new Index({\n      url: process.env.UPSTASH_VECTOR_URL!,\n      token: process.env.UPSTASH_VECTOR_TOKEN!,\n    });\n\n    this.redis = new Redis({\n      url: process.env.UPSTASH_REDIS_URL!,\n      token: process.env.UPSTASH_REDIS_TOKEN!,\n    });\n  }\n\n  async storeDocument(document: RAGProcessedDocument): Promise<void> {\n    const { error: docError } = await this.supabase\n      .from(\"rag_documents\")\n      .upsert(\n        {\n          id: document.documentId,\n          source_url: document.sourceUrl,\n          raw_text: document.rawText,\n          status: \"processed\",\n        },\n        { onConflict: \"id\" }\n      );\n\n    if (docError) {\n      throw new Error(`Error storing document metadata: ${docError.message}`);\n    }\n\n    const chunksToInsert = document.chunks.map((chunk) => ({\n      id: `${document.documentId}_${chunk.metadata.chunkIndex}`,\n      document_id: document.documentId,\n      content: chunk.content,\n      embedding: chunk.embedding,\n      chunk_index: chunk.metadata.chunkIndex,\n      metadata: chunk.metadata,\n    }));\n\n    const batchSize = 100;\n    for (let i = 0; i < chunksToInsert.length; i += batchSize) {\n      const batch = chunksToInsert.slice(i, i + batchSize);\n      const { error: chunkError } = await this.supabase\n        .from(\"document_chunks_enhanced\")\n        .insert(batch);\n      if (chunkError) {\n        throw new Error(`Error storing document chunks: ${chunkError.message}`);\n      }\n    }\n  }\n\n  private async getUserProfile(userId: string): Promise<UserProfile | null> {\n    const { data, error } = await this.supabase\n      .from(\"profiles\")\n      .select(\n        \"id, country_of_residence, country_of_interest, country_of_citizenship, immigration_goals\"\n      )\n      .eq(\"id\", userId)\n      .single();\n\n    if (error) {\n      if (error.code === \"PGRST116\") {\n        // PGRST116: \"The result contains 0 rows\" - this is not an error for us\n        console.log(`No profile found for userId: ${userId}`);\n        return null;\n      }\n      console.error(`Error fetching profile for userId ${userId}:`, error);\n      return null;\n    }\n\n    if (!data) {\n      return null;\n    }\n\n    // Manually map snake_case from DB to camelCase for the UserProfile type\n    return {\n      id: data.id,\n      countryOfResidence: data.country_of_residence,\n      countryOfInterest: data.country_of_interest,\n      countryOfCitizenship: data.country_of_citizenship,\n      immigrationGoals: data.immigration_goals || [], // This is now a JSONB array\n    };\n  }\n\n  private async understandQuery(\n    query: string,\n    userProfile?: UserProfile | null\n  ): Promise<UnderstoodQuery> {\n    const cacheKey = `understood_query_${\n      userProfile?.id || \"anonymous\"\n    }_${query}`;\n    const cachedResult = this.queryCache.get<UnderstoodQuery>(cacheKey);\n\n    if (cachedResult) {\n      console.log(\"Returning cached understood query.\");\n      return cachedResult;\n    }\n\n    const profileContext = userProfile\n      ? `\n      Here is the user's profile for context:\n      - Current Country of Residence: ${userProfile.countryOfResidence || \"N/A\"}\n      - Country of Citizenship: ${userProfile.countryOfCitizenship || \"N/A\"}\n      - Country of Interest for Immigration: ${\n        userProfile.countryOfInterest || \"N/A\"\n      }\n      - Stated Immigration Goals: ${\n        userProfile.immigrationGoals?.join(\", \") || \"N/A\"\n      }\n      \n      Personalize the queries based on this context. For example, if the user asks about \"visa requirements\", assume they are asking for themselves.`\n      : \"The user is anonymous. Generate a generic query.\";\n\n    const prompt = `\n      Analyze the following user query for an immigration assistant bot. \n      ${profileContext}\n\n      User Query: \"${query}\"\n\n      1.  **Embedding Query**: Rephrase the user's query to be a clear, concise question or statement that captures the core semantic meaning, incorporating the user's context. This will be used to find semantically similar document chunks.\n      2.  **Full-Text Query**: Extract the most important keywords and entities from the user's query and profile. Combine them using OR operators for a broad full-text search. Focus on nouns, proper nouns, and key immigration terms.\n\n      Return the result as a JSON object with two keys: \"embedding_query\" and \"full_text_query\".\n    `;\n\n    try {\n      const response = await this.openai.chat.completions.create({\n        model: \"gpt-4o\",\n        messages: [\n          {\n            role: \"system\",\n            content:\n              \"You are a query optimization expert for retrieval systems specializing in immigration.\",\n          },\n          { role: \"user\", content: prompt },\n        ],\n        response_format: { type: \"json_object\" },\n      });\n\n      const content = response.choices[0]?.message?.content;\n      if (!content) throw new Error(\"No content returned from OpenAI.\");\n      const parsedContent = JSON.parse(content);\n\n      const result: UnderstoodQuery = {\n        embedding_query: parsedContent.embedding_query || query,\n        full_text_query:\n          parsedContent.full_text_query || query.split(\" \").join(\" | \"),\n      };\n\n      this.queryCache.set(cacheKey, result);\n      return result;\n    } catch (error) {\n      console.warn(\n        \"Error understanding query, falling back to basic search:\",\n        error\n      );\n      return {\n        embedding_query: query,\n        full_text_query: query.split(\" \").join(\" | \"),\n      };\n    }\n  }\n\n  private async extractEntitiesFromQuery(\n    query: string\n  ): Promise<{ name: string; type: string }[]> {\n    try {\n      const response = await this.openai.chat.completions.create({\n        model: \"gpt-4o\",\n        messages: [\n          {\n            role: \"system\",\n            content:\n              \"You are an expert at extracting key entities (like countries, visa types, organizations) from user queries about immigration.\",\n          },\n          {\n            role: \"user\",\n            content: `Extract all named entities from the following query and classify them. Query: \"${query}\"\n\nRespond with a JSON object containing a single key \"entities\", which is an array of objects, where each object has \"name\" and \"type\".\nExample: {\"entities\": [{\"name\": \"Canada\", \"type\": \"Country\"}, {\"name\": \"Express Entry\", \"type\": \"Visa/Program\"}]}`,\n          },\n        ],\n        response_format: { type: \"json_object\" },\n      });\n\n      const content = response.choices[0]?.message?.content;\n      if (!content) return [];\n\n      const parsed = JSON.parse(content);\n      return parsed.entities || [];\n    } catch (error) {\n      console.error(\"Error extracting entities from query:\", error);\n      return [];\n    }\n  }\n\n  private async searchKnowledgeGraph(\n    entities: { name: string; type: string }[]\n  ): Promise<KGContext> {\n    if (entities.length === 0) {\n      return { entities: [], relationships: [] };\n    }\n\n    const entityNames = entities.map((e) => e.name);\n\n    try {\n      // Use new RPC that encapsulates the join logic in SQL to avoid complex REST filters\n      const { data, error } = await this.supabase.rpc(\"get_related_edges\", {\n        p_entity_names: entityNames,\n      });\n\n      if (error) {\n        console.error(\"Error searching knowledge graph:\", error);\n        return { entities, relationships: [] };\n      }\n\n      const relationships = (data || []).map((row: any) => ({\n        source_entity_name: row.source_entity_name ?? \"\",\n        target_entity_name: row.target_entity_name ?? \"\",\n        type: row.relationship_type,\n      }));\n\n      return {\n        entities,\n        relationships,\n      };\n    } catch (err) {\n      console.error(\"Unexpected KG search error\", err);\n      return { entities, relationships: [] };\n    }\n  }\n\n  /**\n   * Attempt to fetch a cached retrieval result from the `rag_query_cache` table.\n   */\n  private async getCachedRetrieval(\n    queryHash: string\n  ): Promise<RetrievalResult | null> {\n    const { data, error } = await this.supabase\n      .from(\"rag_query_cache\")\n      .select(\"retrieved_chunks, kg_entities\")\n      .eq(\"query_hash\", queryHash)\n      .lte(\"expires_at\", new Date().toISOString())\n      .maybeSingle();\n\n    if (error) {\n      console.warn(\"Cache lookup error:\", error.message);\n      return null;\n    }\n    if (!data) return null;\n\n    try {\n      const chunks = JSON.parse(\n        data.retrieved_chunks\n      ) as RAGProcessedDocumentChunk[];\n      const kgContext = JSON.parse(data.kg_entities) as KGContext;\n      return { chunks, kgContext };\n    } catch {\n      return null;\n    }\n  }\n\n  /**\n   * Store the retrieval result in the cache table for faster future access.\n   */\n  private hashQuery(query: string, options: QueryOptions): string {\n    const payload = JSON.stringify({ query, ...options });\n    return crypto.createHash(\"sha256\").update(payload).digest(\"hex\");\n  }\n\n  private async cacheRetrieval(queryHash: string, result: RetrievalResult) {\n    try {\n      // Cache in Supabase for long-term storage\n      await this.supabase.from(\"rag_query_cache\").upsert({\n        query_hash: queryHash,\n        query_text: \"\", // optional full text\n        retrieved_chunks: JSON.stringify(result.chunks),\n        kg_entities: JSON.stringify(result.kgContext),\n        expires_at: new Date(Date.now() + 1000 * 60 * 60).toISOString(), // 1 hour ttl\n      });\n\n      // Cache in Redis for fast access\n      await this.redis.setex(\n        `hybrid:${queryHash}`,\n        3600,\n        JSON.stringify(result)\n      );\n    } catch (err) {\n      console.warn(\"Cache write error\", err);\n    }\n  }\n\n  // Enhanced search method with multi-level caching\n  async search(\n    query: string,\n    options: QueryOptions = {}\n  ): Promise<RetrievalResult> {\n    const startTime = Date.now();\n    const queryHash = this.hashQuery(query, options);\n\n    try {\n      // L1 Cache: Check Redis for exact query results\n      const redisKey = `hybrid:${queryHash}`;\n      const redisResult = await this.redis.get(redisKey);\n      if (redisResult) {\n        console.log(\"✅ Redis cache hit\");\n        const cached = JSON.parse(redisResult as string);\n        return cached;\n      }\n\n      // L2 Cache: Check Supabase cache\n      const cached = await this.getCachedRetrieval(queryHash);\n      if (cached) {\n        // Populate Redis cache for next time\n        await this.redis.setex(redisKey, 3600, JSON.stringify(cached));\n        return cached;\n      }\n\n      // L3 Cache: Check Upstash vector cache for similar queries\n      const { limit = 10, threshold = 0.7, userId } = options;\n      let userProfile: UserProfile | null = null;\n      if (userId) {\n        userProfile = await this.getUserProfile(userId);\n      }\n\n      const { embedding_query } = await this.understandQuery(\n        query,\n        userProfile\n      );\n      const queryEmbedding = (\n        await this.openai.embeddings.create({\n          model: \"text-embedding-3-small\",\n          input: embedding_query,\n        })\n      ).data[0].embedding;\n\n      try {\n        const nsIndex = userId\n          ? this.vectorCache.namespace(`user_${userId}`)\n          : this.vectorCache;\n\n        const vectorCacheResults = await nsIndex.query({\n          vector: queryEmbedding,\n          topK: 3,\n          includeMetadata: true,\n        });\n\n        if (\n          vectorCacheResults.length > 0 &&\n          vectorCacheResults[0].score > 0.95\n        ) {\n          console.log(\"✅ Upstash vector cache hit\");\n          const cachedResult = vectorCacheResults[0].metadata as any;\n          if (cachedResult?.result) {\n            return cachedResult.result;\n          }\n        }\n      } catch (vectorCacheError) {\n        console.warn(\"Vector cache query failed:\", vectorCacheError);\n      }\n\n      // L4 Storage: Perform actual retrieval\n      console.log(\"🔍 Cache miss, performing retrieval\");\n      const result = await this._searchWithoutCache(query, options);\n\n      // Cache the results for future queries\n      await this.cacheRetrieval(queryHash, result);\n\n      // Cache query embedding for semantic similarity\n      const nsIndex = userId\n        ? this.vectorCache.namespace(`user_${userId}`)\n        : this.vectorCache;\n\n      await nsIndex.upsert({\n        id: queryHash,\n        vector: queryEmbedding,\n        metadata: {\n          query,\n          result,\n          cachedAt: Date.now(),\n        },\n      });\n\n      return result;\n    } catch (error) {\n      console.error(\"Enhanced hybrid retrieval failed:\", error);\n      throw error;\n    }\n  }\n\n  // Extract existing search logic into private _searchWithoutCache\n  private async _searchWithoutCache(\n    query: string,\n    options: QueryOptions = {}\n  ): Promise<RetrievalResult> {\n    const { limit = 10, threshold = 0.7, userId } = options;\n\n    let userProfile: UserProfile | null = null;\n    if (userId) {\n      userProfile = await this.getUserProfile(userId);\n    }\n\n    // 1. Understand Query (same as before)\n    const { embedding_query, full_text_query } = await this.understandQuery(\n      query,\n      userProfile\n    );\n\n    // 2. Generate Embedding for the Semantic Part of the Query\n    const queryEmbedding = (\n      await this.openai.embeddings.create({\n        model: \"text-embedding-3-small\",\n        input: embedding_query,\n      })\n    ).data[0].embedding;\n\n    // 3. Hybrid Search for Document Chunks (same as before)\n    const { data: chunksData, error: chunksError } = await this.supabase.rpc(\n      \"search_rag_hybrid\",\n      {\n        p_query_embedding: queryEmbedding,\n        p_query_text: full_text_query,\n        p_match_count: limit,\n        p_similarity_threshold: threshold,\n      }\n    );\n\n    if (chunksError) {\n      throw new Error(`Search error: ${chunksError.message}`);\n    }\n\n    const rows: SearchRAGHybridRow[] = (chunksData ||\n      []) as SearchRAGHybridRow[];\n\n    // Structured log for observability (non-cached path)\n    console.log(\n      \"[HybridRetriever] _searchWithoutCache top 3:\",\n      rows.slice(0, 3).map((r) => ({ id: r.chunk_id, score: r.final_score }))\n    );\n\n    const retrievedChunks: RAGProcessedDocumentChunk[] = rows.map((item) => ({\n      content: item.content,\n      embedding: [], // Embedding not needed for context\n      metadata: {\n        sourceUrl: item.source_url,\n        documentId: item.document_id,\n        chunkIndex: item.chunk_index,\n      },\n    }));\n\n    // 4. Extract Entities and Search Knowledge Graph\n    const extractedEntities = await this.extractEntitiesFromQuery(query);\n    const kgContext = await this.searchKnowledgeGraph(extractedEntities);\n\n    // Image search\n    const imageRetriever = new ImageRetriever(this.openai);\n    const images = await imageRetriever.retrieve(query, 5);\n\n    // 4. Personalization based on user profile\n    const personalizedChunks = this.personalizeChunks(\n      retrievedChunks,\n      userProfile\n    );\n\n    const rerankedChunks = await this.rerankResults(personalizedChunks, query);\n\n    const retrievalResult: RetrievalResult = {\n      chunks: rerankedChunks,\n      kgContext: kgContext,\n      images,\n      userContext: userProfile,\n    };\n\n    return retrievalResult;\n  }\n\n  private async rerankResults(\n    chunks: RAGProcessedDocumentChunk[],\n    query: string\n  ): Promise<RAGProcessedDocumentChunk[]> {\n    if (chunks.length <= 3) return chunks; // No need to rerank few\n\n    const prompt = `You are a reranking model. Given a user query and a list of document excerpts, rank the excerpts from most to least relevant. Respond with a JSON array of integers representing the new order (1-based indices).`;\n\n    const excerpts = chunks\n      .map((c, i) => `Excerpt ${i + 1}: \"${c.content.slice(0, 300)}\"`)\n      .join(\"\\n\\n\");\n\n    const response = await this.openai.chat.completions.create({\n      model: \"gpt-4o-mini\", // cheaper reranker\n      messages: [\n        { role: \"system\", content: prompt },\n        {\n          role: \"user\",\n          content: `Query: \"${query}\"\n\n${excerpts}\n\nReturn JSON array now.`,\n        },\n      ],\n      response_format: { type: \"json_object\" },\n    });\n\n    let order: number[] = [];\n    try {\n      const content = response.choices[0]?.message?.content || \"\";\n      order = JSON.parse(content);\n    } catch {\n      return chunks;\n    }\n\n    const reordered: RAGProcessedDocumentChunk[] = [];\n    order.forEach((idx) => {\n      const c = chunks[idx - 1];\n      if (c) reordered.push(c);\n    });\n    // Append any missing\n    chunks.forEach((c) => {\n      if (!reordered.includes(c)) reordered.push(c);\n    });\n    return reordered;\n  }\n\n  // ---------------- PERSONALIZATION ----------------\n  private personalizeChunks(\n    chunks: RAGProcessedDocumentChunk[],\n    profile: UserProfile | null\n  ): RAGProcessedDocumentChunk[] {\n    if (!profile) return chunks;\n\n    const interestedCountry = profile.countryOfInterest?.toLowerCase();\n    if (!interestedCountry) return chunks;\n\n    let historyBoost: Record<string, number> = {};\n    if (profile.id) {\n      const analyzer = new QueryHistoryAnalyzer(this.openai);\n      // fire and forget, but could await for synchronous personalization\n      analyzer.getTopInterests(profile.id).then((interests) => {\n        interests.forEach((i) => {\n          historyBoost[i.entity.toLowerCase()] = i.score;\n        });\n      });\n    }\n\n    // preference boost from feedback\n    let prefBoost: Record<string, number> = {};\n    if (profile?.id) {\n      const learner = new PreferenceLearner();\n      learner.getUserWeights(profile.id).then((prefs) => {\n        prefs.forEach((p) => {\n          prefBoost[p.keyword] = p.weight;\n        });\n      });\n    }\n\n    return chunks.sort((a, b) => {\n      const aScore = this.matchCountry(a, interestedCountry) ? 1 : 0;\n      const bScore = this.matchCountry(b, interestedCountry) ? 1 : 0;\n\n      const aHist = this.histBoost(a, historyBoost);\n      const bHist = this.histBoost(b, historyBoost);\n\n      const aPref = this.histBoost(a, prefBoost);\n      const bPref = this.histBoost(b, prefBoost);\n\n      return bScore + bHist + bPref - (aScore + aHist + aPref);\n    });\n  }\n\n  private matchCountry(chunk: RAGProcessedDocumentChunk, country: string) {\n    return (\n      chunk.content.toLowerCase().includes(country) ||\n      chunk.metadata.sourceUrl?.toLowerCase().includes(country)\n    );\n  }\n\n  private histBoost(\n    chunk: RAGProcessedDocumentChunk,\n    boost: Record<string, number>\n  ) {\n    for (const key in boost) {\n      if (chunk.content.toLowerCase().includes(key)) return boost[key];\n    }\n    return 0;\n  }\n}\n","import { CoreMessage, StreamData, streamText } from \"ai\";\nimport { createOpenAI } from \"@ai-sdk/openai\";\nimport {\n  RetrievalResult,\n  RAGProcessedDocumentChunk,\n  UserContext,\n  GeneratedResponse,\n} from \"@/lib/rag/types\";\n\n/**\n * A generator that creates context-aware and personalized responses,\n * supporting real-time streaming and post-generation processing.\n */\nexport class ContextAwareGenerator {\n  private llm;\n\n  constructor(apiKey?: string) {\n    // Create a custom OpenAI provider instance with the API key.\n    // The AI SDK will automatically use the OPENAI_API_KEY environment\n    // variable if the apiKey is not provided.\n    this.llm = createOpenAI({\n      apiKey: apiKey,\n    });\n  }\n\n  /**\n   * Generates a streaming response based on a query, retrieval context, and user profile.\n   */\n  async generate(\n    query: string,\n    retrievedContext: RetrievalResult,\n    userContext?: UserContext | null\n  ) {\n    const context = this.buildContext(retrievedContext, userContext);\n    const systemPrompt = this.createSystemPrompt(context);\n    const messages: CoreMessage[] = [\n      { role: \"system\", content: systemPrompt },\n      { role: \"user\", content: query },\n    ];\n\n    const data = new StreamData();\n    // Immediately pass retrieval context to the client for display\n    data.append(JSON.parse(JSON.stringify({ retrievedContext })));\n\n    const result = await streamText({\n      // Use the specific model from our configured provider\n      model: this.llm(\"gpt-4o\"),\n      messages,\n      onFinish: async (completion) => {\n        const validatedText = await this.validateResponse(\n          completion.text,\n          context\n        );\n        const sources = this.addCitations(validatedText, retrievedContext);\n        const confidence = completion.finishReason === \"stop\" ? 0.9 : 0.5;\n\n        const finalResponse: GeneratedResponse = {\n          text: validatedText,\n          sources: sources,\n          confidence: confidence,\n        };\n        // Append the final, processed data to the stream\n        data.append(JSON.parse(JSON.stringify(finalResponse)));\n        data.close();\n      },\n    });\n\n    // Use toDataStreamResponse() to create a ReadableStream with the Vercel AI protocol.\n    return result.toDataStreamResponse();\n  }\n\n  /**\n   * Constructs the full context string to be passed to the LLM.\n   */\n  public buildContext(\n    retrievedContext: RetrievalResult,\n    userContext?: UserContext | null\n  ): BuiltContext {\n    const chunkContext = retrievedContext.chunks\n      .map(\n        (chunk, index) =>\n          `Source [${index + 1}] (${chunk.metadata.sourceUrl}):\\n${\n            chunk.content\n          }`\n      )\n      .join(\"\\n\\n---\\n\\n\");\n\n    const kgContext = `\nKnowledge Graph Context:\nEntities: ${\n      retrievedContext.kgContext.entities.length > 0\n        ? retrievedContext.kgContext.entities\n            .map((e) => `${e.name} (${e.type})`)\n            .join(\", \")\n        : \"None\"\n    }\nRelationships: ${\n      retrievedContext.kgContext.relationships.length > 0\n        ? retrievedContext.kgContext.relationships\n            .map(\n              (r) =>\n                `${r.source_entity_name} -> ${r.type} -> ${r.target_entity_name}`\n            )\n            .join(\", \")\n        : \"None\"\n    }\n    `;\n\n    let userProfileContext = \"User Profile: Not provided.\";\n    if (userContext) {\n      userProfileContext = `\nUser Profile:\n- Country of Residence: ${userContext.countryOfResidence || \"Not specified\"}\n- Country of Interest: ${userContext.countryOfInterest || \"Not specified\"}\n- Immigration Goals: ${\n        (userContext.immigrationGoals || []).join(\", \") || \"Not specified\"\n      }\n    `;\n    }\n\n    const formattedContext = `${chunkContext}\\n\\n${kgContext}\\n\\n${userProfileContext}`;\n    const temporalContext = this.getTemporalContext();\n\n    return { formattedContext, temporalContext };\n  }\n\n  /**\n   * Gets the current date to provide temporal context.\n   */\n  private getTemporalContext(): string {\n    return new Date().toISOString();\n  }\n\n  /**\n   * Creates the system prompt with instructions for the LLM.\n   */\n  public createSystemPrompt(context: BuiltContext): string {\n    return `You are a specialized AI assistant for Hijraah, an immigration services platform. Your role is to provide accurate, context-aware, and personalized immigration advice.\n\n    **Instructions:**\n    1.  **Primary Source:** Base your answers strictly on the provided \"Source\" and \"Knowledge Graph Context\". Do not use outside knowledge.\n    2.  **Citation:** When you use information from a source, cite it by number (e.g., [1], [2]).\n    3.  **Personalization:** Use the \"User Profile\" to tailor your response to the user's specific situation.\n    4.  **Temporal Awareness:** Consider the \"Current Date\" provided as ${context.temporalContext} for any time-sensitive information.\n    5.  **No Information:** If the context does not contain an answer, explicitly state that you cannot answer based on the provided information. Do not invent details.\n    6.  **Persona:** Maintain a professional, empathetic, and encouraging tone.\n\n    **Provided Context:**\n    ${context.formattedContext}\n    `;\n  }\n\n  /**\n   * Validates the generated response.\n   */\n  private async validateResponse(\n    response: string,\n    context: BuiltContext\n  ): Promise<string> {\n    if (!response || response.trim().length === 0) {\n      return \"I am sorry, but I could not generate a response based on the provided information.\";\n    }\n    return response;\n  }\n\n  /**\n   * Extracts source citations from the final text.\n   */\n  private addCitations(\n    responseText: string,\n    retrievedContext: RetrievalResult\n  ): GeneratedResponse[\"sources\"] {\n    const mentionedSources = new Set<number>();\n    const regex = /\\[(\\d+)\\]/g;\n    let match;\n    while ((match = regex.exec(responseText)) !== null) {\n      mentionedSources.add(parseInt(match[1], 10));\n    }\n\n    if (mentionedSources.size === 0) {\n      // If no explicit citations, assume all sources were relevant to the response\n      return retrievedContext.chunks;\n    }\n\n    return Array.from(mentionedSources)\n      .sort((a, b) => a - b)\n      .map((num) => retrievedContext.chunks[num - 1])\n      .filter(Boolean); // Filter out any invalid index\n  }\n}\n\n/**\n * Internal representation of the context passed to the LLM.\n */\ninterface BuiltContext {\n  formattedContext: string;\n  temporalContext: string;\n}\n"],"names":["createSupabaseClient","role","url","process","key","env","SUPABASE_SERVICE_KEY","SUPABASE_SERVICE_ROLE_KEY","SUPABASE_ANON_KEY","createSupabaseJsClient","auth","autoRefreshToken","persistSession","QueryHistoryAnalyzer","constructor","openai","supabase","getTopInterests","userId","limit","data","error","from","select","eq","order","ascending","combined","map","d","query_text","join","response","chat","completions","create","model","messages","content","slice","response_format","type","JSON","parse","choices","message","PreferenceLearner","getUserWeights","gt","wordCounts","forEach","words","row","feedback_text","split","w","toLowerCase","length","Object","entries","k","v","keyword","weight","Math","min","sort","a","b","retrieve","query","embedding","embeddings","input","err","code","test","console","warn","rpc","p_query_embedding","p_match_count","image_url","metadata","score","similarity","HybridRetriever","supabaseClient","openaiClient","queryCache","NodeCache","stdTTL","vectorCache","Index","UPSTASH_VECTOR_URL","token","UPSTASH_VECTOR_TOKEN","redis","Redis","UPSTASH_REDIS_URL","UPSTASH_REDIS_TOKEN","storeDocument","document","docError","upsert","id","documentId","source_url","sourceUrl","raw_text","rawText","status","onConflict","chunksToInsert","chunks","chunk","chunkIndex","document_id","chunk_index","i","batchSize","batch","chunkError","insert","getUserProfile","single","countryOfResidence","country_of_residence","countryOfInterest","country_of_interest","countryOfCitizenship","country_of_citizenship","immigrationGoals","immigration_goals","understandQuery","userProfile","cacheKey","cachedResult","get","profileContext","prompt","parsedContent","result","embedding_query","full_text_query","set","extractEntitiesFromQuery","entities","searchKnowledgeGraph","relationships","entityNames","e","name","p_entity_names","source_entity_name","target_entity_name","relationship_type","getCachedRetrieval","queryHash","lte","Date","toISOString","maybeSingle","retrieved_chunks","kgContext","kg_entities","options","payload","stringify","crypto","update","digest","cacheRetrieval","query_hash","expires_at","now","setex","search","hashQuery","redisKey","redisResult","cached","threshold","queryEmbedding","nsIndex","namespace","vectorCacheResults","vector","topK","includeMetadata","vectorCacheError","_searchWithoutCache","cachedAt","chunksData","chunksError","p_query_text","p_similarity_threshold","retrievedChunks","rows","item","extractedEntities","imageRetriever","ImageRetriever","images","personalizedChunks","personalizeChunks","rerankedChunks","rerankResults","userContext","excerpts","c","reordered","idx","push","includes","profile","interestedCountry","historyBoost","then","interests","entity","prefBoost","prefs","p","aScore","matchCountry","bScore","aHist","histBoost","aPref","bHist","bPref","country","boost","ContextAwareGenerator","apiKey","llm","createOpenAI","generate","retrievedContext","context","buildContext","systemPrompt","createSystemPrompt","StreamData","append","streamText","onFinish","completion","validatedText","validateResponse","text","sources","addCitations","confidence","finishReason","finalResponse","close","toDataStreamResponse","chunkContext","index","r","userProfileContext","formattedContext","temporalContext","getTemporalContext","trim","responseText","match","mentionedSources","Set","regex","exec","add","parseInt","size","Array","num","filter","Boolean"],"sourceRoot":""}