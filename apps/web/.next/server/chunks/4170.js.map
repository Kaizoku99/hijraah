{"version":3,"file":"4170.js","mappings":"+fAmMA,cACA,QACA,wEAAgG,2BAA8B,KAC9H,QACA,UACA,sBAAoD,kBAAkC,KACtF,QACA,SACA,EAAK,KACL,CAAG,WACH,CAGA,cACA,UACA,WACA,YACA,cACA,cACA,sBACA,sBACA,qBACA,iBACA,kBACA,SACA,eACA,CACA,CAKA,MAA4B,IAAQ,EACpC,MAAS,IAAQ,EACjB,QAAa,IAAQ,GAIrB,KAAU,IAAQ,aAClB,MAAW,IAAK,aAChB,KAAU,IAAO,EAAE,IAAQ,GAAI,IAAQ,cACvC,CAAG,CACH,CAAC,EACD,EAAkC,QAA8B,EAChE,cACA,kCACC,EAGD,YACA,KACA,QACA,UACC,EACD,OACA,oBACA,yBACA,wCACA,CACA,CA0GA,YACA,mBACA,+BACA,eACA,gBACA,aACA,CACA,gCACA,MACA,kEACA,CACA,yCACA,aAoqBA,mCAnqBA,OAEA,4CACA,CACA,eACA,4BAEA,wBACA,oCAEA,SACA,OACA,SACA,YACA,cACA,OACA,OACA,mBACA,kBACA,gBACA,iBACA,OACA,mBACG,MACH,gBA6oBA,EACA,IA7oBA,aACA,WACA,GACA,QACA,2BACA,cACA,CAAO,EAEP,kFACA,QACA,2BACA,yBACA,8EACA,CAAO,EAEP,6CACA,2CACA,UAAgB,IAA8B,EAC9C,+DACA,CAAO,EAEP,qCACA,UAAgB,IAA8B,EAC9C,+DACA,CAAO,EAEP,aAAY,cAAsC,SA9YlD,CACA,SACA,8BACA,6BACC,EACD,SACA,KACA,aAAe,aAAgB,IAC/B,UACA,aACA,UACA,aACA,QAA4B,wBAAyB,EACrD,KAEA,iBACA,QAA4B,2BAA4B,EACxD,KAEA,cACA,QACA,aACA,oDACA,CAAa,EACb,KAEA,SAEA,YACA,oCAFA,EAEmE,EAGnE,CACA,KAEA,YACA,qCACA,QAA0B,8BAAwC,EAClE,KACA,CACA,QACA,YACA,sBACA,YACA,eACA,WACA,OAAyB,wBAEzB,aACA,OACA,iBACA,WACA,sDAAqF,qCAAkD,SAAS,QAAyB,UAAa,EAEtL,mFAEA,CAEA,YACA,yBACA,UAA4B,IAA6B,EACzD,+EACA,CAAmB,EAEnB,mBACA,gBACA,OACA,mBACA,aAAqC,yBACrC,CAEA,iBACA,iBACA,OACA,mBACA,aAAqC,yBACrC,CAEA,uBACA,OACA,YACA,MACA,wCAA8E,EAAM,MACpF,gCAAyD,SAAS,OAAU,EAE5E,CAEA,SACA,UAA8B,IAA6B,EAC3D,wCAA+D,YAAe,kBACzD,CAErB,CAEA,CACA,CAAW,CACX,CAAS,EACT,KAEA,kBACA,SACA,KACA,eACA,eACA,WACA,UACA,KAEA,iBACA,QACA,gBACA,gBACA,UACA,gBACA,gCACA,CACA,CAAe,CAGf,CAEA,MACA,cACA,UAAsB,IAA6B,EACnD,gFACA,CAAa,EAEb,QACA,iBACA,UACA,6CACA,CAAW,CACX,EAAU,IACV,QACA,iBACA,UACA,8BACA,CAAW,EAEX,KACA,CACA,WACA,eACA,EACA,QACA,gBACA,gBACA,gCACA,CAAa,EAEb,QACA,YACA,0BACA,gCACA,CAAa,EAGb,KAEA,SAEA,iCADA,EAC8D,EAE9D,CAEA,gBAAW,aACX,EAyOA,CACA,SACA,2BACA,kBAgnBA,EAFA,EA9mBA,cAmnBA,kEAFA,QAhnBA,GAEA,aACA,OAEA,mBAEA,mCACA,sFACA,sJACA,wBACA,oDAEA,aACA,cACA,QACA,oBACA,mBACA,iGACA,mBACA,aACA,gBACA,UACA,mCACA,0BAEA,EAAQ,CAAI,oBAAsB,OAClC,OACA,OAGA,qFACA,uDACA,6DACA,iEACA,sHAEA,UACA,EAuEA,OAtEA,iBACA,sBACA,qBACA,QACA,2BACA,sBACA,2DACA,CAAS,GAET,gBACA,eACA,QACA,2BACA,eACA,oDACA,CAAS,GAET,4BACA,2BACA,QACA,2BACA,2BACA,gEACA,CAAS,GAET,2BACA,0BACA,QACA,2BACA,0BACA,+DACA,CAAS,GAET,qBACA,oBACA,QACA,aACA,yDACA,CAAS,GAET,mBACA,kBACA,QACA,aACA,wDACA,CAAS,GAET,uBACA,sBACA,QACA,aACA,2DACA,CAAS,GAET,qBACA,+BACA,uCAEA,sBAEM,2GACN,sBACA,qBACA,QACA,2BACA,sBACA,0FACA,CAAS,GAGT,GACA,eACA,UAAgB,4DAxRhB,UACA,OACA,8BACA,oBACC,EACD,MACA,yDACA,KACA,WACA,OAAa,gDAEb,mBACA,MACA,SACA,eACA,4BACA,QAA4B,+BAAgC,EAE5D,QACA,YACA,0BACA,wBACS,EAGT,WACA,OACA,YACA,qBACA,cACA,EAGA,OADA,QAEA,WACA,WACA,YACA,OACA,YACA,qBACA,cACA,CACA,gBACA,UAAkB,IAA8B,EAChD,iEACA,CAAS,CACT,SACA,OACA,YACA,eAA2B,gBAA2B,CACtD,cACA,CACA,CACA,CACA,SACA,eACA,4BACA,QAA0B,+BAAgC,EAE1D,QACA,gBACA,UACA,YACA,0BACA,wBACA,kBACA,CACA,CAAO,EAGP,WACA,OAAa,2CAEb,aACA,UACA,WACA,WACA,eACA,OAAe,qCACf,YACA,OACA,QACA,aACA,gBACA,UACA,eACA,CACA,CAAS,CACT,cACA,CACA,SAEA,UAAgB,IAA8B,EAC9C,+CAFA,EAEyE,EAClE,CAEP,CACA,EAuL6E,CAC7E,OACA,2BACA,iDACS,EACT,OACA,MACA,KACA,QACA,cACA,YACA,eACA,CAAW,CACX,qBAEA,CACA,kBACA,OACA,MACA,KACA,gEACA,mBACA,aACA,gBACA,UACA,mCACA,0BAEA,EAAc,CAAI,mBAClB,CAAW,CACX,UACA,CAEA,mBACA,OACA,QACA,KACA,eACA,iBACa,CACb,WACA,CACA,iBACA,+BACA,8BAEA,EACY,CACZ,KACA,aACA,gBACA,UAA0B,iBAC1B,CAAa,CACb,OACA,CACA,gBACA,UACA,iBACA,+BACA,6BACA,+CACA,CACA,EACA,CACW,CACX,UACA,CAEA,SAEA,iCADA,EAC8D,EAE9D,CACA,CACA,oBACA,oBACA,IAAY,mBAAuB,gBACnC,CACA,kBACA,QACA,WACA,CAAM,MAAQ,QAAa,EAC3B,qBACA,yBACA,oBACA,CAAO,EACP,QAAe,QAAc,kCAC7B,OACA,wBACA,0BAAiC,QAAyB,CAC1D,GAEA,0BACA,wBACK,EACL,CAAY,iBAAsC,EAClD,eACA,uDACA,mDACA,GAA+B,WAa/B,MAZA,2CACA,6DAEA,qDACA,gFAEA,qDACA,gFAEA,wCACA,6DAEA,CACA,0CACA,2EACA,CACA,wBACA,WAAsB,QAAU,GAChC,sCACA,wCAEA,iDACA,MACA,OACA,wBACA,4BAA0D,QAAU,GACpE,yBACA,0BAEA,CAAO,EACP,gCACA,OACA,sEACA,6EACA,CAAO,CACP,mBAAiB,gBAAwB,CACzC,aAAqB,iBAA6C,CAClE,SAAiB,uBAA4B,CAC7C,cACA,WACA,uBACA,kBACA,CACA,CACA,sBAwEA,EAvEA,oCACA,+BAmCA,OACA,OAnCA,oBACA,SAQA,GAPA,WAA+B,uCAA+C,EAC9E,QACA,WACA,kBACA,iBACa,EAEb,YACA,yBACA,WACA,uBACA,wBACA,wBACA,oBACA,qBACe,EACf,WACA,iBACA,KACe,EAGf,WACA,cACA,4BACA,cACA,oBACA,oCACW,EACX,SACA,CACA,CAAO,EAGP,kBACA,0BACA,oBAEA,CACA,SAAY,cAAiB,gBAC7B,GACA,KACA,UAEA,qDAAiE,kBAAsB,MACvF,EACA,iBAAY,WAAmC,MAAQ,QAAa,EACpE,qBACA,yBACA,qBACO,EACP,QAAe,QAAc,kCAC7B,OACA,wBACA,0BAAiC,QAAgC,CACjE,GAEA,0BACA,wBACK,EACL,CAAY,iBAAsC,EAClD,KACA,YACA,GACA,oBACA,uBACA,EAEA,KACA,0BAAY,GAA2B,cACvC,GAA+B,WAC/B,OACA,qBACA,qBACA,eACA,4BACA,eACA,UACA,WAAmC,2BAAmC,EACtE,MACA,CACA,cACA,gBACA,UACA,WAAmC,2BAAmC,EACtE,MACA,CAQA,GAPA,IACA,KACA,WACA,yBACA,QACe,GAEf,eACA,IACA,gBACA,oBACA,wBACA,4BACA,CAAgB,QAChB,GACA,8BACA,iCACA,EACA,2CACA,6DAEA,qDACA,gFAEA,qDACA,gFAEA,wCACA,4DAEA,CACA,mBAIA,GAHA,wCACA,uBAEA,+BACA,OAEA,aACA,kBACA,WACA,kBACA,oBACe,EAEf,QACA,2BAEA,4BACA,mBACA,cAEA,gCACA,CACA,gBACA,GAAoB,QAAU,GAC9B,yBACA,OACA,EACA,cACA,WACA,gBACA,cACA,eACA,uBACA,UAA8B,IAAwB,EACtD,OACA,mCACA,CAAqB,EAErB,cACA,UAA8B,IAAwB,EACtD,OACA,uCACA,CAAqB,EAErB,8CACA,UAA8B,IAAwB,EACtD,OACA,kDACA,CAAqB,CAErB,OACA,QACA,gBACA,UACA,qBACA,6CACA,CAAqB,CACrB,cACA,EACA,WACA,8FACA,+BACA,WACA,uBACA,wBACA,gBACA,yBACA,kCACA,CAAuB,EAEC,QAAc,yBACtC,WACA,iBACA,wBACA,4BAAuE,QAAU,GACjF,yBACA,0BACuB,EACvB,mBAGA,QACA,CACA,YACA,gBAGA,iDACA,+EAEA,WACA,uBACA,wBACA,gBACA,yBACA,iDACA,CAAiB,EACjB,6FAA2J,QAAc,yBACzK,WACA,iBACA,wBACA,4BAAkE,QAAU,GAC5E,yBACA,0BACmB,EACnB,kBAEA,CAEA,CAAW,CACX,SACA,QACA,WACA,cACA,eACA,WACA,OACA,4CACA,mDACA,CAAe,CACf,6BAA8C,GAAmB,GACpD,CACb,CACA,CAAS,GAET,mBAAiB,gBAAwB,CACzC,aAAqB,UAA0B,CAC/C,SAAiB,uBAA4B,CAC7C,UACA,CACA,CACA,EACA,EAA6B,IAAS,EACtC,cAAiB,IAAS,aAC1B,kBAAqB,IAAS,aAC9B,sBAAyB,IAAS,EAClC,cAAmB,IAAS,YAC5B,CAAG,YACH,0BAA6B,IAAS,EACtC,iBAAsB,IAAS,aAC/B,2BAAgC,IAAS,aACzC,2BAAgC,IAAS,YACzC,CAAG,WACH,CAAC,YACD,EAA+B,IAAS,EACxC,GAAM,IAAS,aACf,QAAW,IAAS,aACpB,MAAS,IAAS,aAClB,QAAW,IAAQ,CACf,IAAS,EACb,QAAe,IAAS,EACxB,KAAc,IAAU,wBACxB,QAAiB,IAAS,aAC1B,cAAuB,IAAS,EAChC,UAAqB,IAAS,GAC9B,KAAgB,IAAS,EACzB,CAAS,YACT,WAAoB,IAAQ,CAClB,IAAS,EACnB,GAAgB,IAAS,aACzB,KAAkB,IAAU,aAC5B,SAAsB,IAAS,EAC/B,KAAoB,IAAS,GAC7B,UAAyB,IAAS,EAClC,CAAa,CACb,CAAW,GACX,SACA,CAAO,EACP,MAAa,IAAS,GACtB,SAAgB,IAAS,EACzB,QAAiB,IAAQ,CACf,IAAS,EACnB,MAAmB,IAAS,GAC5B,QAAqB,IAAS,GAC9B,aAA0B,IAAQ,CACpB,IAAS,EACvB,MAAuB,IAAS,GAChC,QAAyB,IAAS,EAClC,CAAe,EAEf,CAAW,GACX,UACA,CAAO,YACP,cAAqB,IAAS,YAC9B,CAAK,GAEL,OACA,CAAC,EACD,EAA4B,IAAQ,EAClC,IAAS,EACX,GAAQ,IAAS,aACjB,QAAa,IAAS,aACtB,MAAW,IAAS,aACpB,QAAa,IAAQ,CACf,IAAS,EACf,MAAe,IAAS,EACxB,KAAgB,IAAO,0BACvB,QAAmB,IAAS,aAC5B,cAAyB,IAAS,EAClC,KAAkB,IAAS,cAC3B,UAAuB,IAAS,aAChC,CAAW,YACX,WAAsB,IAAQ,CAClB,IAAS,EACrB,MAAqB,IAAS,GAC9B,GAAkB,IAAS,aAC3B,KAAoB,IAAU,uBAC9B,SAAwB,IAAS,EACjC,KAAsB,IAAS,aAC/B,UAA2B,IAAS,YACpC,CAAe,CACf,CAAa,GACb,SACA,CAAS,YACT,SAAkB,IAAS,EAC3B,QAAmB,IAAQ,CACf,IAAS,EACrB,MAAqB,IAAS,GAC9B,QAAuB,IAAS,GAChC,aAA4B,IAAQ,CACpB,IAAS,EACzB,MAAyB,IAAS,GAClC,QAA2B,IAAS,EACpC,CAAiB,EAEjB,CAAa,GACb,UACA,CAAS,YACT,cAAuB,IAAS,aAChC,MAAe,IAAS,EACxB,CAAO,GAEP,OACA,CAAG,EACH,EACA,EACA,cACA,wBACA,CAWA,OACA,WACA,0BACA,CAAG,CACH,sBACA,0BACA,CAAG,CACH,cACA,0BACA,CAAG,CACH,yBACA,0BACA,CAAG,CACH,IACA,6BACA,CAAG,CACH,iBACA,6BACA,CAAG,CACH,WACA,6BACA,CAAG,CACH,sBACA,6BACA,CAAG,CACH,WACA,6BACA,CAAG,CACH,sBACA,6BACA,CACA,EAsGA,cACA,4CACA,QACA,4BACA,iEACA,WACA,QACA,SACA,EAAO,EACP,GACA,CAAG,CACH,CAGA,YACA,mBACA,+BACA,wCACA,eACA,gBACA,aACA,CACA,eACA,4BAEA,SACA,OACA,cACA,SACA,YACA,cACA,OACA,OACA,mBACA,kBACA,gBACA,iBACA,OACG,EACH,MACA,aACA,WACA,GACA,QACA,2BACA,cACA,CAAO,EAEP,0BACA,QACA,2BACA,yBACA,gDACA,CAAO,EAEP,IAAY,0BA1IZ,UACA,SACA,cACA,cACA,wBACC,EACD,0GACA,OAAa,6BAEb,SAOA,aAAe,aAAgB,EAN/B,uBACA,MAAe;;AAEf,EACA,cAE+B,GAC/B,UACA,aACA,UAAkB,IAAkB,EACpC,gDAA2D,QAAQ,EACnE,QACA,CAAS,CAET,aACA,gBACA,eACA,WACA,kBAEA,QACA,UAAwB,IAA8B,EACtD,sBACA,CAAe,CAEf,CACA,CAAS,WACT,MAAmB,EAAK;AACxB,EAAE;;AAEF,EACA,KACA,CACA,iBACA,gBACA,eACA,WACA,kBAEA,YACA,UAAwB,IAA8B,EACtD,kCACA,CAAe,CAEf,CACA,CAAS,WACT,MAAmB,EAAU;AAC7B,EAAE;;AAEF,EACA,KACA,CACA,WACA,UAAkB,IAA8B,EAChD,6BACA,CAAS,CAET,SAEA,iCADA,EAC8D,EAE9D,CAIA,OACA,OAHA,MAAa,EAAU;AACvB,EAGA;AACA,EAAE,EAAK,IAEP,EA0DsD,QAAoC,gBAAqB,EAC/G,oCACA,GAEA,mBAEA,wBACA,mCACA,kJACA,4BACA,wBAEA,aACA,cACA,QACA,oBACA,mBACA,OAEA,SAEA,wBACA,EACA,UACA,cACA,qCACA,UAAoB,IAA8B,EAClD,qBACA,CAAW,EAEX,gBACA,UAAoB,IAA8B,EAClD,0BACA,CAAW,EAEX,OAAiB,kBAEjB,mBACA,UAAkB,IAA8B,EAChD,gCACA,CAAS,CAET,mBACA,UAAkB,IAA8B,EAChD,gCACA,CAAS,CAET,SAEA,iCADA,EAC8D,EAE9D,CACA,CACA,oBACA,IAAY,mBAAiB,gBAC7B,CACA,kBACA,QACA,WACA,CAAM,MAAQ,QAAc,EAC5B,qBACA,oBACA,qBACO,EACP,QAAe,QAAe,kCAC9B,OACA,wBACA,0BAAiC,QAA0B,CAC3D,GAEA,0BACA,wBACK,EACL,CAAY,eAAoC,EAChD,eACA,OACA,YACA,OACA,mCACA,2CACO,CACP,gCACA,uBACA,mBAAiB,gBAAwB,CACzC,aAAqB,iBAA6C,CAClE,cACA,WACA,SAAiB,uBACjB,CACA,CACA,kBACA,IA2BA,EA3BA,MAAY,cAAiB,gBAC7B,GACA,KACA,UAEA,qDAAiE,kBAAsB,MACvF,EACA,iBAAY,WAAmC,MAAQ,QAAc,EACrE,qBACA,oBACA,qBACO,EACP,QAAe,QAAe,kCAC9B,OACA,wBACA,0BAAiC,QAAiC,CAClE,GAEA,0BACA,wBACK,EACL,CAAY,eAAoC,EAChD,YACA,GACA,wBACA,6BAGA,KACA,OACA,qBACA,qBACA,eACA,eACA,UACA,WAAmC,2BAAmC,EACtE,MACA,CACA,cACA,gBACA,UACA,WAAmC,2BAAmC,EACtE,MACA,CACA,IACA,KACA,WACA,yBACA,OACA,CAAe,GAEf,eACA,IACA,mCACA,2CACA,EAEA,mBACA,wCACA,uBAEA,+BACA,WACA,kBACA,iBACe,EAEf,QACA,2BAEA,4BACA,mBACA,aAEA,CAAW,CACX,SACA,WACA,cACA,eACA,WACA,OACA,CAAa,CACb,CACA,CAAS,GAET,mBAAiB,gBAAwB,CACzC,aAAqB,UAA0B,CAC/C,WACA,SAAiB,uBACjB,CACA,CACA,EACA,EAAqC,IAAS,EAC9C,GAAM,IAAS,aACf,QAAW,IAAS,aACpB,MAAS,IAAS,aAClB,QAAW,IAAQ,CACf,IAAS,EACb,KAAY,IAAS,GACrB,cAAqB,IAAS,GAC9B,SAAgB,IAAS,EACzB,OAAgB,IAAQ,CAAC,IAAS,IAClC,eAAwB,IAAQ,CAAC,IAAS,IAC1C,aAAsB,IAAQ,CAAC,IAAS,CAAC,IAAS,GAAI,IAAS,eAC/D,CAAO,WACP,CAAK,GAEL,MAAS,IAAS,EAClB,cAAmB,IAAS,GAC5B,kBAAuB,IAAS,EAChC,CAAG,CACH,CAAC,EACD,EAAkC,IAAQ,EACxC,IAAS,EACX,GAAQ,IAAS,aACjB,QAAa,IAAS,aACtB,MAAW,IAAS,aACpB,QAAa,IAAQ,CACf,IAAS,EACf,KAAc,IAAS,GACvB,cAAuB,IAAS,aAChC,MAAe,IAAS,GACxB,SAAkB,IAAS,EAC3B,OAAkB,IAAQ,CAAC,IAAS,IACpC,eAA0B,IAAQ,CAAC,IAAS,IAC5C,aAAwB,IAAQ,CAAC,IAAS,CAAC,IAAS,GAAI,IAAS,eACjE,CAAS,WACT,CAAO,GAEP,MAAW,IAAS,EACpB,cAAqB,IAAS,GAC9B,kBAAyB,IAAS,EAClC,CAAK,WACL,CAAG,EACH,EACA,EAYA,QACA,mBACA,+BACA,eACA,gBACA,aACA,CACA,eACA,4BAEA,2BACA,MACA,0DACA,CACA,4BACA,MACA,uDACA,CACA,eACA,SACA,UACA,cACG,EACH,sCACA,UAAgB,IAAkC,EAClD,uBACA,qBACA,+CACA,QACA,CAAO,EAEP,oBAAY,WAAmC,MAAQ,QAAc,EACrE,qBACA,mBACA,oBACA,CAAO,EACP,QAAe,QAAe,0BAC9B,MACA,mBACA,QACA,wBACA,oCACA,wBACO,CACP,wBACA,0BAAiC,QAA0B,CAC3D,GAEA,cACA,wBACK,EACL,OACA,sCACA,eAAgC,8BAAuC,OACvE,aAAqB,UACrB,CACA,CACA,EACA,EAAwC,IAAS,EACjD,KAAQ,IAAQ,CAAC,IAAS,EAAG,UAAW,IAAQ,CAAC,IAAS,IAAK,GAC/D,MAAS,IAAS,EAAG,cAAe,IAAS,GAAI,WACjD,CAAC,EAWD,GACA,aACA,cACA,gBACA,EACA,2BAGA,QACA,mBACA,eACA,gBACA,cACA,8BACA,CACA,uBACA,QACA,+EACA,CACA,eACA,4BAEA,kBACA,SACA,IACA,OACA,cACA,OACA,kBACA,UACA,cACG,EACH,YACA,eACA,GACA,QACA,2BACA,sBACA,uEACA,CAAO,EAEP,SACA,QAAsB,0CAA8C,EAEpE,2GACA,CAAY,2BAAmC,MAAQ,QAAc,EACrE,qBACA,2BACA,qBACO,EACP,QAAe,QAAe,0BAC9B,MACA,mBACA,SACA,IACA,OACA,0BAA0D,CAC1D,yBAA2D,4BAA8B,EACzF,CAAO,CACP,wBACA,0BAAiC,QAA0B,CAC3D,GAEA,cACA,wBACK,EACL,OACA,iCACA,WACA,UACA,YACA,qBACA,SACA,CACA,CACA,CACA,EACA,EAAgC,IAAS,EACzC,KAAQ,IAAQ,CAAC,IAAS,EAAG,SAAU,IAAS,GAAI,EACpD,CAAC,EAWD,EAAkC,IAAS,EAC3C,QAAW,IAAQ,CAAC,IAAS,cAC7B,SAAY,IAAS,aACrB,OAAU,IAAS,aACnB,YAAe,IAAS,sCACxB,uBAA0B,IAAQ,CAAC,IAAO,oDAC1C,CAAC,EACD,GACA,eACA,YACA,cACA,iBACA,gBACA,aACA,eACA,aACA,aACA,cACA,WACA,YACA,WACA,aACA,cACA,aACA,YACA,cACA,YACA,WACA,YACA,WACA,eACA,eACA,gBACA,aACA,cACA,aACA,YACA,YACA,aACA,gBACA,gBACA,WACA,aACA,WACA,YACA,eACA,aACA,YACA,gBACA,cACA,aACA,aACA,YACA,eACA,aACA,aACA,aACA,aACA,WACA,UACA,aACA,eACA,UACA,gBACA,UACA,EACA,QACA,iBACA,eACA,cACA,8BACA,CACA,eACA,4BAEA,SACA,QACA,YACA,kBACG,EACH,cAEA,MAA0B,QAAoB,EAC9C,kBACA,kBACA,QACA,CAAK,EACL,eACA,kDAA6E,QAAyB,MAGtG,GAFA,+BACA,sCAAwD,OAAiB,GACzE,GACA,OACA,qCACA,uCACA,mCACA,6CACA,mEACA,EACA,gBACA,UACA,aACA,qBAEA,CACA,CACA,OACA,WACA,SA3BA,EA4BA,CACA,CACA,oBACA,gBACA,2GACA,CAAY,uBAAqB,gBACjC,CACA,QACA,kBACA,WACA,CAAM,MAAQ,OAAiB,EAC/B,qBACA,6BACA,qBACO,EACP,QAAe,QAAe,kCAC9B,WACA,wBACA,0BAAiC,QAA0B,CAC3D,GAEA,0BACA,wBACK,EACL,yDACA,OACA,YACA,sDACA,YACA,oBACA,gBACA,CAAO,QACP,WACA,gDACA,WACA,UACA,YACA,qBACA,UACA,MACA,CACA,CACA,CACA,EACA,EAAwC,IAAS,EACjD,KAAQ,IAAS,GACjB,SAAY,IAAS,aACrB,SAAY,IAAS,aACrB,MAAS,IAAQ,CACb,IAAS,EACb,KAAY,IAAS,GACrB,MAAa,IAAS,GACtB,IAAW,IAAS,EACpB,CAAK,GACL,SACA,CAAC,EA0ID,YACA,eACA,eACC,EACD,UACA,YACA,UACA,4BACA,yBACA,cACA,sBACA,sBACA,SACA,+BACA,CACA,CAsFA,YACA,iBACA,+BACA,wCACA,kCACA,eACA,aACA,CACA,eACA,4BAEA,SACA,OACA,YACA,cACA,gBACA,OACA,OACA,kBACA,mBACA,OACA,SACA,mBACA,iBACG,MACH,MAkiBA,EAjiBA,SACA,EAiiBA,CADA,EAhiBA,cAiiBA,gBACA,oDACA,CACA,oBACA,2BACA,yBACA,EAEA,CACA,oBACA,8BACA,yBACA,EAEA,CACA,oBACA,2BACA,yBACA,EAljBA,eACA,GACA,QACA,2BACA,cACA,CAAO,EAEP,SACA,QACA,2BACA,cACA,CAAO,EAEP,SACA,QACA,2BACA,yBACA,CAAO,EAEP,SACA,QACA,2BACA,0BACA,CAAO,EAEP,SACA,QACA,2BACA,uBACA,CAAO,EAEP,aAAY,cAxRZ,UACA,SACA,oBACC,EACD,SACA,KACA,aAAe,aAAgB,IAC/B,UACA,aACA,UACA,aACA,QAA4B,wBAAyB,EACrD,KAEA,iBACA,QAA4B,2BAA4B,EACxD,KAEA,cACA,QACA,aACA,oDACA,CAAa,EACb,KAEA,SAEA,YACA,oCAFA,EAEmE,EAGnE,CACA,KAEA,YACA,QACA,YACA,sBACA,YACA,eACA,WACA,OAAyB,8BAEzB,aACA,OACA,mBACA,4DAAyF,qCAAkD,SAAS,QAA0B,UAAa,EAE3L,mFAGA,YACA,yBACA,UAA4B,IAA8B,EAC1D,0CACA,CAAmB,EAEnB,GACA,oBADA,WAEA,OACA,kBACA,wCAA4E,EAAM,MAClF,gCAAuD,SAAS,OAAU,CAC1E,CAGA,WAA8B,IAA8B,EAC5D,6DACA,CAAqB,CAIrB,CACA,CAAW,CACX,CAAS,EACT,KAEA,iBACA,eACA,eACA,WACA,QACA,iBACA,UAA4B,+BAAsC,EACnD,EACf,KAEA,iBACA,QACA,qBACA,qBACA,gBACA,gCACA,CAAe,CAGf,CAEA,KAEA,YACA,eACA,QACA,4BACA,qBACA,+BACA,CAAW,EAEX,KAEA,SAEA,iCADA,EAC8D,EAE9D,CAEA,gBAAW,aACX,EAmKkD,CAClD,SACA,sCACK,EACL,aACA,MAA0B,QAAqB,EAC/C,kBACA,kBACA,QACA,CAAK,EACL,8CACA,GACA,mBACA,QACA,cACA,QACA,oBACA,sCACA,MACA,uBACA,mBACA,SACA,mCACA,0BACA,iBACY,CAAI,mBAChB,CACA,CAAO,CAEP,mCACA,uDACA,yDACA,6BACA,2BACA,2CAEA,+GACA,WACA,8CACA,yBACW,CACX,+CACA,2BACA,CAEA,CAAO,CACP,8BACA,iBACA,GAoBA,OAlBA,qBACA,sBACA,qBACA,QACA,2BACA,sBACA,2DACA,CAAS,GAET,gBACA,eACA,QACA,2BACA,eACA,oDACA,CAAS,IAGT,GACA,eACA,UAAgB,gCAAmC,SAjNnD,CACA,OACA,SACC,EACD,MACA,yDACA,KACA,WACA,OAAa,gDAEb,mBACA,KACA,eACA,eACA,eACA,QACA,gBACA,YACA,0BACA,wBACA,kBACA,CAAS,EACT,KACA,wBAEA,8BADA,KAEA,QACA,0BACA,6CACA,kCACa,EAGb,QAAgC,+BAAgC,EAGhE,KACA,SACA,QAA4B,+BAAgC,CAE5D,CAEA,WACA,OAAa,2CAEb,aACA,UACA,WACA,WACA,eACA,OAAe,qCACf,YACA,qCACA,OACA,QACA,aACA,yBACA,CAAW,CACX,cACA,EAEA,OACA,QACA,aACA,gBACA,gBACS,CACT,cACA,CAEA,SAEA,UAAgB,IAA8B,EAC9C,+CAFA,EAEyE,EAClE,CAEP,CACA,EAoImD,CACnD,OACA,QAEA,CAAS,EACT,OACA,MACA,KACA,QACA,aACA,CAAW,CACX,qBAEA,CACA,kBACA,OACA,MACA,KACA,MACA,uBACA,mBACA,SACA,mCACA,0BACA,iBACgB,CAAI,mBACpB,CACA,CAAW,CACX,UACA,CAEA,mBACA,OACA,MACA,KACA,aAA2B,iCAAwC,CACnE,OACA,CACA,gBACA,iBACA,+BACA,6BACA,QACA,EACA,CACW,CACX,UACA,CAEA,SAEA,iCAA6C,EAAiB,EAE9D,CACA,CACA,oBACA,kBACA,IAAY,mBAAuB,gBACnC,CACA,kBACA,QACA,WACA,CAAM,MAAQ,QAAc,EAC5B,qBACA,kBACA,qBACO,EACP,QAAe,QAAe,kCAC9B,OACA,wBACA,0BAAiC,QAA0B,CACnD,IAAS,EACjB,GAAc,IAAS,GACvB,WAAsB,IAAS,GAC/B,MAAiB,IAAS,GAC1B,OAAkB,IAAQ,CACd,IAAqB,SACnB,IAAS,EACvB,KAAsB,IAAU,YAChC,KAAsB,IAAU,cAChC,QAAyB,IAAQ,CACf,IAAS,EAC3B,KAA0B,IAAU,gBACpC,KAA0B,IAAS,GACnC,YAAiC,IAAQ,CACnB,IAAS,EAC/B,KAA8B,IAAU,iBACxC,YAAqC,IAAS,GAC9C,UAAmC,IAAS,GAC5C,IAA6B,IAAS,GACtC,MAA+B,IAAS,EACxC,CAAuB,EAEvB,CAAmB,EAEnB,CAAe,EACD,IAAS,EACvB,KAAsB,IAAU,kBAChC,QAAyB,IAAS,GAClC,KAAsB,IAAS,GAC/B,UAA2B,IAAS,EACpC,CAAe,EACD,IAAS,EACvB,KAAsB,IAAU,mBAChC,CAAe,EACD,IAAS,EACvB,KAAsB,IAAU,iBAChC,CAAe,EACD,IAAS,EACvB,KAAsB,IAAU,cAChC,QAAyB,IAAQ,CACf,IAAS,EAC3B,KAA0B,IAAU,iBACpC,KAA0B,IAAS,EACnC,CAAmB,EAEnB,CAAe,EACf,GAEA,mBAA8B,IAAS,EAAG,OAAQ,IAAS,GAAI,aAC/D,OACA,CAAS,GAET,0BACA,wBACK,EACL,iGACA,wDACA,wBACA,qBACA,gBACA,iBACA,CAAK,EACL,qFACA,OACA,iCACA,kBACA,yBACA,UACA,OACA,iBACA,qEAAgH,QAAW,GAC3H,UACA,cAEA,CAAS,GAET,gBACA,4DACA,uBACA,CAAO,EACP,8BACA,uBACA,YACA,WACA,EAAO,SACP,OACA,kCACA,sCACA,CAAO,CACP,SACA,iBACA,cACA,CAAO,CACP,aACA,UACA,MACA,CAAO,CACP,SACA,sBACA,CAAO,CACP,UACA,QACA,qCACA,gBACO,CACP,kBACA,QACA,gBACA,kGACA,kGACA,CACA,CAAO,CACP,UACA,CACA,CACA,kBACA,IAAY,mBAAuB,gBACnC,CAAY,2BAAmC,MAAQ,QAAc,EACrE,qBACA,kBACA,qBACO,EACP,QAAe,QAAe,kCAC9B,MACA,KACA,SACA,CAAO,CACP,wBACA,0BAAiC,QAAiC,CAClE,GAEA,0BACA,wBACK,EACL,OACA,YACA,MACA,MACA,OACA,OACA,OACA,KACA,KACA,OACA,qBACA,qBACA,mBACA,gBA8MA,EA7MA,eACA,UACA,WAAmC,2BAAmC,EACtE,MACA,CACA,cACA,GAiNA,sCAhNA,gCACA,mBACA,qBACA,2BAEA,WACA,uBACA,wBACA,0BACA,qBACA,+BACiB,QAEH,GAgMd,kDAhMc,CACd,8BACA,GACA,WACA,uBACA,wBACA,wBACA,oBACA,sBACiB,CAEjB,EAAc,IAkLd,6BAjLA,gBACA,WACA,yBACA,iBACA,8CACA,yBACe,GAkKf,sCAhKA,WACA,kBACA,iBACA,CAAe,EAkLf,0CAjLc,EAiLd,KAhLA,WACA,iBACA,kBACe,EA2Jf,sCA1Jc,+BACd,yBACA,KACA,WACA,iBACA,wBACA,0BACA,qBACA,sBACe,GAoJf,wBADA,EAlJc,GAmJd,sCAlJA,KACA,qEACA,cACA,CAAe,EACf,gCACA,iCACA,uFACA,4FACc,4CAsJd,MArJA,WACA,cACA,QACA,iBACA,kEAAgH,QAAW,GAC3H,qBACA,yBAEA,CAAe,CACf,CACA,CAAW,CACX,SACA,WACA,cACA,eACA,oBAAuB,qBAAgC,CACvD,wBACA,kBACA,QACA,aACA,qBACA,iBACA,CACA,CACA,EACa,CACb,CACA,CAAS,GAET,SACA,iBACA,cACA,CAAO,CACP,aAAqB,UAA0B,CAC/C,SAAiB,uBAA4B,CAC7C,UACA,CACA,CACA,EACA,EAAkB,IAAS,EAC3B,aAAgB,IAAS,GACzB,qBAAwB,IAAS,EAAG,cAAe,IAAS,aAAc,YAC1E,cAAiB,IAAS,GAC1B,sBAAyB,IAAS,EAAG,iBAAkB,IAAS,aAAc,WAC9E,CAAC,EACD,EAA2B,IAAS,EACpC,KAAQ,IAAU,+BAClB,MAAS,IAAS,EAClB,CAAC,EACD,EAAkC,IAAS,EAC3C,KAAQ,IAAO,+CACf,SAAY,IAAS,EACrB,mBAAwB,IAAS,EAAG,OAAQ,IAAS,GAAI,YACzD,OACA,CAAG,CACH,CAAC,EACD,EAAiC,IAAS,EAC1C,KAAQ,IAAU,qBAClB,SAAY,IAAS,EACrB,GAAQ,IAAS,GACjB,WAAgB,IAAS,GACzB,MAAW,IAAS,EACpB,CAAG,CACH,CAAC,EACD,EAAmC,IAAS,EAC5C,KAAQ,IAAU,8BAClB,aAAgB,IAAS,GACzB,KAAQ,IAAqB,SACzB,IAAS,EACb,KAAY,IAAU,WACtB,CAAK,EACD,IAAS,EACb,KAAY,IAAU,kBACtB,GAAU,IAAS,GACnB,QAAe,IAAS,GACxB,KAAY,IAAS,GACrB,UAAiB,IAAS,GAC1B,OAAc,IAAU,aACxB,CAAK,EACL,CACA,CAAC,EACD,EAA+C,IAAS,EACxD,KAAQ,IAAU,2CAClB,QAAW,IAAS,GACpB,aAAgB,IAAS,GACzB,MAAS,IAAS,EAClB,CAAC,EACD,EAAoC,IAAS,EAC7C,KAAQ,IAAU,+BAClB,aAAgB,IAAS,GACzB,KAAQ,IAAqB,SACzB,IAAS,EACb,KAAY,IAAU,WACtB,CAAK,EACD,IAAS,EACb,KAAY,IAAU,kBACtB,GAAU,IAAS,GACnB,QAAe,IAAS,GACxB,KAAY,IAAS,GACrB,UAAiB,IAAS,EAC1B,CAAK,EACL,CACA,CAAC,EACD,EAAoC,IAAS,EAC7C,KAAQ,IAAU,0CAClB,WAAc,IAAS,EACvB,KAAU,IAAU,iBACpB,IAAS,IAAS,GAClB,MAAW,IAAS,EACpB,CAAG,CACH,CAAC,EACD,EAA8C,IAAS,EACvD,KAAQ,IAAU,0CAClB,QAAW,IAAS,GACpB,aAAgB,IAAS,GACzB,cAAiB,IAAS,GAC1B,MAAS,IAAS,EAClB,CAAC,EACD,EAAiC,IAAQ,EACzC,EACA,EACA,EACA,EACA,EACA,EACA,EACA,EACE,IAAS,EAAG,KAAM,IAAS,GAAI,gBAEjC,EA8CA,EAA2C,IAAS,EACpD,SAAY,IAAM,aAClB,kBAAqB,IAAU,aAC/B,mBAAsB,IAAS,aAC/B,MAAS,IAAU,aACnB,KAAQ,IAAS,aACjB,gBAAmB,IAAS,aAC5B,cAAiB,IAAU,aAC3B,aAAgB,IAAS,aACzB,iBAAoB,IAAS,YAC7B,CAAC,EAID,EAAiC,IAAS,GAAG,EAe7C,GACA,iBAfA,UACA,oBACA,eACA,CAAE,EAAI,EACN,OACA,wBACA,+BACA,MACA,oBACA,cACA,CAAK,CACL,YACA,CACA,CAGA,EAUA,EAAkC,IAAS,EAC3C,aAAgB,IAAS,aACzB,MAAS,IAAS,uCAClB,CAAC,EACD,QACA,iBACA,eACA,cACA,8BACA,CACA,eACA,4BAEA,SACA,OACA,gBACA,qBACA,QACA,eACA,kBACG,EACH,SACA,EAA0B,QAAqB,EAC/C,kBACA,kBACA,QACA,CAAK,EACL,GACA,mBACA,QACA,QACA,sBACA,QACA,cACA,EAYA,GAXA,IACA,oDACA,oBAEA,QACA,2BACA,uBACA,sCAAiD,EAAa,sBACrD,GAGT,GACA,SACA,gBACA,gBACA,OACA,QAEA,CACA,CACA,OACA,cACA,UACA,CACA,CACA,oBACA,UACA,2GACA,aAAY,cAAwB,gBACpC,CACA,QACA,kBACA,WACA,CAAM,MAAQ,QAAc,EAC5B,qBACA,qBACA,qBACO,EACP,QAAe,QAAe,kCAC9B,OACA,wBACA,0BAAiC,QAA2B,GAC5D,0BACA,uBACA,CAAK,EACL,OACA,QACA,WACA,SACA,sBACA,CAAO,CACP,UACA,YACA,qBACA,UACA,MACA,CACA,CACA,CACA,EAGA,eAAkC,EAClC,UACA,eAAwB,QAAoB,2CAC5C,2CACA,8BACA,QACA,wBAA6B,QAAU,EACvC,gBACA,yCACA,oBACA,CAAK,EAAE,EACP,qCACA,2BACA,aACA,CAAG,CACH,SAAiD,cACjD,YAAiB,EAAa,OAC9B,WAAY,EAAM,MAAQ,EAAQ,EAAE,EAAK,EACzC,UACA,gBACA,cACG,EACH,SAAuD,cACvD,YAAiB,EAAa,aAC9B,WAAY,EAAM,MAAQ,EAAQ,EAAE,EAAK,EACzC,UACA,gBACA,cACG,EACH,SAAsD,cACtD,YAAiB,EAAa,YAC9B,MAAY,OAAM,MAAQ,EAAQ,EAAE,EAAK,EACzC,UACA,cACG,EACH,SAAkD,cAClD,YAAiB,EAAa,QAC9B,WAAY,EAAM,MAAQ,EAAQ,EAAE,EAAK,EACzC,UACA,cACG,EACH,cACA,YAAiB,EAAa,gBAC9B,WAAY,EAAM,MAAQ,EAAQ,EAAE,EAAK,EACzC,UACA,cACG,EACH,cACA,YAAiB,EAAa,SAC9B,MAAY,OAAM,MAAQ,EAAQ,EAAE,EAAK,EACzC,UACA,cACG,EACH,UACA,cACA,YACA,0EAGA,6BACA,EACA,EACA,GAGA,MACA,EASA,gBACA,aACA,EAeA,OAdA,kBACA,SACA,eACA,YAdA,GACA,SACA,YAAmB,EAAa,YAChC,WAAc,EAAM,MAAQ,EAAQ,EAAE,EAAK,EAC3C,UACA,aACA,CAAK,EASL,cACA,kBACA,uBACA,UACA,eACA,kBACA,uBACA,WACA,gBACA,UACA,CACA,CACA,SACA,sBAEA,CAAC","sources":["webpack://@hijraah/web/../../node_modules/.pnpm/@ai-sdk+openai@1.3.22_zod@3.25.72/node_modules/@ai-sdk/openai/dist/index.mjs"],"sourcesContent":["// src/openai-provider.ts\nimport {\n  loadApiKey,\n  withoutTrailingSlash\n} from \"@ai-sdk/provider-utils\";\n\n// src/openai-chat-language-model.ts\nimport {\n  InvalidResponseDataError,\n  UnsupportedFunctionalityError as UnsupportedFunctionalityError3\n} from \"@ai-sdk/provider\";\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonResponseHandler,\n  generateId,\n  isParsableJson,\n  postJsonToApi\n} from \"@ai-sdk/provider-utils\";\nimport { z as z2 } from \"zod\";\n\n// src/convert-to-openai-chat-messages.ts\nimport {\n  UnsupportedFunctionalityError\n} from \"@ai-sdk/provider\";\nimport { convertUint8ArrayToBase64 } from \"@ai-sdk/provider-utils\";\nfunction convertToOpenAIChatMessages({\n  prompt,\n  useLegacyFunctionCalling = false,\n  systemMessageMode = \"system\"\n}) {\n  const messages = [];\n  const warnings = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        switch (systemMessageMode) {\n          case \"system\": {\n            messages.push({ role: \"system\", content });\n            break;\n          }\n          case \"developer\": {\n            messages.push({ role: \"developer\", content });\n            break;\n          }\n          case \"remove\": {\n            warnings.push({\n              type: \"other\",\n              message: \"system messages are removed for this model\"\n            });\n            break;\n          }\n          default: {\n            const _exhaustiveCheck = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`\n            );\n          }\n        }\n        break;\n      }\n      case \"user\": {\n        if (content.length === 1 && content[0].type === \"text\") {\n          messages.push({ role: \"user\", content: content[0].text });\n          break;\n        }\n        messages.push({\n          role: \"user\",\n          content: content.map((part, index) => {\n            var _a, _b, _c, _d;\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"text\", text: part.text };\n              }\n              case \"image\": {\n                return {\n                  type: \"image_url\",\n                  image_url: {\n                    url: part.image instanceof URL ? part.image.toString() : `data:${(_a = part.mimeType) != null ? _a : \"image/jpeg\"};base64,${convertUint8ArrayToBase64(part.image)}`,\n                    // OpenAI specific extension: image detail\n                    detail: (_c = (_b = part.providerMetadata) == null ? void 0 : _b.openai) == null ? void 0 : _c.imageDetail\n                  }\n                };\n              }\n              case \"file\": {\n                if (part.data instanceof URL) {\n                  throw new UnsupportedFunctionalityError({\n                    functionality: \"'File content parts with URL data' functionality not supported.\"\n                  });\n                }\n                switch (part.mimeType) {\n                  case \"audio/wav\": {\n                    return {\n                      type: \"input_audio\",\n                      input_audio: { data: part.data, format: \"wav\" }\n                    };\n                  }\n                  case \"audio/mp3\":\n                  case \"audio/mpeg\": {\n                    return {\n                      type: \"input_audio\",\n                      input_audio: { data: part.data, format: \"mp3\" }\n                    };\n                  }\n                  case \"application/pdf\": {\n                    return {\n                      type: \"file\",\n                      file: {\n                        filename: (_d = part.filename) != null ? _d : `part-${index}.pdf`,\n                        file_data: `data:application/pdf;base64,${part.data}`\n                      }\n                    };\n                  }\n                  default: {\n                    throw new UnsupportedFunctionalityError({\n                      functionality: `File content part type ${part.mimeType} in user messages`\n                    });\n                  }\n                }\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        let text = \"\";\n        const toolCalls = [];\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              text += part.text;\n              break;\n            }\n            case \"tool-call\": {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: \"function\",\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.args)\n                }\n              });\n              break;\n            }\n          }\n        }\n        if (useLegacyFunctionCalling) {\n          if (toolCalls.length > 1) {\n            throw new UnsupportedFunctionalityError({\n              functionality: \"useLegacyFunctionCalling with multiple tool calls in one message\"\n            });\n          }\n          messages.push({\n            role: \"assistant\",\n            content: text,\n            function_call: toolCalls.length > 0 ? toolCalls[0].function : void 0\n          });\n        } else {\n          messages.push({\n            role: \"assistant\",\n            content: text,\n            tool_calls: toolCalls.length > 0 ? toolCalls : void 0\n          });\n        }\n        break;\n      }\n      case \"tool\": {\n        for (const toolResponse of content) {\n          if (useLegacyFunctionCalling) {\n            messages.push({\n              role: \"function\",\n              name: toolResponse.toolName,\n              content: JSON.stringify(toolResponse.result)\n            });\n          } else {\n            messages.push({\n              role: \"tool\",\n              tool_call_id: toolResponse.toolCallId,\n              content: JSON.stringify(toolResponse.result)\n            });\n          }\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return { messages, warnings };\n}\n\n// src/map-openai-chat-logprobs.ts\nfunction mapOpenAIChatLogProbsOutput(logprobs) {\n  var _a, _b;\n  return (_b = (_a = logprobs == null ? void 0 : logprobs.content) == null ? void 0 : _a.map(({ token, logprob, top_logprobs }) => ({\n    token,\n    logprob,\n    topLogprobs: top_logprobs ? top_logprobs.map(({ token: token2, logprob: logprob2 }) => ({\n      token: token2,\n      logprob: logprob2\n    })) : []\n  }))) != null ? _b : void 0;\n}\n\n// src/map-openai-finish-reason.ts\nfunction mapOpenAIFinishReason(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\n\n// src/openai-error.ts\nimport { z } from \"zod\";\nimport { createJsonErrorResponseHandler } from \"@ai-sdk/provider-utils\";\nvar openaiErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish()\n  })\n});\nvar openaiFailedResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: openaiErrorDataSchema,\n  errorToMessage: (data) => data.error.message\n});\n\n// src/get-response-metadata.ts\nfunction getResponseMetadata({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\n\n// src/openai-prepare-tools.ts\nimport {\n  UnsupportedFunctionalityError as UnsupportedFunctionalityError2\n} from \"@ai-sdk/provider\";\nfunction prepareTools({\n  mode,\n  useLegacyFunctionCalling = false,\n  structuredOutputs\n}) {\n  var _a;\n  const tools = ((_a = mode.tools) == null ? void 0 : _a.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, tool_choice: void 0, toolWarnings };\n  }\n  const toolChoice = mode.toolChoice;\n  if (useLegacyFunctionCalling) {\n    const openaiFunctions = [];\n    for (const tool of tools) {\n      if (tool.type === \"provider-defined\") {\n        toolWarnings.push({ type: \"unsupported-tool\", tool });\n      } else {\n        openaiFunctions.push({\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters\n        });\n      }\n    }\n    if (toolChoice == null) {\n      return {\n        functions: openaiFunctions,\n        function_call: void 0,\n        toolWarnings\n      };\n    }\n    const type2 = toolChoice.type;\n    switch (type2) {\n      case \"auto\":\n      case \"none\":\n      case void 0:\n        return {\n          functions: openaiFunctions,\n          function_call: void 0,\n          toolWarnings\n        };\n      case \"required\":\n        throw new UnsupportedFunctionalityError2({\n          functionality: \"useLegacyFunctionCalling and toolChoice: required\"\n        });\n      default:\n        return {\n          functions: openaiFunctions,\n          function_call: { name: toolChoice.toolName },\n          toolWarnings\n        };\n    }\n  }\n  const openaiTools2 = [];\n  for (const tool of tools) {\n    if (tool.type === \"provider-defined\") {\n      toolWarnings.push({ type: \"unsupported-tool\", tool });\n    } else {\n      openaiTools2.push({\n        type: \"function\",\n        function: {\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters,\n          strict: structuredOutputs ? true : void 0\n        }\n      });\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: openaiTools2, tool_choice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: openaiTools2, tool_choice: type, toolWarnings };\n    case \"tool\":\n      return {\n        tools: openaiTools2,\n        tool_choice: {\n          type: \"function\",\n          function: {\n            name: toolChoice.toolName\n          }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError2({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\n\n// src/openai-chat-language-model.ts\nvar OpenAIChatLanguageModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get supportsStructuredOutputs() {\n    var _a;\n    return (_a = this.settings.structuredOutputs) != null ? _a : isReasoningModel(this.modelId);\n  }\n  get defaultObjectGenerationMode() {\n    if (isAudioModel(this.modelId)) {\n      return \"tool\";\n    }\n    return this.supportsStructuredOutputs ? \"json\" : \"tool\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get supportsImageUrls() {\n    return !this.settings.downloadImages;\n  }\n  getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    providerMetadata\n  }) {\n    var _a, _b, _c, _d, _e, _f, _g, _h;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if ((responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && !this.supportsStructuredOutputs) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is only supported with structuredOutputs\"\n      });\n    }\n    const useLegacyFunctionCalling = this.settings.useLegacyFunctionCalling;\n    if (useLegacyFunctionCalling && this.settings.parallelToolCalls === true) {\n      throw new UnsupportedFunctionalityError3({\n        functionality: \"useLegacyFunctionCalling with parallelToolCalls\"\n      });\n    }\n    if (useLegacyFunctionCalling && this.supportsStructuredOutputs) {\n      throw new UnsupportedFunctionalityError3({\n        functionality: \"structuredOutputs with useLegacyFunctionCalling\"\n      });\n    }\n    const { messages, warnings: messageWarnings } = convertToOpenAIChatMessages(\n      {\n        prompt,\n        useLegacyFunctionCalling,\n        systemMessageMode: getSystemMessageMode(this.modelId)\n      }\n    );\n    warnings.push(...messageWarnings);\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      logit_bias: this.settings.logitBias,\n      logprobs: this.settings.logprobs === true || typeof this.settings.logprobs === \"number\" ? true : void 0,\n      top_logprobs: typeof this.settings.logprobs === \"number\" ? this.settings.logprobs : typeof this.settings.logprobs === \"boolean\" ? this.settings.logprobs ? 0 : void 0 : void 0,\n      user: this.settings.user,\n      parallel_tool_calls: this.settings.parallelToolCalls,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? this.supportsStructuredOutputs && responseFormat.schema != null ? {\n        type: \"json_schema\",\n        json_schema: {\n          schema: responseFormat.schema,\n          strict: true,\n          name: (_a = responseFormat.name) != null ? _a : \"response\",\n          description: responseFormat.description\n        }\n      } : { type: \"json_object\" } : void 0,\n      stop: stopSequences,\n      seed,\n      // openai specific settings:\n      // TODO remove in next major version; we auto-map maxTokens now\n      max_completion_tokens: (_b = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _b.maxCompletionTokens,\n      store: (_c = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _c.store,\n      metadata: (_d = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _d.metadata,\n      prediction: (_e = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _e.prediction,\n      reasoning_effort: (_g = (_f = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _f.reasoningEffort) != null ? _g : this.settings.reasoningEffort,\n      // messages:\n      messages\n    };\n    if (isReasoningModel(this.modelId)) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_p != null) {\n        baseArgs.top_p = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topP\",\n          details: \"topP is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.frequency_penalty != null) {\n        baseArgs.frequency_penalty = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"frequencyPenalty\",\n          details: \"frequencyPenalty is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.presence_penalty != null) {\n        baseArgs.presence_penalty = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"presencePenalty\",\n          details: \"presencePenalty is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.logit_bias != null) {\n        baseArgs.logit_bias = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"logitBias is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.logprobs != null) {\n        baseArgs.logprobs = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"logprobs is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_logprobs != null) {\n        baseArgs.top_logprobs = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"topLogprobs is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.max_tokens != null) {\n        if (baseArgs.max_completion_tokens == null) {\n          baseArgs.max_completion_tokens = baseArgs.max_tokens;\n        }\n        baseArgs.max_tokens = void 0;\n      }\n    } else if (this.modelId.startsWith(\"gpt-4o-search-preview\") || this.modelId.startsWith(\"gpt-4o-mini-search-preview\")) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for the search preview models and has been removed.\"\n        });\n      }\n    }\n    switch (type) {\n      case \"regular\": {\n        const { tools, tool_choice, functions, function_call, toolWarnings } = prepareTools({\n          mode,\n          useLegacyFunctionCalling,\n          structuredOutputs: this.supportsStructuredOutputs\n        });\n        return {\n          args: {\n            ...baseArgs,\n            tools,\n            tool_choice,\n            functions,\n            function_call\n          },\n          warnings: [...warnings, ...toolWarnings]\n        };\n      }\n      case \"object-json\": {\n        return {\n          args: {\n            ...baseArgs,\n            response_format: this.supportsStructuredOutputs && mode.schema != null ? {\n              type: \"json_schema\",\n              json_schema: {\n                schema: mode.schema,\n                strict: true,\n                name: (_h = mode.name) != null ? _h : \"response\",\n                description: mode.description\n              }\n            } : { type: \"json_object\" }\n          },\n          warnings\n        };\n      }\n      case \"object-tool\": {\n        return {\n          args: useLegacyFunctionCalling ? {\n            ...baseArgs,\n            function_call: {\n              name: mode.tool.name\n            },\n            functions: [\n              {\n                name: mode.tool.name,\n                description: mode.tool.description,\n                parameters: mode.tool.parameters\n              }\n            ]\n          } : {\n            ...baseArgs,\n            tool_choice: {\n              type: \"function\",\n              function: { name: mode.tool.name }\n            },\n            tools: [\n              {\n                type: \"function\",\n                function: {\n                  name: mode.tool.name,\n                  description: mode.tool.description,\n                  parameters: mode.tool.parameters,\n                  strict: this.supportsStructuredOutputs ? true : void 0\n                }\n              }\n            ]\n          },\n          warnings\n        };\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async doGenerate(options) {\n    var _a, _b, _c, _d, _e, _f, _g, _h;\n    const { args: body, warnings } = this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiChatResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = body;\n    const choice = response.choices[0];\n    const completionTokenDetails = (_a = response.usage) == null ? void 0 : _a.completion_tokens_details;\n    const promptTokenDetails = (_b = response.usage) == null ? void 0 : _b.prompt_tokens_details;\n    const providerMetadata = { openai: {} };\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens) != null) {\n      providerMetadata.openai.reasoningTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens) != null) {\n      providerMetadata.openai.acceptedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens) != null) {\n      providerMetadata.openai.rejectedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens;\n    }\n    if ((promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens) != null) {\n      providerMetadata.openai.cachedPromptTokens = promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens;\n    }\n    return {\n      text: (_c = choice.message.content) != null ? _c : void 0,\n      toolCalls: this.settings.useLegacyFunctionCalling && choice.message.function_call ? [\n        {\n          toolCallType: \"function\",\n          toolCallId: generateId(),\n          toolName: choice.message.function_call.name,\n          args: choice.message.function_call.arguments\n        }\n      ] : (_d = choice.message.tool_calls) == null ? void 0 : _d.map((toolCall) => {\n        var _a2;\n        return {\n          toolCallType: \"function\",\n          toolCallId: (_a2 = toolCall.id) != null ? _a2 : generateId(),\n          toolName: toolCall.function.name,\n          args: toolCall.function.arguments\n        };\n      }),\n      finishReason: mapOpenAIFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: (_f = (_e = response.usage) == null ? void 0 : _e.prompt_tokens) != null ? _f : NaN,\n        completionTokens: (_h = (_g = response.usage) == null ? void 0 : _g.completion_tokens) != null ? _h : NaN\n      },\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      request: { body: JSON.stringify(body) },\n      response: getResponseMetadata(response),\n      warnings,\n      logprobs: mapOpenAIChatLogProbsOutput(choice.logprobs),\n      providerMetadata\n    };\n  }\n  async doStream(options) {\n    if (this.settings.simulateStreaming) {\n      const result = await this.doGenerate(options);\n      const simulatedStream = new ReadableStream({\n        start(controller) {\n          controller.enqueue({ type: \"response-metadata\", ...result.response });\n          if (result.text) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: result.text\n            });\n          }\n          if (result.toolCalls) {\n            for (const toolCall of result.toolCalls) {\n              controller.enqueue({\n                type: \"tool-call-delta\",\n                toolCallType: \"function\",\n                toolCallId: toolCall.toolCallId,\n                toolName: toolCall.toolName,\n                argsTextDelta: toolCall.args\n              });\n              controller.enqueue({\n                type: \"tool-call\",\n                ...toolCall\n              });\n            }\n          }\n          controller.enqueue({\n            type: \"finish\",\n            finishReason: result.finishReason,\n            usage: result.usage,\n            logprobs: result.logprobs,\n            providerMetadata: result.providerMetadata\n          });\n          controller.close();\n        }\n      });\n      return {\n        stream: simulatedStream,\n        rawCall: result.rawCall,\n        rawResponse: result.rawResponse,\n        warnings: result.warnings\n      };\n    }\n    const { args, warnings } = this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.compatibility === \"strict\" ? { include_usage: true } : void 0\n    };\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        openaiChatChunkSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = args;\n    const toolCalls = [];\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: void 0,\n      completionTokens: void 0\n    };\n    let logprobs;\n    let isFirstChunk = true;\n    const { useLegacyFunctionCalling } = this.settings;\n    const providerMetadata = { openai: {} };\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            var _a, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l;\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata(value)\n              });\n            }\n            if (value.usage != null) {\n              const {\n                prompt_tokens,\n                completion_tokens,\n                prompt_tokens_details,\n                completion_tokens_details\n              } = value.usage;\n              usage = {\n                promptTokens: prompt_tokens != null ? prompt_tokens : void 0,\n                completionTokens: completion_tokens != null ? completion_tokens : void 0\n              };\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.reasoning_tokens) != null) {\n                providerMetadata.openai.reasoningTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.reasoning_tokens;\n              }\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.accepted_prediction_tokens) != null) {\n                providerMetadata.openai.acceptedPredictionTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.accepted_prediction_tokens;\n              }\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.rejected_prediction_tokens) != null) {\n                providerMetadata.openai.rejectedPredictionTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.rejected_prediction_tokens;\n              }\n              if ((prompt_tokens_details == null ? void 0 : prompt_tokens_details.cached_tokens) != null) {\n                providerMetadata.openai.cachedPromptTokens = prompt_tokens_details == null ? void 0 : prompt_tokens_details.cached_tokens;\n              }\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapOpenAIFinishReason(choice.finish_reason);\n            }\n            if ((choice == null ? void 0 : choice.delta) == null) {\n              return;\n            }\n            const delta = choice.delta;\n            if (delta.content != null) {\n              controller.enqueue({\n                type: \"text-delta\",\n                textDelta: delta.content\n              });\n            }\n            const mappedLogprobs = mapOpenAIChatLogProbsOutput(\n              choice == null ? void 0 : choice.logprobs\n            );\n            if (mappedLogprobs == null ? void 0 : mappedLogprobs.length) {\n              if (logprobs === void 0) logprobs = [];\n              logprobs.push(...mappedLogprobs);\n            }\n            const mappedToolCalls = useLegacyFunctionCalling && delta.function_call != null ? [\n              {\n                type: \"function\",\n                id: generateId(),\n                function: delta.function_call,\n                index: 0\n              }\n            ] : delta.tool_calls;\n            if (mappedToolCalls != null) {\n              for (const toolCallDelta of mappedToolCalls) {\n                const index = toolCallDelta.index;\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== \"function\") {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`\n                    });\n                  }\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`\n                    });\n                  }\n                  if (((_a = toolCallDelta.function) == null ? void 0 : _a.name) == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`\n                    });\n                  }\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: \"function\",\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: (_b = toolCallDelta.function.arguments) != null ? _b : \"\"\n                    },\n                    hasFinished: false\n                  };\n                  const toolCall2 = toolCalls[index];\n                  if (((_c = toolCall2.function) == null ? void 0 : _c.name) != null && ((_d = toolCall2.function) == null ? void 0 : _d.arguments) != null) {\n                    if (toolCall2.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: \"tool-call-delta\",\n                        toolCallType: \"function\",\n                        toolCallId: toolCall2.id,\n                        toolName: toolCall2.function.name,\n                        argsTextDelta: toolCall2.function.arguments\n                      });\n                    }\n                    if (isParsableJson(toolCall2.function.arguments)) {\n                      controller.enqueue({\n                        type: \"tool-call\",\n                        toolCallType: \"function\",\n                        toolCallId: (_e = toolCall2.id) != null ? _e : generateId(),\n                        toolName: toolCall2.function.name,\n                        args: toolCall2.function.arguments\n                      });\n                      toolCall2.hasFinished = true;\n                    }\n                  }\n                  continue;\n                }\n                const toolCall = toolCalls[index];\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n                if (((_f = toolCallDelta.function) == null ? void 0 : _f.arguments) != null) {\n                  toolCall.function.arguments += (_h = (_g = toolCallDelta.function) == null ? void 0 : _g.arguments) != null ? _h : \"\";\n                }\n                controller.enqueue({\n                  type: \"tool-call-delta\",\n                  toolCallType: \"function\",\n                  toolCallId: toolCall.id,\n                  toolName: toolCall.function.name,\n                  argsTextDelta: (_i = toolCallDelta.function.arguments) != null ? _i : \"\"\n                });\n                if (((_j = toolCall.function) == null ? void 0 : _j.name) != null && ((_k = toolCall.function) == null ? void 0 : _k.arguments) != null && isParsableJson(toolCall.function.arguments)) {\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallType: \"function\",\n                    toolCallId: (_l = toolCall.id) != null ? _l : generateId(),\n                    toolName: toolCall.function.name,\n                    args: toolCall.function.arguments\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n          flush(controller) {\n            var _a, _b;\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              logprobs,\n              usage: {\n                promptTokens: (_a = usage.promptTokens) != null ? _a : NaN,\n                completionTokens: (_b = usage.completionTokens) != null ? _b : NaN\n              },\n              ...providerMetadata != null ? { providerMetadata } : {}\n            });\n          }\n        })\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      request: { body: JSON.stringify(body) },\n      warnings\n    };\n  }\n};\nvar openaiTokenUsageSchema = z2.object({\n  prompt_tokens: z2.number().nullish(),\n  completion_tokens: z2.number().nullish(),\n  prompt_tokens_details: z2.object({\n    cached_tokens: z2.number().nullish()\n  }).nullish(),\n  completion_tokens_details: z2.object({\n    reasoning_tokens: z2.number().nullish(),\n    accepted_prediction_tokens: z2.number().nullish(),\n    rejected_prediction_tokens: z2.number().nullish()\n  }).nullish()\n}).nullish();\nvar openaiChatResponseSchema = z2.object({\n  id: z2.string().nullish(),\n  created: z2.number().nullish(),\n  model: z2.string().nullish(),\n  choices: z2.array(\n    z2.object({\n      message: z2.object({\n        role: z2.literal(\"assistant\").nullish(),\n        content: z2.string().nullish(),\n        function_call: z2.object({\n          arguments: z2.string(),\n          name: z2.string()\n        }).nullish(),\n        tool_calls: z2.array(\n          z2.object({\n            id: z2.string().nullish(),\n            type: z2.literal(\"function\"),\n            function: z2.object({\n              name: z2.string(),\n              arguments: z2.string()\n            })\n          })\n        ).nullish()\n      }),\n      index: z2.number(),\n      logprobs: z2.object({\n        content: z2.array(\n          z2.object({\n            token: z2.string(),\n            logprob: z2.number(),\n            top_logprobs: z2.array(\n              z2.object({\n                token: z2.string(),\n                logprob: z2.number()\n              })\n            )\n          })\n        ).nullable()\n      }).nullish(),\n      finish_reason: z2.string().nullish()\n    })\n  ),\n  usage: openaiTokenUsageSchema\n});\nvar openaiChatChunkSchema = z2.union([\n  z2.object({\n    id: z2.string().nullish(),\n    created: z2.number().nullish(),\n    model: z2.string().nullish(),\n    choices: z2.array(\n      z2.object({\n        delta: z2.object({\n          role: z2.enum([\"assistant\"]).nullish(),\n          content: z2.string().nullish(),\n          function_call: z2.object({\n            name: z2.string().optional(),\n            arguments: z2.string().optional()\n          }).nullish(),\n          tool_calls: z2.array(\n            z2.object({\n              index: z2.number(),\n              id: z2.string().nullish(),\n              type: z2.literal(\"function\").nullish(),\n              function: z2.object({\n                name: z2.string().nullish(),\n                arguments: z2.string().nullish()\n              })\n            })\n          ).nullish()\n        }).nullish(),\n        logprobs: z2.object({\n          content: z2.array(\n            z2.object({\n              token: z2.string(),\n              logprob: z2.number(),\n              top_logprobs: z2.array(\n                z2.object({\n                  token: z2.string(),\n                  logprob: z2.number()\n                })\n              )\n            })\n          ).nullable()\n        }).nullish(),\n        finish_reason: z2.string().nullish(),\n        index: z2.number()\n      })\n    ),\n    usage: openaiTokenUsageSchema\n  }),\n  openaiErrorDataSchema\n]);\nfunction isReasoningModel(modelId) {\n  return modelId.startsWith(\"o\");\n}\nfunction isAudioModel(modelId) {\n  return modelId.startsWith(\"gpt-4o-audio-preview\");\n}\nfunction getSystemMessageMode(modelId) {\n  var _a, _b;\n  if (!isReasoningModel(modelId)) {\n    return \"system\";\n  }\n  return (_b = (_a = reasoningModels[modelId]) == null ? void 0 : _a.systemMessageMode) != null ? _b : \"developer\";\n}\nvar reasoningModels = {\n  \"o1-mini\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-mini-2024-09-12\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-preview\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-preview-2024-09-12\": {\n    systemMessageMode: \"remove\"\n  },\n  o3: {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-2025-04-16\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-mini\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-mini-2025-01-31\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o4-mini\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o4-mini-2025-04-16\": {\n    systemMessageMode: \"developer\"\n  }\n};\n\n// src/openai-completion-language-model.ts\nimport {\n  UnsupportedFunctionalityError as UnsupportedFunctionalityError5\n} from \"@ai-sdk/provider\";\nimport {\n  combineHeaders as combineHeaders2,\n  createEventSourceResponseHandler as createEventSourceResponseHandler2,\n  createJsonResponseHandler as createJsonResponseHandler2,\n  postJsonToApi as postJsonToApi2\n} from \"@ai-sdk/provider-utils\";\nimport { z as z3 } from \"zod\";\n\n// src/convert-to-openai-completion-prompt.ts\nimport {\n  InvalidPromptError,\n  UnsupportedFunctionalityError as UnsupportedFunctionalityError4\n} from \"@ai-sdk/provider\";\nfunction convertToOpenAICompletionPrompt({\n  prompt,\n  inputFormat,\n  user = \"user\",\n  assistant = \"assistant\"\n}) {\n  if (inputFormat === \"prompt\" && prompt.length === 1 && prompt[0].role === \"user\" && prompt[0].content.length === 1 && prompt[0].content[0].type === \"text\") {\n    return { prompt: prompt[0].content[0].text };\n  }\n  let text = \"\";\n  if (prompt[0].role === \"system\") {\n    text += `${prompt[0].content}\n\n`;\n    prompt = prompt.slice(1);\n  }\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        throw new InvalidPromptError({\n          message: \"Unexpected system message in prompt: ${content}\",\n          prompt\n        });\n      }\n      case \"user\": {\n        const userMessage = content.map((part) => {\n          switch (part.type) {\n            case \"text\": {\n              return part.text;\n            }\n            case \"image\": {\n              throw new UnsupportedFunctionalityError4({\n                functionality: \"images\"\n              });\n            }\n          }\n        }).join(\"\");\n        text += `${user}:\n${userMessage}\n\n`;\n        break;\n      }\n      case \"assistant\": {\n        const assistantMessage = content.map((part) => {\n          switch (part.type) {\n            case \"text\": {\n              return part.text;\n            }\n            case \"tool-call\": {\n              throw new UnsupportedFunctionalityError4({\n                functionality: \"tool-call messages\"\n              });\n            }\n          }\n        }).join(\"\");\n        text += `${assistant}:\n${assistantMessage}\n\n`;\n        break;\n      }\n      case \"tool\": {\n        throw new UnsupportedFunctionalityError4({\n          functionality: \"tool messages\"\n        });\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  text += `${assistant}:\n`;\n  return {\n    prompt: text,\n    stopSequences: [`\n${user}:`]\n  };\n}\n\n// src/map-openai-completion-logprobs.ts\nfunction mapOpenAICompletionLogProbs(logprobs) {\n  return logprobs == null ? void 0 : logprobs.tokens.map((token, index) => ({\n    token,\n    logprob: logprobs.token_logprobs[index],\n    topLogprobs: logprobs.top_logprobs ? Object.entries(logprobs.top_logprobs[index]).map(\n      ([token2, logprob]) => ({\n        token: token2,\n        logprob\n      })\n    ) : []\n  }));\n}\n\n// src/openai-completion-language-model.ts\nvar OpenAICompletionLanguageModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.defaultObjectGenerationMode = void 0;\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    mode,\n    inputFormat,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    seed\n  }) {\n    var _a;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if (responseFormat != null && responseFormat.type !== \"text\") {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format is not supported.\"\n      });\n    }\n    const { prompt: completionPrompt, stopSequences } = convertToOpenAICompletionPrompt({ prompt, inputFormat });\n    const stop = [...stopSequences != null ? stopSequences : [], ...userStopSequences != null ? userStopSequences : []];\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      echo: this.settings.echo,\n      logit_bias: this.settings.logitBias,\n      logprobs: typeof this.settings.logprobs === \"number\" ? this.settings.logprobs : typeof this.settings.logprobs === \"boolean\" ? this.settings.logprobs ? 0 : void 0 : void 0,\n      suffix: this.settings.suffix,\n      user: this.settings.user,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      seed,\n      // prompt:\n      prompt: completionPrompt,\n      // stop sequences:\n      stop: stop.length > 0 ? stop : void 0\n    };\n    switch (type) {\n      case \"regular\": {\n        if ((_a = mode.tools) == null ? void 0 : _a.length) {\n          throw new UnsupportedFunctionalityError5({\n            functionality: \"tools\"\n          });\n        }\n        if (mode.toolChoice) {\n          throw new UnsupportedFunctionalityError5({\n            functionality: \"toolChoice\"\n          });\n        }\n        return { args: baseArgs, warnings };\n      }\n      case \"object-json\": {\n        throw new UnsupportedFunctionalityError5({\n          functionality: \"object-json mode\"\n        });\n      }\n      case \"object-tool\": {\n        throw new UnsupportedFunctionalityError5({\n          functionality: \"object-tool mode\"\n        });\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async doGenerate(options) {\n    const { args, warnings } = this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler2(\n        openaiCompletionResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { prompt: rawPrompt, ...rawSettings } = args;\n    const choice = response.choices[0];\n    return {\n      text: choice.text,\n      usage: {\n        promptTokens: response.usage.prompt_tokens,\n        completionTokens: response.usage.completion_tokens\n      },\n      finishReason: mapOpenAIFinishReason(choice.finish_reason),\n      logprobs: mapOpenAICompletionLogProbs(choice.logprobs),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata(response),\n      warnings,\n      request: { body: JSON.stringify(args) }\n    };\n  }\n  async doStream(options) {\n    const { args, warnings } = this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.compatibility === \"strict\" ? { include_usage: true } : void 0\n    };\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler2(\n        openaiCompletionChunkSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { prompt: rawPrompt, ...rawSettings } = args;\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN\n    };\n    let logprobs;\n    let isFirstChunk = true;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata(value)\n              });\n            }\n            if (value.usage != null) {\n              usage = {\n                promptTokens: value.usage.prompt_tokens,\n                completionTokens: value.usage.completion_tokens\n              };\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapOpenAIFinishReason(choice.finish_reason);\n            }\n            if ((choice == null ? void 0 : choice.text) != null) {\n              controller.enqueue({\n                type: \"text-delta\",\n                textDelta: choice.text\n              });\n            }\n            const mappedLogprobs = mapOpenAICompletionLogProbs(\n              choice == null ? void 0 : choice.logprobs\n            );\n            if (mappedLogprobs == null ? void 0 : mappedLogprobs.length) {\n              if (logprobs === void 0) logprobs = [];\n              logprobs.push(...mappedLogprobs);\n            }\n          },\n          flush(controller) {\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              logprobs,\n              usage\n            });\n          }\n        })\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body: JSON.stringify(body) }\n    };\n  }\n};\nvar openaiCompletionResponseSchema = z3.object({\n  id: z3.string().nullish(),\n  created: z3.number().nullish(),\n  model: z3.string().nullish(),\n  choices: z3.array(\n    z3.object({\n      text: z3.string(),\n      finish_reason: z3.string(),\n      logprobs: z3.object({\n        tokens: z3.array(z3.string()),\n        token_logprobs: z3.array(z3.number()),\n        top_logprobs: z3.array(z3.record(z3.string(), z3.number())).nullable()\n      }).nullish()\n    })\n  ),\n  usage: z3.object({\n    prompt_tokens: z3.number(),\n    completion_tokens: z3.number()\n  })\n});\nvar openaiCompletionChunkSchema = z3.union([\n  z3.object({\n    id: z3.string().nullish(),\n    created: z3.number().nullish(),\n    model: z3.string().nullish(),\n    choices: z3.array(\n      z3.object({\n        text: z3.string(),\n        finish_reason: z3.string().nullish(),\n        index: z3.number(),\n        logprobs: z3.object({\n          tokens: z3.array(z3.string()),\n          token_logprobs: z3.array(z3.number()),\n          top_logprobs: z3.array(z3.record(z3.string(), z3.number())).nullable()\n        }).nullish()\n      })\n    ),\n    usage: z3.object({\n      prompt_tokens: z3.number(),\n      completion_tokens: z3.number()\n    }).nullish()\n  }),\n  openaiErrorDataSchema\n]);\n\n// src/openai-embedding-model.ts\nimport {\n  TooManyEmbeddingValuesForCallError\n} from \"@ai-sdk/provider\";\nimport {\n  combineHeaders as combineHeaders3,\n  createJsonResponseHandler as createJsonResponseHandler3,\n  postJsonToApi as postJsonToApi3\n} from \"@ai-sdk/provider-utils\";\nimport { z as z4 } from \"zod\";\nvar OpenAIEmbeddingModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get maxEmbeddingsPerCall() {\n    var _a;\n    return (_a = this.settings.maxEmbeddingsPerCall) != null ? _a : 2048;\n  }\n  get supportsParallelCalls() {\n    var _a;\n    return (_a = this.settings.supportsParallelCalls) != null ? _a : true;\n  }\n  async doEmbed({\n    values,\n    headers,\n    abortSignal\n  }) {\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values\n      });\n    }\n    const { responseHeaders, value: response } = await postJsonToApi3({\n      url: this.config.url({\n        path: \"/embeddings\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders3(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: \"float\",\n        dimensions: this.settings.dimensions,\n        user: this.settings.user\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler3(\n        openaiTextEmbeddingResponseSchema\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      embeddings: response.data.map((item) => item.embedding),\n      usage: response.usage ? { tokens: response.usage.prompt_tokens } : void 0,\n      rawResponse: { headers: responseHeaders }\n    };\n  }\n};\nvar openaiTextEmbeddingResponseSchema = z4.object({\n  data: z4.array(z4.object({ embedding: z4.array(z4.number()) })),\n  usage: z4.object({ prompt_tokens: z4.number() }).nullish()\n});\n\n// src/openai-image-model.ts\nimport {\n  combineHeaders as combineHeaders4,\n  createJsonResponseHandler as createJsonResponseHandler4,\n  postJsonToApi as postJsonToApi4\n} from \"@ai-sdk/provider-utils\";\nimport { z as z5 } from \"zod\";\n\n// src/openai-image-settings.ts\nvar modelMaxImagesPerCall = {\n  \"dall-e-3\": 1,\n  \"dall-e-2\": 10,\n  \"gpt-image-1\": 10\n};\nvar hasDefaultResponseFormat = /* @__PURE__ */ new Set([\"gpt-image-1\"]);\n\n// src/openai-image-model.ts\nvar OpenAIImageModel = class {\n  constructor(modelId, settings, config) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get maxImagesPerCall() {\n    var _a, _b;\n    return (_b = (_a = this.settings.maxImagesPerCall) != null ? _a : modelMaxImagesPerCall[this.modelId]) != null ? _b : 1;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal\n  }) {\n    var _a, _b, _c, _d;\n    const warnings = [];\n    if (aspectRatio != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"aspectRatio\",\n        details: \"This model does not support aspect ratio. Use `size` instead.\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"seed\" });\n    }\n    const currentDate = (_c = (_b = (_a = this.config._internal) == null ? void 0 : _a.currentDate) == null ? void 0 : _b.call(_a)) != null ? _c : /* @__PURE__ */ new Date();\n    const { value: response, responseHeaders } = await postJsonToApi4({\n      url: this.config.url({\n        path: \"/images/generations\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders4(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(_d = providerOptions.openai) != null ? _d : {},\n        ...!hasDefaultResponseFormat.has(this.modelId) ? { response_format: \"b64_json\" } : {}\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler4(\n        openaiImageResponseSchema\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      images: response.data.map((item) => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar openaiImageResponseSchema = z5.object({\n  data: z5.array(z5.object({ b64_json: z5.string() }))\n});\n\n// src/openai-transcription-model.ts\nimport {\n  combineHeaders as combineHeaders5,\n  convertBase64ToUint8Array,\n  createJsonResponseHandler as createJsonResponseHandler5,\n  parseProviderOptions,\n  postFormDataToApi\n} from \"@ai-sdk/provider-utils\";\nimport { z as z6 } from \"zod\";\nvar openAIProviderOptionsSchema = z6.object({\n  include: z6.array(z6.string()).nullish(),\n  language: z6.string().nullish(),\n  prompt: z6.string().nullish(),\n  temperature: z6.number().min(0).max(1).nullish().default(0),\n  timestampGranularities: z6.array(z6.enum([\"word\", \"segment\"])).nullish().default([\"segment\"])\n});\nvar languageMap = {\n  afrikaans: \"af\",\n  arabic: \"ar\",\n  armenian: \"hy\",\n  azerbaijani: \"az\",\n  belarusian: \"be\",\n  bosnian: \"bs\",\n  bulgarian: \"bg\",\n  catalan: \"ca\",\n  chinese: \"zh\",\n  croatian: \"hr\",\n  czech: \"cs\",\n  danish: \"da\",\n  dutch: \"nl\",\n  english: \"en\",\n  estonian: \"et\",\n  finnish: \"fi\",\n  french: \"fr\",\n  galician: \"gl\",\n  german: \"de\",\n  greek: \"el\",\n  hebrew: \"he\",\n  hindi: \"hi\",\n  hungarian: \"hu\",\n  icelandic: \"is\",\n  indonesian: \"id\",\n  italian: \"it\",\n  japanese: \"ja\",\n  kannada: \"kn\",\n  kazakh: \"kk\",\n  korean: \"ko\",\n  latvian: \"lv\",\n  lithuanian: \"lt\",\n  macedonian: \"mk\",\n  malay: \"ms\",\n  marathi: \"mr\",\n  maori: \"mi\",\n  nepali: \"ne\",\n  norwegian: \"no\",\n  persian: \"fa\",\n  polish: \"pl\",\n  portuguese: \"pt\",\n  romanian: \"ro\",\n  russian: \"ru\",\n  serbian: \"sr\",\n  slovak: \"sk\",\n  slovenian: \"sl\",\n  spanish: \"es\",\n  swahili: \"sw\",\n  swedish: \"sv\",\n  tagalog: \"tl\",\n  tamil: \"ta\",\n  thai: \"th\",\n  turkish: \"tr\",\n  ukrainian: \"uk\",\n  urdu: \"ur\",\n  vietnamese: \"vi\",\n  welsh: \"cy\"\n};\nvar OpenAITranscriptionModel = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    audio,\n    mediaType,\n    providerOptions\n  }) {\n    var _a, _b, _c, _d, _e;\n    const warnings = [];\n    const openAIOptions = parseProviderOptions({\n      provider: \"openai\",\n      providerOptions,\n      schema: openAIProviderOptionsSchema\n    });\n    const formData = new FormData();\n    const blob = audio instanceof Uint8Array ? new Blob([audio]) : new Blob([convertBase64ToUint8Array(audio)]);\n    formData.append(\"model\", this.modelId);\n    formData.append(\"file\", new File([blob], \"audio\", { type: mediaType }));\n    if (openAIOptions) {\n      const transcriptionModelOptions = {\n        include: (_a = openAIOptions.include) != null ? _a : void 0,\n        language: (_b = openAIOptions.language) != null ? _b : void 0,\n        prompt: (_c = openAIOptions.prompt) != null ? _c : void 0,\n        temperature: (_d = openAIOptions.temperature) != null ? _d : void 0,\n        timestamp_granularities: (_e = openAIOptions.timestampGranularities) != null ? _e : void 0\n      };\n      for (const key in transcriptionModelOptions) {\n        const value = transcriptionModelOptions[key];\n        if (value !== void 0) {\n          formData.append(key, String(value));\n        }\n      }\n    }\n    return {\n      formData,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a, _b, _c, _d, _e, _f;\n    const currentDate = (_c = (_b = (_a = this.config._internal) == null ? void 0 : _a.currentDate) == null ? void 0 : _b.call(_a)) != null ? _c : /* @__PURE__ */ new Date();\n    const { formData, warnings } = this.getArgs(options);\n    const {\n      value: response,\n      responseHeaders,\n      rawValue: rawResponse\n    } = await postFormDataToApi({\n      url: this.config.url({\n        path: \"/audio/transcriptions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders5(this.config.headers(), options.headers),\n      formData,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler5(\n        openaiTranscriptionResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const language = response.language != null && response.language in languageMap ? languageMap[response.language] : void 0;\n    return {\n      text: response.text,\n      segments: (_e = (_d = response.words) == null ? void 0 : _d.map((word) => ({\n        text: word.word,\n        startSecond: word.start,\n        endSecond: word.end\n      }))) != null ? _e : [],\n      language,\n      durationInSeconds: (_f = response.duration) != null ? _f : void 0,\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n};\nvar openaiTranscriptionResponseSchema = z6.object({\n  text: z6.string(),\n  language: z6.string().nullish(),\n  duration: z6.number().nullish(),\n  words: z6.array(\n    z6.object({\n      word: z6.string(),\n      start: z6.number(),\n      end: z6.number()\n    })\n  ).nullish()\n});\n\n// src/responses/openai-responses-language-model.ts\nimport {\n  combineHeaders as combineHeaders6,\n  createEventSourceResponseHandler as createEventSourceResponseHandler3,\n  createJsonResponseHandler as createJsonResponseHandler6,\n  generateId as generateId2,\n  parseProviderOptions as parseProviderOptions2,\n  postJsonToApi as postJsonToApi5\n} from \"@ai-sdk/provider-utils\";\nimport { z as z7 } from \"zod\";\n\n// src/responses/convert-to-openai-responses-messages.ts\nimport {\n  UnsupportedFunctionalityError as UnsupportedFunctionalityError6\n} from \"@ai-sdk/provider\";\nimport { convertUint8ArrayToBase64 as convertUint8ArrayToBase642 } from \"@ai-sdk/provider-utils\";\nfunction convertToOpenAIResponsesMessages({\n  prompt,\n  systemMessageMode\n}) {\n  const messages = [];\n  const warnings = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        switch (systemMessageMode) {\n          case \"system\": {\n            messages.push({ role: \"system\", content });\n            break;\n          }\n          case \"developer\": {\n            messages.push({ role: \"developer\", content });\n            break;\n          }\n          case \"remove\": {\n            warnings.push({\n              type: \"other\",\n              message: \"system messages are removed for this model\"\n            });\n            break;\n          }\n          default: {\n            const _exhaustiveCheck = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`\n            );\n          }\n        }\n        break;\n      }\n      case \"user\": {\n        messages.push({\n          role: \"user\",\n          content: content.map((part, index) => {\n            var _a, _b, _c, _d;\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"input_text\", text: part.text };\n              }\n              case \"image\": {\n                return {\n                  type: \"input_image\",\n                  image_url: part.image instanceof URL ? part.image.toString() : `data:${(_a = part.mimeType) != null ? _a : \"image/jpeg\"};base64,${convertUint8ArrayToBase642(part.image)}`,\n                  // OpenAI specific extension: image detail\n                  detail: (_c = (_b = part.providerMetadata) == null ? void 0 : _b.openai) == null ? void 0 : _c.imageDetail\n                };\n              }\n              case \"file\": {\n                if (part.data instanceof URL) {\n                  throw new UnsupportedFunctionalityError6({\n                    functionality: \"File URLs in user messages\"\n                  });\n                }\n                switch (part.mimeType) {\n                  case \"application/pdf\": {\n                    return {\n                      type: \"input_file\",\n                      filename: (_d = part.filename) != null ? _d : `part-${index}.pdf`,\n                      file_data: `data:application/pdf;base64,${part.data}`\n                    };\n                  }\n                  default: {\n                    throw new UnsupportedFunctionalityError6({\n                      functionality: \"Only PDF files are supported in user messages\"\n                    });\n                  }\n                }\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              messages.push({\n                role: \"assistant\",\n                content: [{ type: \"output_text\", text: part.text }]\n              });\n              break;\n            }\n            case \"tool-call\": {\n              messages.push({\n                type: \"function_call\",\n                call_id: part.toolCallId,\n                name: part.toolName,\n                arguments: JSON.stringify(part.args)\n              });\n              break;\n            }\n          }\n        }\n        break;\n      }\n      case \"tool\": {\n        for (const part of content) {\n          messages.push({\n            type: \"function_call_output\",\n            call_id: part.toolCallId,\n            output: JSON.stringify(part.result)\n          });\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return { messages, warnings };\n}\n\n// src/responses/map-openai-responses-finish-reason.ts\nfunction mapOpenAIResponseFinishReason({\n  finishReason,\n  hasToolCalls\n}) {\n  switch (finishReason) {\n    case void 0:\n    case null:\n      return hasToolCalls ? \"tool-calls\" : \"stop\";\n    case \"max_output_tokens\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    default:\n      return hasToolCalls ? \"tool-calls\" : \"unknown\";\n  }\n}\n\n// src/responses/openai-responses-prepare-tools.ts\nimport {\n  UnsupportedFunctionalityError as UnsupportedFunctionalityError7\n} from \"@ai-sdk/provider\";\nfunction prepareResponsesTools({\n  mode,\n  strict\n}) {\n  var _a;\n  const tools = ((_a = mode.tools) == null ? void 0 : _a.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, tool_choice: void 0, toolWarnings };\n  }\n  const toolChoice = mode.toolChoice;\n  const openaiTools2 = [];\n  for (const tool of tools) {\n    switch (tool.type) {\n      case \"function\":\n        openaiTools2.push({\n          type: \"function\",\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters,\n          strict: strict ? true : void 0\n        });\n        break;\n      case \"provider-defined\":\n        switch (tool.id) {\n          case \"openai.web_search_preview\":\n            openaiTools2.push({\n              type: \"web_search_preview\",\n              search_context_size: tool.args.searchContextSize,\n              user_location: tool.args.userLocation\n            });\n            break;\n          default:\n            toolWarnings.push({ type: \"unsupported-tool\", tool });\n            break;\n        }\n        break;\n      default:\n        toolWarnings.push({ type: \"unsupported-tool\", tool });\n        break;\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: openaiTools2, tool_choice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: openaiTools2, tool_choice: type, toolWarnings };\n    case \"tool\": {\n      if (toolChoice.toolName === \"web_search_preview\") {\n        return {\n          tools: openaiTools2,\n          tool_choice: {\n            type: \"web_search_preview\"\n          },\n          toolWarnings\n        };\n      }\n      return {\n        tools: openaiTools2,\n        tool_choice: {\n          type: \"function\",\n          name: toolChoice.toolName\n        },\n        toolWarnings\n      };\n    }\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError7({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\n\n// src/responses/openai-responses-language-model.ts\nvar OpenAIResponsesLanguageModel = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v1\";\n    this.defaultObjectGenerationMode = \"json\";\n    this.supportsStructuredOutputs = true;\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    mode,\n    maxTokens,\n    temperature,\n    stopSequences,\n    topP,\n    topK,\n    presencePenalty,\n    frequencyPenalty,\n    seed,\n    prompt,\n    providerMetadata,\n    responseFormat\n  }) {\n    var _a, _b, _c;\n    const warnings = [];\n    const modelConfig = getResponsesModelConfig(this.modelId);\n    const type = mode.type;\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"seed\"\n      });\n    }\n    if (presencePenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"presencePenalty\"\n      });\n    }\n    if (frequencyPenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"frequencyPenalty\"\n      });\n    }\n    if (stopSequences != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"stopSequences\"\n      });\n    }\n    const { messages, warnings: messageWarnings } = convertToOpenAIResponsesMessages({\n      prompt,\n      systemMessageMode: modelConfig.systemMessageMode\n    });\n    warnings.push(...messageWarnings);\n    const openaiOptions = parseProviderOptions2({\n      provider: \"openai\",\n      providerOptions: providerMetadata,\n      schema: openaiResponsesProviderOptionsSchema\n    });\n    const isStrict = (_a = openaiOptions == null ? void 0 : openaiOptions.strictSchemas) != null ? _a : true;\n    const baseArgs = {\n      model: this.modelId,\n      input: messages,\n      temperature,\n      top_p: topP,\n      max_output_tokens: maxTokens,\n      ...(responseFormat == null ? void 0 : responseFormat.type) === \"json\" && {\n        text: {\n          format: responseFormat.schema != null ? {\n            type: \"json_schema\",\n            strict: isStrict,\n            name: (_b = responseFormat.name) != null ? _b : \"response\",\n            description: responseFormat.description,\n            schema: responseFormat.schema\n          } : { type: \"json_object\" }\n        }\n      },\n      // provider options:\n      metadata: openaiOptions == null ? void 0 : openaiOptions.metadata,\n      parallel_tool_calls: openaiOptions == null ? void 0 : openaiOptions.parallelToolCalls,\n      previous_response_id: openaiOptions == null ? void 0 : openaiOptions.previousResponseId,\n      store: openaiOptions == null ? void 0 : openaiOptions.store,\n      user: openaiOptions == null ? void 0 : openaiOptions.user,\n      instructions: openaiOptions == null ? void 0 : openaiOptions.instructions,\n      // model-specific settings:\n      ...modelConfig.isReasoningModel && ((openaiOptions == null ? void 0 : openaiOptions.reasoningEffort) != null || (openaiOptions == null ? void 0 : openaiOptions.reasoningSummary) != null) && {\n        reasoning: {\n          ...(openaiOptions == null ? void 0 : openaiOptions.reasoningEffort) != null && {\n            effort: openaiOptions.reasoningEffort\n          },\n          ...(openaiOptions == null ? void 0 : openaiOptions.reasoningSummary) != null && {\n            summary: openaiOptions.reasoningSummary\n          }\n        }\n      },\n      ...modelConfig.requiredAutoTruncation && {\n        truncation: \"auto\"\n      }\n    };\n    if (modelConfig.isReasoningModel) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_p != null) {\n        baseArgs.top_p = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topP\",\n          details: \"topP is not supported for reasoning models\"\n        });\n      }\n    }\n    switch (type) {\n      case \"regular\": {\n        const { tools, tool_choice, toolWarnings } = prepareResponsesTools({\n          mode,\n          strict: isStrict\n          // TODO support provider options on tools\n        });\n        return {\n          args: {\n            ...baseArgs,\n            tools,\n            tool_choice\n          },\n          warnings: [...warnings, ...toolWarnings]\n        };\n      }\n      case \"object-json\": {\n        return {\n          args: {\n            ...baseArgs,\n            text: {\n              format: mode.schema != null ? {\n                type: \"json_schema\",\n                strict: isStrict,\n                name: (_c = mode.name) != null ? _c : \"response\",\n                description: mode.description,\n                schema: mode.schema\n              } : { type: \"json_object\" }\n            }\n          },\n          warnings\n        };\n      }\n      case \"object-tool\": {\n        return {\n          args: {\n            ...baseArgs,\n            tool_choice: { type: \"function\", name: mode.tool.name },\n            tools: [\n              {\n                type: \"function\",\n                name: mode.tool.name,\n                description: mode.tool.description,\n                parameters: mode.tool.parameters,\n                strict: isStrict\n              }\n            ]\n          },\n          warnings\n        };\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async doGenerate(options) {\n    var _a, _b, _c, _d, _e, _f, _g;\n    const { args: body, warnings } = this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi5({\n      url: this.config.url({\n        path: \"/responses\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders6(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler6(\n        z7.object({\n          id: z7.string(),\n          created_at: z7.number(),\n          model: z7.string(),\n          output: z7.array(\n            z7.discriminatedUnion(\"type\", [\n              z7.object({\n                type: z7.literal(\"message\"),\n                role: z7.literal(\"assistant\"),\n                content: z7.array(\n                  z7.object({\n                    type: z7.literal(\"output_text\"),\n                    text: z7.string(),\n                    annotations: z7.array(\n                      z7.object({\n                        type: z7.literal(\"url_citation\"),\n                        start_index: z7.number(),\n                        end_index: z7.number(),\n                        url: z7.string(),\n                        title: z7.string()\n                      })\n                    )\n                  })\n                )\n              }),\n              z7.object({\n                type: z7.literal(\"function_call\"),\n                call_id: z7.string(),\n                name: z7.string(),\n                arguments: z7.string()\n              }),\n              z7.object({\n                type: z7.literal(\"web_search_call\")\n              }),\n              z7.object({\n                type: z7.literal(\"computer_call\")\n              }),\n              z7.object({\n                type: z7.literal(\"reasoning\"),\n                summary: z7.array(\n                  z7.object({\n                    type: z7.literal(\"summary_text\"),\n                    text: z7.string()\n                  })\n                )\n              })\n            ])\n          ),\n          incomplete_details: z7.object({ reason: z7.string() }).nullable(),\n          usage: usageSchema\n        })\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const outputTextElements = response.output.filter((output) => output.type === \"message\").flatMap((output) => output.content).filter((content) => content.type === \"output_text\");\n    const toolCalls = response.output.filter((output) => output.type === \"function_call\").map((output) => ({\n      toolCallType: \"function\",\n      toolCallId: output.call_id,\n      toolName: output.name,\n      args: output.arguments\n    }));\n    const reasoningSummary = (_b = (_a = response.output.find((item) => item.type === \"reasoning\")) == null ? void 0 : _a.summary) != null ? _b : null;\n    return {\n      text: outputTextElements.map((content) => content.text).join(\"\\n\"),\n      sources: outputTextElements.flatMap(\n        (content) => content.annotations.map((annotation) => {\n          var _a2, _b2, _c2;\n          return {\n            sourceType: \"url\",\n            id: (_c2 = (_b2 = (_a2 = this.config).generateId) == null ? void 0 : _b2.call(_a2)) != null ? _c2 : generateId2(),\n            url: annotation.url,\n            title: annotation.title\n          };\n        })\n      ),\n      finishReason: mapOpenAIResponseFinishReason({\n        finishReason: (_c = response.incomplete_details) == null ? void 0 : _c.reason,\n        hasToolCalls: toolCalls.length > 0\n      }),\n      toolCalls: toolCalls.length > 0 ? toolCalls : void 0,\n      reasoning: reasoningSummary ? reasoningSummary.map((summary) => ({\n        type: \"text\",\n        text: summary.text\n      })) : void 0,\n      usage: {\n        promptTokens: response.usage.input_tokens,\n        completionTokens: response.usage.output_tokens\n      },\n      rawCall: {\n        rawPrompt: void 0,\n        rawSettings: {}\n      },\n      rawResponse: {\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      request: {\n        body: JSON.stringify(body)\n      },\n      response: {\n        id: response.id,\n        timestamp: new Date(response.created_at * 1e3),\n        modelId: response.model\n      },\n      providerMetadata: {\n        openai: {\n          responseId: response.id,\n          cachedPromptTokens: (_e = (_d = response.usage.input_tokens_details) == null ? void 0 : _d.cached_tokens) != null ? _e : null,\n          reasoningTokens: (_g = (_f = response.usage.output_tokens_details) == null ? void 0 : _f.reasoning_tokens) != null ? _g : null\n        }\n      },\n      warnings\n    };\n  }\n  async doStream(options) {\n    const { args: body, warnings } = this.getArgs(options);\n    const { responseHeaders, value: response } = await postJsonToApi5({\n      url: this.config.url({\n        path: \"/responses\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders6(this.config.headers(), options.headers),\n      body: {\n        ...body,\n        stream: true\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler3(\n        openaiResponsesChunkSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const self = this;\n    let finishReason = \"unknown\";\n    let promptTokens = NaN;\n    let completionTokens = NaN;\n    let cachedPromptTokens = null;\n    let reasoningTokens = null;\n    let responseId = null;\n    const ongoingToolCalls = {};\n    let hasToolCalls = false;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            var _a, _b, _c, _d, _e, _f, _g, _h;\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (isResponseOutputItemAddedChunk(value)) {\n              if (value.item.type === \"function_call\") {\n                ongoingToolCalls[value.output_index] = {\n                  toolName: value.item.name,\n                  toolCallId: value.item.call_id\n                };\n                controller.enqueue({\n                  type: \"tool-call-delta\",\n                  toolCallType: \"function\",\n                  toolCallId: value.item.call_id,\n                  toolName: value.item.name,\n                  argsTextDelta: value.item.arguments\n                });\n              }\n            } else if (isResponseFunctionCallArgumentsDeltaChunk(value)) {\n              const toolCall = ongoingToolCalls[value.output_index];\n              if (toolCall != null) {\n                controller.enqueue({\n                  type: \"tool-call-delta\",\n                  toolCallType: \"function\",\n                  toolCallId: toolCall.toolCallId,\n                  toolName: toolCall.toolName,\n                  argsTextDelta: value.delta\n                });\n              }\n            } else if (isResponseCreatedChunk(value)) {\n              responseId = value.response.id;\n              controller.enqueue({\n                type: \"response-metadata\",\n                id: value.response.id,\n                timestamp: new Date(value.response.created_at * 1e3),\n                modelId: value.response.model\n              });\n            } else if (isTextDeltaChunk(value)) {\n              controller.enqueue({\n                type: \"text-delta\",\n                textDelta: value.delta\n              });\n            } else if (isResponseReasoningSummaryTextDeltaChunk(value)) {\n              controller.enqueue({\n                type: \"reasoning\",\n                textDelta: value.delta\n              });\n            } else if (isResponseOutputItemDoneChunk(value) && value.item.type === \"function_call\") {\n              ongoingToolCalls[value.output_index] = void 0;\n              hasToolCalls = true;\n              controller.enqueue({\n                type: \"tool-call\",\n                toolCallType: \"function\",\n                toolCallId: value.item.call_id,\n                toolName: value.item.name,\n                args: value.item.arguments\n              });\n            } else if (isResponseFinishedChunk(value)) {\n              finishReason = mapOpenAIResponseFinishReason({\n                finishReason: (_a = value.response.incomplete_details) == null ? void 0 : _a.reason,\n                hasToolCalls\n              });\n              promptTokens = value.response.usage.input_tokens;\n              completionTokens = value.response.usage.output_tokens;\n              cachedPromptTokens = (_c = (_b = value.response.usage.input_tokens_details) == null ? void 0 : _b.cached_tokens) != null ? _c : cachedPromptTokens;\n              reasoningTokens = (_e = (_d = value.response.usage.output_tokens_details) == null ? void 0 : _d.reasoning_tokens) != null ? _e : reasoningTokens;\n            } else if (isResponseAnnotationAddedChunk(value)) {\n              controller.enqueue({\n                type: \"source\",\n                source: {\n                  sourceType: \"url\",\n                  id: (_h = (_g = (_f = self.config).generateId) == null ? void 0 : _g.call(_f)) != null ? _h : generateId2(),\n                  url: value.annotation.url,\n                  title: value.annotation.title\n                }\n              });\n            }\n          },\n          flush(controller) {\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage: { promptTokens, completionTokens },\n              ...(cachedPromptTokens != null || reasoningTokens != null) && {\n                providerMetadata: {\n                  openai: {\n                    responseId,\n                    cachedPromptTokens,\n                    reasoningTokens\n                  }\n                }\n              }\n            });\n          }\n        })\n      ),\n      rawCall: {\n        rawPrompt: void 0,\n        rawSettings: {}\n      },\n      rawResponse: { headers: responseHeaders },\n      request: { body: JSON.stringify(body) },\n      warnings\n    };\n  }\n};\nvar usageSchema = z7.object({\n  input_tokens: z7.number(),\n  input_tokens_details: z7.object({ cached_tokens: z7.number().nullish() }).nullish(),\n  output_tokens: z7.number(),\n  output_tokens_details: z7.object({ reasoning_tokens: z7.number().nullish() }).nullish()\n});\nvar textDeltaChunkSchema = z7.object({\n  type: z7.literal(\"response.output_text.delta\"),\n  delta: z7.string()\n});\nvar responseFinishedChunkSchema = z7.object({\n  type: z7.enum([\"response.completed\", \"response.incomplete\"]),\n  response: z7.object({\n    incomplete_details: z7.object({ reason: z7.string() }).nullish(),\n    usage: usageSchema\n  })\n});\nvar responseCreatedChunkSchema = z7.object({\n  type: z7.literal(\"response.created\"),\n  response: z7.object({\n    id: z7.string(),\n    created_at: z7.number(),\n    model: z7.string()\n  })\n});\nvar responseOutputItemDoneSchema = z7.object({\n  type: z7.literal(\"response.output_item.done\"),\n  output_index: z7.number(),\n  item: z7.discriminatedUnion(\"type\", [\n    z7.object({\n      type: z7.literal(\"message\")\n    }),\n    z7.object({\n      type: z7.literal(\"function_call\"),\n      id: z7.string(),\n      call_id: z7.string(),\n      name: z7.string(),\n      arguments: z7.string(),\n      status: z7.literal(\"completed\")\n    })\n  ])\n});\nvar responseFunctionCallArgumentsDeltaSchema = z7.object({\n  type: z7.literal(\"response.function_call_arguments.delta\"),\n  item_id: z7.string(),\n  output_index: z7.number(),\n  delta: z7.string()\n});\nvar responseOutputItemAddedSchema = z7.object({\n  type: z7.literal(\"response.output_item.added\"),\n  output_index: z7.number(),\n  item: z7.discriminatedUnion(\"type\", [\n    z7.object({\n      type: z7.literal(\"message\")\n    }),\n    z7.object({\n      type: z7.literal(\"function_call\"),\n      id: z7.string(),\n      call_id: z7.string(),\n      name: z7.string(),\n      arguments: z7.string()\n    })\n  ])\n});\nvar responseAnnotationAddedSchema = z7.object({\n  type: z7.literal(\"response.output_text.annotation.added\"),\n  annotation: z7.object({\n    type: z7.literal(\"url_citation\"),\n    url: z7.string(),\n    title: z7.string()\n  })\n});\nvar responseReasoningSummaryTextDeltaSchema = z7.object({\n  type: z7.literal(\"response.reasoning_summary_text.delta\"),\n  item_id: z7.string(),\n  output_index: z7.number(),\n  summary_index: z7.number(),\n  delta: z7.string()\n});\nvar openaiResponsesChunkSchema = z7.union([\n  textDeltaChunkSchema,\n  responseFinishedChunkSchema,\n  responseCreatedChunkSchema,\n  responseOutputItemDoneSchema,\n  responseFunctionCallArgumentsDeltaSchema,\n  responseOutputItemAddedSchema,\n  responseAnnotationAddedSchema,\n  responseReasoningSummaryTextDeltaSchema,\n  z7.object({ type: z7.string() }).passthrough()\n  // fallback for unknown chunks\n]);\nfunction isTextDeltaChunk(chunk) {\n  return chunk.type === \"response.output_text.delta\";\n}\nfunction isResponseOutputItemDoneChunk(chunk) {\n  return chunk.type === \"response.output_item.done\";\n}\nfunction isResponseFinishedChunk(chunk) {\n  return chunk.type === \"response.completed\" || chunk.type === \"response.incomplete\";\n}\nfunction isResponseCreatedChunk(chunk) {\n  return chunk.type === \"response.created\";\n}\nfunction isResponseFunctionCallArgumentsDeltaChunk(chunk) {\n  return chunk.type === \"response.function_call_arguments.delta\";\n}\nfunction isResponseOutputItemAddedChunk(chunk) {\n  return chunk.type === \"response.output_item.added\";\n}\nfunction isResponseAnnotationAddedChunk(chunk) {\n  return chunk.type === \"response.output_text.annotation.added\";\n}\nfunction isResponseReasoningSummaryTextDeltaChunk(chunk) {\n  return chunk.type === \"response.reasoning_summary_text.delta\";\n}\nfunction getResponsesModelConfig(modelId) {\n  if (modelId.startsWith(\"o\")) {\n    if (modelId.startsWith(\"o1-mini\") || modelId.startsWith(\"o1-preview\")) {\n      return {\n        isReasoningModel: true,\n        systemMessageMode: \"remove\",\n        requiredAutoTruncation: false\n      };\n    }\n    return {\n      isReasoningModel: true,\n      systemMessageMode: \"developer\",\n      requiredAutoTruncation: false\n    };\n  }\n  return {\n    isReasoningModel: false,\n    systemMessageMode: \"system\",\n    requiredAutoTruncation: false\n  };\n}\nvar openaiResponsesProviderOptionsSchema = z7.object({\n  metadata: z7.any().nullish(),\n  parallelToolCalls: z7.boolean().nullish(),\n  previousResponseId: z7.string().nullish(),\n  store: z7.boolean().nullish(),\n  user: z7.string().nullish(),\n  reasoningEffort: z7.string().nullish(),\n  strictSchemas: z7.boolean().nullish(),\n  instructions: z7.string().nullish(),\n  reasoningSummary: z7.string().nullish()\n});\n\n// src/openai-tools.ts\nimport { z as z8 } from \"zod\";\nvar WebSearchPreviewParameters = z8.object({});\nfunction webSearchPreviewTool({\n  searchContextSize,\n  userLocation\n} = {}) {\n  return {\n    type: \"provider-defined\",\n    id: \"openai.web_search_preview\",\n    args: {\n      searchContextSize,\n      userLocation\n    },\n    parameters: WebSearchPreviewParameters\n  };\n}\nvar openaiTools = {\n  webSearchPreview: webSearchPreviewTool\n};\n\n// src/openai-speech-model.ts\nimport {\n  combineHeaders as combineHeaders7,\n  createBinaryResponseHandler,\n  parseProviderOptions as parseProviderOptions3,\n  postJsonToApi as postJsonToApi6\n} from \"@ai-sdk/provider-utils\";\nimport { z as z9 } from \"zod\";\nvar OpenAIProviderOptionsSchema = z9.object({\n  instructions: z9.string().nullish(),\n  speed: z9.number().min(0.25).max(4).default(1).nullish()\n});\nvar OpenAISpeechModel = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    text,\n    voice = \"alloy\",\n    outputFormat = \"mp3\",\n    speed,\n    instructions,\n    providerOptions\n  }) {\n    const warnings = [];\n    const openAIOptions = parseProviderOptions3({\n      provider: \"openai\",\n      providerOptions,\n      schema: OpenAIProviderOptionsSchema\n    });\n    const requestBody = {\n      model: this.modelId,\n      input: text,\n      voice,\n      response_format: \"mp3\",\n      speed,\n      instructions\n    };\n    if (outputFormat) {\n      if ([\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"].includes(outputFormat)) {\n        requestBody.response_format = outputFormat;\n      } else {\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"outputFormat\",\n          details: `Unsupported output format: ${outputFormat}. Using mp3 instead.`\n        });\n      }\n    }\n    if (openAIOptions) {\n      const speechModelOptions = {};\n      for (const key in speechModelOptions) {\n        const value = speechModelOptions[key];\n        if (value !== void 0) {\n          requestBody[key] = value;\n        }\n      }\n    }\n    return {\n      requestBody,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a, _b, _c;\n    const currentDate = (_c = (_b = (_a = this.config._internal) == null ? void 0 : _a.currentDate) == null ? void 0 : _b.call(_a)) != null ? _c : /* @__PURE__ */ new Date();\n    const { requestBody, warnings } = this.getArgs(options);\n    const {\n      value: audio,\n      responseHeaders,\n      rawValue: rawResponse\n    } = await postJsonToApi6({\n      url: this.config.url({\n        path: \"/audio/speech\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders7(this.config.headers(), options.headers),\n      body: requestBody,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createBinaryResponseHandler(),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      audio,\n      warnings,\n      request: {\n        body: JSON.stringify(requestBody)\n      },\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n};\n\n// src/openai-provider.ts\nfunction createOpenAI(options = {}) {\n  var _a, _b, _c;\n  const baseURL = (_a = withoutTrailingSlash(options.baseURL)) != null ? _a : \"https://api.openai.com/v1\";\n  const compatibility = (_b = options.compatibility) != null ? _b : \"compatible\";\n  const providerName = (_c = options.name) != null ? _c : \"openai\";\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: \"OPENAI_API_KEY\",\n      description: \"OpenAI\"\n    })}`,\n    \"OpenAI-Organization\": options.organization,\n    \"OpenAI-Project\": options.project,\n    ...options.headers\n  });\n  const createChatModel = (modelId, settings = {}) => new OpenAIChatLanguageModel(modelId, settings, {\n    provider: `${providerName}.chat`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    compatibility,\n    fetch: options.fetch\n  });\n  const createCompletionModel = (modelId, settings = {}) => new OpenAICompletionLanguageModel(modelId, settings, {\n    provider: `${providerName}.completion`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    compatibility,\n    fetch: options.fetch\n  });\n  const createEmbeddingModel = (modelId, settings = {}) => new OpenAIEmbeddingModel(modelId, settings, {\n    provider: `${providerName}.embedding`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createImageModel = (modelId, settings = {}) => new OpenAIImageModel(modelId, settings, {\n    provider: `${providerName}.image`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createTranscriptionModel = (modelId) => new OpenAITranscriptionModel(modelId, {\n    provider: `${providerName}.transcription`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createSpeechModel = (modelId) => new OpenAISpeechModel(modelId, {\n    provider: `${providerName}.speech`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createLanguageModel = (modelId, settings) => {\n    if (new.target) {\n      throw new Error(\n        \"The OpenAI model function cannot be called with the new keyword.\"\n      );\n    }\n    if (modelId === \"gpt-3.5-turbo-instruct\") {\n      return createCompletionModel(\n        modelId,\n        settings\n      );\n    }\n    return createChatModel(modelId, settings);\n  };\n  const createResponsesModel = (modelId) => {\n    return new OpenAIResponsesLanguageModel(modelId, {\n      provider: `${providerName}.responses`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch\n    });\n  };\n  const provider = function(modelId, settings) {\n    return createLanguageModel(modelId, settings);\n  };\n  provider.languageModel = createLanguageModel;\n  provider.chat = createChatModel;\n  provider.completion = createCompletionModel;\n  provider.responses = createResponsesModel;\n  provider.embedding = createEmbeddingModel;\n  provider.textEmbedding = createEmbeddingModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.image = createImageModel;\n  provider.imageModel = createImageModel;\n  provider.transcription = createTranscriptionModel;\n  provider.transcriptionModel = createTranscriptionModel;\n  provider.speech = createSpeechModel;\n  provider.speechModel = createSpeechModel;\n  provider.tools = openaiTools;\n  return provider;\n}\nvar openai = createOpenAI({\n  compatibility: \"strict\"\n  // strict for OpenAI API\n});\nexport {\n  createOpenAI,\n  openai\n};\n//# sourceMappingURL=index.mjs.map"],"names":[],"sourceRoot":""}